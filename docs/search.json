[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "PYTHON SQL NSS/AFNS\n\nA quantitative fixed-income analytics toolkit for visualizing, forecasting, and analyzing treasury yields.\n\nGitHub\n\n\n\nMarch 20th, 2025"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "As a quantitative finance enthusiast with a focus on fixed-income markets and macroeconomic modeling, I’m pursuing opportunities in fixed-income asset management, equity research, risk analysis, or quantitative macro.\n\nPreview\n\n\n  \n\nClick the image to view the full PDF\n\n\n\n\nLast updated: April 2025\n\n\n Download Full CV (PDF)"
  },
  {
    "objectID": "blog/posts/HW4/index.html#introduction",
    "href": "blog/posts/HW4/index.html#introduction",
    "title": "JAX-Driven Heat Diffusion: Sparse and Vectorized PDE Simulation",
    "section": "Introduction",
    "text": "Introduction\nHeat diffusion is a fundamental process in physics and engineering, described by the heat equation, a partial differential equation (PDE) modeling thermal energy transfer. In this project, we simulate 2D heat diffusion using multiple computational approaches:\n\nMatrix-vector multiplication (explicit finite difference method).\nSparse matrix representation in JAX (efficient storage and computation).\nNumPy vectorized operations (direct array manipulation with np.roll).\nJAX-optimized just-in-time (JIT) compilation (high-speed execution).\n\nThe objective is to compare performance and efficiency across these methods while maintaining correct boundary conditions (to simulate heat diffusion properly) and stable numerical updates. Sparse representations and JIT-optimized computations significantly reduce runtime complexity compared to basic implementations. The results will be visualized using heatmaps at regular intervals to analyze diffusion patterns. Additionally, results will be accompanied by runtimes to compare code efficiency."
  },
  {
    "objectID": "blog/posts/HW4/index.html#the-math-of-heat-diffusion-simulations",
    "href": "blog/posts/HW4/index.html#the-math-of-heat-diffusion-simulations",
    "title": "JAX-Driven Heat Diffusion: Sparse and Vectorized PDE Simulation",
    "section": "The Math of Heat Diffusion Simulations",
    "text": "The Math of Heat Diffusion Simulations\nBefore we dive into the code that simulates heat diffusion, we must understand the math behind it.\n\nThe Continuous 2D Heat Equation\nThe heat diffusion equation describes how heat spreads over time in a two-dimensional space. The general form is:\n\\[\\frac{\\partial u}{\\partial t} = \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\]\nwhere:\n\n\\(u(x, y, t)\\) represents the temperature at position \\((x, y)\\) at time \\(t\\).\nThe right-hand side represents the Laplacian operator, which models how heat spreads from a point to its neighbors.\n\n\n\nDiscretization Using Finite Differences\nWe approximate the second derivatives using the finite difference method. Using a uniform grid with spacing \\(\\Delta x = \\Delta y\\) we approximate:\n\\[\\frac{\\partial^2 u}{\\partial x^2} \\approx \\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{\\Delta x^2}\\]\n\\[\\frac{\\partial^2 u}{\\partial y^2} \\approx \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{\\Delta y^2}\\]\nSubstituting these into the heat equation:\n\\[\\frac{u_{i,j}^{k+1} - u_{i,j}^{k}}{\\Delta t} = \\frac{u_{i+1,j}^{k} - 2u_{i,j}^{k} + u_{i-1,j}^{k}}{\\Delta x^2} + \\frac{u_{i,j+1}^{k} - 2u_{i,j}^{k} + u_{i,j-1}^{k}}{\\Delta y^2} \\]\nRearranging for \\(u_{i,j}^{k+1}\\):\n\\[u_{i,j}^{k+1} = u_{i,j}^{k} + \\epsilon \\left( u_{i+1,j}^{k} + u_{i-1,j}^{k} + u_{i,j+1}^{k} + u_{i,j-1}^{k} - 4u_{i,j}^{k} \\right)\\]\nwhere \\(\\epsilon = \\frac{ \\Delta t}{\\Delta x^2}\\) is the stability parameter.\n\n\nMatrix Representation of the Finite Difference Method\nWe represent the system in matrix form:\n\\[\nu_{k+1} = u_k + \\epsilon A u_k\n\\]\nwhere:\n\n\\(u_k\\) is the flattened temperature grid at time \\(k\\).\n\\(A\\) is the finite difference matrix that applies the heat diffusion stencil.\n\nThe finite difference matrix \\(A\\) for a grid of size \\(N \\times N\\) is:\n\\[\nA =\n\\begin{bmatrix}\nT & I & 0 & 0 & \\dots & 0  \\\\\nI & T & I & 0 & \\dots & 0  \\\\\n0 & I & T & I & \\dots & 0  \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\dots & T & I  \\\\\n0 & 0 & 0 & \\dots & I & T\n\\end{bmatrix}\n\\]\nwhere: - \\(T\\) is an \\(N \\times N\\) tridiagonal matrix representing left-right interactions:\n\\[\n  T =\n  \\begin{bmatrix}\n  -4 & 1  & 0  & \\dots & 0  \\\\\n  1  & -4 & 1  & \\dots & 0  \\\\\n  0  & 1  & -4 & \\dots & 0  \\\\\n  \\vdots & \\vdots & \\vdots & \\ddots & \\vdots  \\\\\n  0  & 0  & 0  & \\dots & -4\n  \\end{bmatrix}\n  \\]\n\n\\(I\\) is an \\(N \\times N\\) identity matrix that accounts for top/bottom interactions.\n\nThis matrix enforces:\n\nSelf-weight (-4) at each grid point.\nNeighboring weights (+1) for adjacent grid points."
  },
  {
    "objectID": "blog/posts/HW4/index.html#initializing-our-heat-diffusion-simulation",
    "href": "blog/posts/HW4/index.html#initializing-our-heat-diffusion-simulation",
    "title": "JAX-Driven Heat Diffusion: Sparse and Vectorized PDE Simulation",
    "section": "Initializing Our Heat Diffusion Simulation",
    "text": "Initializing Our Heat Diffusion Simulation\nFor our simulations we will use a grid size N = 101 and stability parameter \\(\\epsilon = 0.2\\).\n\nN = 101\nepsilon = 0.2\n\nWe initialize the simulation with a single heat source at the center.\nWe’ll use the following initial condition:\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n# construct initial condition: 1 unit of heat at midpoint. \nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)"
  },
  {
    "objectID": "blog/posts/HW4/index.html#heat-diffusion-with-matrix-vector-multiplication",
    "href": "blog/posts/HW4/index.html#heat-diffusion-with-matrix-vector-multiplication",
    "title": "JAX-Driven Heat Diffusion: Sparse and Vectorized PDE Simulation",
    "section": "Heat Diffusion With Matrix-Vector Multiplication",
    "text": "Heat Diffusion With Matrix-Vector Multiplication\nIn order to begin our 2D heat simulations we’ll create a file heat_equation.py with the following packages:\n\nimport numpy as np\nimport jax.numpy as jnp\nimport jax\nfrom scipy.sparse import diags\nfrom jax.experimental.sparse import BCOO\n\nUsing the math above we use matrix-vector multiplication to simulate heat diffusion. The grid is flattened into a 1D vector to apply the finite difference matrix efficiently:\n\\[\nu_{k+1} = u_k + \\epsilon A u_k\n\\]\nwhere:\n\n\\(A\\) is the finite difference matrix of size \\(N^2 \\times N^2\\).\n\\(U^k\\) is the flattened temperature grid at time \\(k\\).\n\\(\\epsilon\\) is the stability constant.\n\nThis formula is represented by the following function:\n\nfrom heat_equation import advance_time_matvecmul\nimport inspect\nprint(inspect.getsource(advance_time_matvecmul))\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix, N^2 x N^2. \n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    N = u.shape[0]\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\n\n\n\nIn order for the function above to run we also need to write a function get_A() that constructs the matrix A:\n\nfrom heat_equation import get_A\nprint(inspect.getsource(get_A))\n\ndef get_A(N):\n    \"\"\"Constructs the finite difference matrix A for the heat equation.\n    Args:\n        N: Grid size\n\n    Returns:\n        A: Finite difference matrix (N^2 x N^2).\n    \"\"\"\n    n = N * N\n    diagonals = [\n        -4 * np.ones(n),  # Main diagonal\n        np.ones(n - 1),    # Right neighbor\n        np.ones(n - 1),    # Left neighbor\n        np.ones(n - N),    # Upper neighbor\n        np.ones(n - N)     # Lower neighbor\n    ]\n    # Apply boundary conditions (preventing wrap-around)\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0  \n\n    # Construct the finite difference matrix\n    A = (\n        np.diag(diagonals[0]) +\n        np.diag(diagonals[1], 1) +\n        np.diag(diagonals[2], -1) +\n        np.diag(diagonals[3], N) +\n        np.diag(diagonals[4], -N)\n    )\n    return A\n\n\n\nTo test our functions and create a heat simulation, we’ll write a function that allows us to specify our type of function that updates the heat distribution from heat_equations.py and the version of matrix A we’ll use in each test.\nThe following function wraps our testing and visualization for all test cases:\n\nimport matplotlib.pyplot as plt\nimport time\n\ndef heat_diffusion_test(func, \n                        u_init, \n                        epsilon, \n                        iterations=2700, \n                        snapshot_interval=300, \n                        matrix_A=None):\n    \"\"\"\n    Runs and visualizes the heat diffusion simulation.\n\n    Args:\n        - func (function): The function that updates the heat distribution.\n        - u_init (np.ndarray or jnp.ndarray): Initial heat distribution (N x N grid).\n        - epsilon: Stability constant.\n        - iterations: Total number of iterations to run.\n        - snapshot_interval: Interval at which to capture snapshots.\n        - matrix_A: matrix A.\n    \n    Returns:\n        - Heat simulation visualizations\n        - Total execution time of the simulation.\n    \"\"\"\n    # Copy the initial condition\n    u = np.copy(u_init) if isinstance(u_init, np.ndarray) else jnp.array(u_init)\n\n    # Store snapshots for visualization\n    snapshots = []\n\n    # Precompile JAX function if applicable\n    if \"jax\" in str(type(func)):\n        u = func(u, epsilon)  # JAX functions require precompilation\n        u.block_until_ready()\n\n    # Run the simulation and time it\n    start_time = time.time()\n    for i in range(1, iterations + 1):\n        if matrix_A is not None:  # For matrix-vector multiplication\n            u = func(A, u, epsilon)\n        else:\n            u = func(u, epsilon)\n\n        if i % snapshot_interval == 0:\n            snapshots.append(np.array(u))  # Convert JAX arrays to NumPy if needed\n\n    # Ensure JAX computations complete\n    if isinstance(u, jnp.ndarray):\n        u.block_until_ready()\n    \n    end_time = time.time()\n    elapsed_time = end_time - start_time\n\n    # Plot results in a 3x3 grid\n    fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n    for idx, snapshot in enumerate(snapshots):\n        ax = axes[idx // 3, idx % 3]\n        ax.imshow(snapshot, cmap=\"viridis\", interpolation=\"nearest\")\n        ax.set_title(f\"Iteration {(idx + 1) * snapshot_interval}\")\n\n    plt.tight_layout()\n    plt.show()\n\n    return elapsed_time\n\nWe can now test our first heat simulation using get_A() and heat_diffusion_test() with advance_time_matvecmul() passed as the function in the test arguments:\n\nimport heat_equation\n\nA = get_A(N)\ntime = heat_diffusion_test(advance_time_matvecmul, u0, epsilon, matrix_A=A)\nprint(f\"Total execution time: {time:.2f} seconds\")\n\n\n\n\n\n\n\n\nTotal execution time: 35.84 seconds\n\n\nThis method, while mathematically correct, is computationally expensive (as seen by the runtime of approximately 36 seconds). In the next sections, we optimize it using sparse matrices and vectorized operations."
  },
  {
    "objectID": "blog/posts/HW4/index.html#optimizing-computation-with-sparse-matrices",
    "href": "blog/posts/HW4/index.html#optimizing-computation-with-sparse-matrices",
    "title": "JAX-Driven Heat Diffusion: Sparse and Vectorized PDE Simulation",
    "section": "Optimizing Computation With Sparse Matrices",
    "text": "Optimizing Computation With Sparse Matrices\nThe finite difference matrix \\(A\\) is mostly zeros, leading to wasted computations. Using JAX sparse matrices, we store only nonzero values, reducing memory usage and improving efficiency.\nWe’ll write a function get_sparse_A() that computes the sparse matrix using the batched coordinate BCOO format to only use \\(O(N^2)\\) space for the matrix, and only take \\(O(N^2)\\) time for each update:\n\nfrom heat_equation import get_sparse_A\nprint(inspect.getsource(get_sparse_A))\n\ndef get_sparse_A(N):\n    \"\"\"Constructs the sparse matrix A using JAX sparse format.\"\"\"\n    A = get_A(N) # Get the dense matrix A\n    A_sp_matrix = BCOO.fromdense(jnp.array(A)) # Convert dense matrix to JAX sparse format (BCOO)\n    return A_sp_matrix\n\n\n\nHere, A_sp_matrix is the sparse matrix of A which should make the compute more efficient. We can test this by rerunning the heat simulation using the get_sparse_A() matrix and the same heat_diffusion_test() function, comparing the runtimes:\n\nimport heat_equation\n\nA = get_sparse_A(N)\ntime = heat_diffusion_test(advance_time_matvecmul, u0, epsilon, matrix_A=A)\nprint(f\"Total execution time: {time:.2f} seconds\")\n\n\n\n\n\n\n\n\nTotal execution time: 1.86 seconds\n\n\nRuntime for the heat simulation using the sparse matrix of A is more than 10x faster (19.27x) at approximately 2 seconds. Still, we can make the heat simulation more efficient by replacing advance_time_matvecmul(), which utilizes matrix-vector multiplications with vectorized array operations like np.roll()."
  },
  {
    "objectID": "blog/posts/HW4/index.html#direct-numpy-vectorization",
    "href": "blog/posts/HW4/index.html#direct-numpy-vectorization",
    "title": "JAX-Driven Heat Diffusion: Sparse and Vectorized PDE Simulation",
    "section": "Direct NumPy Vectorization",
    "text": "Direct NumPy Vectorization\nThe matrix-vector multiplication approach is useful, particularly in other PDE problems, such as Poisson equations, where the matrix equation must be solved. However, for the heat equation, it is not necessary in terms of computation. Instead, we can use NumPy’s np.roll() to compute updates directly.\nWe’ll do this by writing the function advance_time_numpy() in heat_equation.py:\n\nfrom heat_equation import advance_time_numpy\nprint(inspect.getsource(advance_time_numpy))\n\ndef advance_time_numpy(u, epsilon):\n    \"\"\"Advances the simulation using NumPy's np.roll for boundary handling.\"\"\"\n    u_padded = np.pad(u, pad_width=1, mode='constant', constant_values=0)  # Pad u with zeros\n    u_next = u + epsilon * (  # Update u using finite differences\n        np.roll(u_padded, 1, axis=0)[1:-1, 1:-1] +  # Shift up\n        np.roll(u_padded, -1, axis=0)[1:-1, 1:-1] +  # Shift down\n        np.roll(u_padded, 1, axis=1)[1:-1, 1:-1] +  # Shift left\n        np.roll(u_padded, -1, axis=1)[1:-1, 1:-1] -  # Shift right\n        4 * u  # Subtract central value\n    )\n    return u_next  # Return updated interior values\n\n\n\nThe function above works by padding the grid with zeros and computes the transformation using np.roll(). We can understand the function with the visual below:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext, we can rerun the simulation using the advance_time_numpy() function as an argument in the visualize_heat_diffusion() function and compare runtimes:\n\nimport heat_equation\n\ntime = heat_diffusion_test(advance_time_numpy, u0, epsilon)\nprint(f\"Total execution time: {time:.2f} seconds\")\n\n\n\n\n\n\n\n\nTotal execution time: 0.21 seconds\n\n\nRuntime for the heat simulation using the direct NumPy vectorization is more than 100x faster (170.67x) than the base matrix-vector simulation we did at 0.21 seconds. While already fast, we can achieve a quicker runtime by applying the jit function to essentially the same function advance_time_numpy(), using the JAX package."
  },
  {
    "objectID": "blog/posts/HW4/index.html#jax-jit-compilation-for-maximum-speed",
    "href": "blog/posts/HW4/index.html#jax-jit-compilation-for-maximum-speed",
    "title": "JAX-Driven Heat Diffusion: Sparse and Vectorized PDE Simulation",
    "section": "JAX JIT Compilation for Maximum Speed",
    "text": "JAX JIT Compilation for Maximum Speed\nUsing JAX’s @jax.jit, we can compile the function into optimized machine code, further reducing execution time.\nWe’ll do this by writing the function advance_time_jax() in heat_equation.py with @jax.jit above the function:\n\nfrom heat_equation import advance_time_jax\nprint(inspect.getsource(advance_time_jax))\n\ndef advance_time_jax(u, epsilon):\n    \"\"\"Advances the simulation using JAX and just-in-time compilation.\"\"\"\n    u_padded = np.pad(u, pad_width=1, mode='constant', constant_values=0)  # Pad u with zeros\n    u_next = u + epsilon * (  # Update u using finite differences\n        jnp.roll(u_padded, 1, axis=0)[1:-1, 1:-1] +  # Shift up\n        jnp.roll(u_padded, -1, axis=0)[1:-1, 1:-1] +  # Shift down\n        jnp.roll(u_padded, 1, axis=1)[1:-1, 1:-1] +  # Shift left\n        jnp.roll(u_padded, -1, axis=1)[1:-1, 1:-1] -  # Shift right\n        4 * u  # Subtract central value\n    )\n    return u_next  # Return updated interior values\n\n\n\nThe only difference between the direct NumPy vectorization function is that we use @jax.jit above our function and replace the NumPy calls np with the JAX NumPy calls jnp.\nWe’ll run the heat diffusion test one last time using the JAX optimized function and compare execution speed:\n\nimport heat_equation\n\ntime = heat_diffusion_test(advance_time_jax, u0, epsilon)\nprint(f\"Total execution time: {time:.2f} seconds\")\n\n\n\n\n\n\n\n\nTotal execution time: 0.06 seconds\n\n\nRuntime for the heat simulation using the JAX jit operation is more than 2x faster (3.5x) than the direct NumPy vectorization simulation we did at 0.06 seconds. This is nearly 600x faster than the base matrix-vector simulation."
  },
  {
    "objectID": "blog/posts/HW4/index.html#conclusion-applications-to-deep-learning",
    "href": "blog/posts/HW4/index.html#conclusion-applications-to-deep-learning",
    "title": "JAX-Driven Heat Diffusion: Sparse and Vectorized PDE Simulation",
    "section": "Conclusion: Applications to Deep Learning",
    "text": "Conclusion: Applications to Deep Learning\nAlthough this blog focuses on simulating heat diffusion, the computational techniques used are directly applicable to deep learning and neural networks. The methods explored, matrix operations, sparse representations, vectorized computations, and JIT compilation, are fundamental to training and optimizing deep learning models."
  },
  {
    "objectID": "blog/posts/HW2/index.html",
    "href": "blog/posts/HW2/index.html",
    "title": "Data-Driven Movie Recommendations: Scrapy Web Scraping and Actor Network Graphs with NetworkX",
    "section": "",
    "text": "Network Graph of Movie Data"
  },
  {
    "objectID": "blog/posts/HW2/index.html#introduction",
    "href": "blog/posts/HW2/index.html#introduction",
    "title": "Data-Driven Movie Recommendations: Scrapy Web Scraping and Actor Network Graphs with NetworkX",
    "section": "Introduction",
    "text": "Introduction\nThe TMDB (The Movie Database) is a comprehensive online database that provides detailed information on movies, TV shows, actors, and production crews. In this project, we will develop a Scrapy-based web crawler to extract movie and actor data from TMDB. The crawler is designed to navigate from a movie’s main page to its full cast list, retrieve actor profiles, and extract their filmographies. We will employ CSS selectors for precise data extraction, implement depth-controlled crawling to avoid unnecessary requests, and optimize request handling to improve performance. Finally, we will use the extracted data to identify patterns in shared actor collaborations across movies using NetworkX. Through this approach, we demonstrate how Scrapy can be leveraged to build a structured dataset for analyzing relationships within the film industry."
  },
  {
    "objectID": "blog/posts/HW2/index.html#setting-up-the-webscraper",
    "href": "blog/posts/HW2/index.html#setting-up-the-webscraper",
    "title": "Data-Driven Movie Recommendations: Scrapy Web Scraping and Actor Network Graphs with NetworkX",
    "section": "Setting Up the Webscraper",
    "text": "Setting Up the Webscraper\nIn this section, we’ll first initialize the Scrapy framework, examine how TMDB organizes its data, and prepare our project for extracting meaningful movie-actor relationships.\n\nInitializing the Scrapy Project\nWe’ll start by creating a new Scrapy project.\nWe can do this by opening the terminal and activating the relevant Python environment:\n\nconda activate PIC16B-25W\n\nNext, create a Scrapy project by running:\n\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\n\nRunning this will create a folder named “TMDB_scraper” with the following contents inside:\n\nTMDB_scraper/\n│── scrapy.cfg                \n└── TMDB_scraper/\n    ├── spiders/              \n    │   └── tmdb_spider.py    # Our main spider script\n    ├── settings.py           # Scrapy settings file\n    ├── middlewares.py        \n    ├── pipelines.py          \n    └── __init__.py           \n\nWe’ll only really need the tmdb_spider.py file, where we will write the logic for our scraper. We’ll also make minor changes to the setting of our spider in settings.py.\n\n\nUnderstanding the TMDB Website Structure\nIn order to extract the correct data, we need to analyze how TMDB organizes and displays its information. A movie page typically exhibits the following structure:\nMovie Starting Page Once on a movie’s starting page, we observe that the URL contains a movie ID. Our project will focus on Harry Potter and the Sorcerer’s Stone. the TMDB URL for Harry Potter and the Sorcerer’s Stone is:\n\nhttps://www.themoviedb.org/movie/671-harry-potter-and-the-philosopher-s-stone/\n\nThe movie ID 671-harry-potter-and-the-philosopher-s-stone will be passed as an argument to our scraper.\nNavigate to the Cast & Crew Page Next we want to click on the “Full Cast & Crew” link which takes us to the following page:\n\nhttps://www.themoviedb.org/movie/671-harry-potter-and-the-philosopher-s-stone/cast\n\nNotice that the URL is the same as the previous page, except for the /cast appended to the end. This page is where our spider will extract the list of actors (ignoring crew members).\nExtracting Actor Profiles Clicking on an actor’s name leads to the actor profile page:\n\nhttps://www.themoviedb.org/person/10980-daniel-radcliffe\n\nOn this page, we need to find the “Acting” section, which contains a list of movies and TV shows the actor has performed in. There are multiple sections such as Production, Writing, and Acting. We must only extract movie titles listed under “Acting”.\n\n\nConfiguring the Scrapy Crawler Settings\nTo prevent excessive data requests while we are testing the crawler, we’ll temporarily add the following line in settings.py:\n\nCLOSESPIDER_PAGECOUNT = 20\n\nThis stops the spider after 20 pages so we don’t accidentally send too many requests while debugging.\nWhen scraping, we’ll look for the 200 status. If we run into the 403 Forbidden error when TMDB detects that we’re a bot. A simple fix is changing the User-Agent in settings.py:\n\nUSER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36\"\n\nThis makes our scraper appear as a normal browser request, reducing the chance of being blocked."
  },
  {
    "objectID": "blog/posts/HW2/index.html#building-the-scrapy-spider",
    "href": "blog/posts/HW2/index.html#building-the-scrapy-spider",
    "title": "Data-Driven Movie Recommendations: Scrapy Web Scraping and Actor Network Graphs with NetworkX",
    "section": "Building the Scrapy Spider",
    "text": "Building the Scrapy Spider\nNow that our Scrapy project is set up and we have analyzed TMDB website structure, we can implement the three parsing methods in our spider:\n\nparse(): Navigates from the movie page to the Full Cast & Crew page.\nparse_full_credits(): Extracts the list of actors in the movie.\nparse_actor_page(): Extracts movies from the actor’s Acting section.\n\nEach of these methods will systematically extract the data we need. We’ll write them into the tmdb_spider.py file.\n\nNavigating to the Cast & Crew Page\nparse(): The parse() method starts from a movie’s main page and navigates to its Full Cast & Crew page. Since the cast page always follows the pattern &lt;movie_url&gt;/cast, we can simply append /cast to the current URL.\nImplementation:\n\ndef parse(self, response):\n    \"\"\"\n    Navigates from the movie page to the Full Cast & Crew page.\n    \"\"\"\n    cast_url = response.url + \"/cast\" # Appends \"/cast\" to the end of the current URL.\n    \n    # Creates a Scrapy request to Cast URL and calls parse_full_credits().\n    yield scrapy.Request(url=cast_url, callback=self.parse_full_credits)\n\nThis method works by:\n\nExtracting the Full Cast & Crew URL by appending “/cast” to the movie’s base URL.\nCreating a Scrapy request to that URL and calling parse_full_credits() once the page is loaded.\n\nThis method does not extract any data but ensures we reach the correct page for scraping actors.\n\n\nExtracting Actor Links\nparse_full_credits(): Once we reach the Full Cast & Crew page, we need to extract only the actors listed in the cast section.\nImplementation:\n\ndef parse_full_credits(self, response):\n    \"\"\"\n    Extracts actors from the Full Cast & Crew page \n    and yields requests for each actor’s page.\n    \"\"\"\n    # Selects all actor profile links &lt;a&gt; elements \n    # inside the cast list &lt;ol class=\"people credits\"&gt;.\n    # Extracts the href attribute, which contains the link to the actor’s TMDB page.\n    actor_links = response.css('ol.people.credits li[data-order] div.info a[href^=\"/person/\"]::attr(href)').getall()\n\n    # Yields a new request for each actor’s profile page, calling parse_actor_page().\n    for link in actor_links:\n        yield scrapy.Request(url=response.urljoin(link), callback=self.parse_actor_page)\n\nThis method works by:\n\nSelecting all actor profile links &lt;a&gt; elements inside the cast list &lt;ol class=\"people credits\"&gt;.\nExtracting the href attribute, containing the link to the actor’s TMDB page.\nYielding a new request for each actor’s profile page, calling parse_actor_page() for further processing.\n\nThis method ensures that only actors (not crew members) are processed.\n\n\nExtracting Movie and TV Show Titles\nparse_actor_page() On an actor’s TMDB page, movies and TV shows are categorized under multiple sections like Acting, Writing, Directing, and Producing. We must extract only titles under the “Acting” section.\nImplementation:\n\ndef parse_actor_page(self, response):\n        \"\"\"\n        Extracts actor's name and their acting roles, yielding unique movie/TV titles.\n        \"\"\"\n        # Selector extracts actor's name\n        actor_name = response.css(\"h2.title a::text\").get()\n        \n        # Selector extracts section headers\n        section_headers = response.css(\"div.credits_list h3::text\").getall()\n\n        # Locates the \"Acting\" section\n        acting_index = section_headers.index(\"Acting\")\n\n        # Get all tables and extract titles from the Acting section\n        all_tables = response.css(\"div.credits_list table.card\")\n        acting_table = all_tables[acting_index]\n        titles = acting_table.css(\"a.tooltip bdi::text\").getall()\n\n        for movie_or_TV_name in titles:\n            yield {\"actor\": actor_name, \"movie_or_TV_name\": movie_or_TV_name}\n\nThis method works by:\n\nExtracting the actor’s name from the &lt;h2 class=\"title\"&gt; section.\nFinding all section headers (e.g., “Production”, “Acting”) to locate the Acting section.\nFinding the matching table for “Acting” and extracting all movie/TV show titles inside.\nYielding a dictionary containing (actor, movie/TV show) pairs.\n\nThis method ensures that only acting credits are included.\n\n\nRunning the Scraper\nBefore we run our scraper, we can check that our CSS selectors are working by running this in the terminal:\n\nscrapy shell \"your_page_link\"\n\nWithin the shell for each page, we can run each selector in our three parse methods to see if it yields the desired output.\nAfter checking our selectors, we are ready to run the scraper with the following line:\n\nscrapy crawl tmdb_spider -o results.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\n\nThis command will:\n\nStart at the Harry Potter and the Sorcerer’s Stone movie page.\nExtract all actors from the Full Cast & Crew page.\nVisit each actor’s profile and extract all movies in which they have acted.\nSave the results in results.csv.\n\nOur CSV file should look like:\n\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nPaul Marc Davis\nArthur & Merlin: Knights of Camelot\n\n\n1\nPaul Marc Davis\nClass\n\n\n2\nPaul Marc Davis\nSon of God\n\n\n3\nPaul Marc Davis\nThe Bible\n\n\n4\nPaul Marc Davis\nThe Sky in Bloom\n\n\n...\n...\n...\n\n\n3121\nRupert Grint\nThe View\n\n\n3122\nRupert Grint\nGMTV\n\n\n3123\nRupert Grint\nThe Tonight Show with Jay Leno\n\n\n3124\nRupert Grint\nAn Audience with...\n\n\n3125\nRupert Grint\nToday\n\n\n\n\n3126 rows × 2 columns\n\n\n\nNow that we have successfully scraped the data, the next step is to analyze the results and create a visualization of shared actors across movies. In the next section, we will:\n\nFilter the data to find movies with the most shared actors.\nCreate a network graph to visually represent relationships between movies and actors."
  },
  {
    "objectID": "blog/posts/HW2/index.html#visualizing-movie-recommendations",
    "href": "blog/posts/HW2/index.html#visualizing-movie-recommendations",
    "title": "Data-Driven Movie Recommendations: Scrapy Web Scraping and Actor Network Graphs with NetworkX",
    "section": "Visualizing Movie Recommendations",
    "text": "Visualizing Movie Recommendations\nOnce the scraper is complete, we need to analyze the results and present them in a meaningful way. Since our dataset consists of movies and actors, a network graph is an intuitive choice for visualization. A network graph allows us to see which movies share actors and how strongly they are connected.\n\nProcessing the Scraped Data\nThe CSV output from our Scrapy spider contains two columns:\n\nActor: The name of the actor.\nMovie or TV Show: The name of the movie or TV show they appeared in.\n\nTo build a movie recommendation system, we’ll look for movies that share multiple actors by the following:\n\nGrouping actors by movie: Create a mapping of each movie to the list of actors who starred in it.\nCompute shared actors: Identify pairs of movies that share actors.\nFilter based on shared actor count: We only include movies that share at least 5 actors in the final visualization.\n\n\n\nBuilding the Network Graph\nWe’ll use NetworkX to create a graph where:\n\nNodes represent movies and actors.\nEdges exist between a movie node and an actor node if the actor starred in the movie.\nMovies with more shared actors are emphasized by larger node size and distinct colors.\n\nAdditionally, in order to enhance readability, we’ll apply:\n\nNode scaling to make movies with more shared actors appear larger.\nColor encoding to assign colors to movie nodes using a thermal colormap, where warmer colors (yellow) indicate more shared actors and cooler colors less.\n\n\n\nDeploying the Movie Network Graph\nFirst, we’ll need to import the necessary packages to create our visualization:\n\nimport networkx as nx\nimport plotly.graph_objects as go\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nNext, we will write a function that will take our CSV file and a minimum number of shared actors per movie as inputs (only recommend movies and TV shows with high actor correlation).\nThe following function plots our interactive movie network graph with our desired specs:\n\ndef generate_movie_actor_network(csv_file, min_shared_actors):\n    \"\"\"\n    Generates a movie-actor network graph from a CSV file containing movie and actor relationships.\n    \n    \"\"\"\n\n    # Loads in the dataset\n    df = pd.read_csv(csv_file)\n\n    # Create a graph\n    G = nx.Graph()\n\n    # Build the mapping of movies to actors\n    movie_actor_map = {}\n    for _, row in df.iterrows():\n        movie = row[\"movie_or_TV_name\"]\n        actor = row[\"actor\"]\n        if movie not in movie_actor_map:\n            movie_actor_map[movie] = set()\n        movie_actor_map[movie].add(actor)\n\n    # Identify valid movies that share at least `min_shared_actors` actors with another movie\n    valid_movies = set()\n    for movie1 in movie_actor_map:\n        for movie2 in movie_actor_map:\n            if movie1 != movie2:\n                shared_actors = movie_actor_map[movie1] & movie_actor_map[movie2]\n                if len(shared_actors) &gt;= min_shared_actors:\n                    valid_movies.add(movie1)\n                    valid_movies.add(movie2)\n\n    # Compute the number of unique actors in each valid movie\n    movie_shared_counts = {\n        movie: len(movie_actor_map[movie]) for movie in valid_movies\n    }\n    max_shared = max(movie_shared_counts.values(), default=1)  # Avoid division by zero\n\n    # Normalize sizes (scale between 10 and 55 for visualization purposes)\n    def scale_size(value, min_size=10, max_size=55):\n        return min_size + ((value - 3) / (max_shared - 3)) * (max_size - min_size) if max_shared &gt; 3 else min_size\n\n    # Assign movie colors based on shared actor count using thermal color scale.\n    norm = plt.Normalize(vmin=min(movie_shared_counts.values(), default=0), vmax=max_shared)\n    thermal_colormap = plt.get_cmap(\"viridis\")\n\n    movie_colors = {\n        movie: f'rgb({int(r*255)},{int(g*255)},{int(b*255)})'\n        for movie, (r, g, b, _) in zip(\n            movie_shared_counts.keys(),\n            [thermal_colormap(norm(v)) for v in movie_shared_counts.values()]\n        )\n    }\n\n    # Add nodes and edges to the graph\n    for movie in valid_movies:\n        G.add_node(movie, type=\"movie\", size=scale_size(movie_shared_counts[movie]), color=movie_colors[movie])\n\n    for movie in valid_movies:\n        for actor in movie_actor_map[movie]:\n            if actor not in G:\n                G.add_node(actor, type=\"actor\", size=8, color=\"lightgray\")  # Smaller nodes for actors\n            G.add_edge(movie, actor)\n\n    # Compute graph layout for better spacing\n    pos = nx.spring_layout(G, seed=42, k=1.7)  # 'k' controls the node spacing\n\n    # Create scatter plot for nodes\n    node_trace = []\n    for node in G.nodes():\n        node_type = G.nodes[node][\"type\"]\n        x, y = pos[node]\n        color = G.nodes[node][\"color\"]\n        size = G.nodes[node][\"size\"]\n\n        # Set hover text for nodes\n        hover_text = node if node_type == \"actor\" else f\"{node}&lt;br&gt;Shared Actors: {movie_shared_counts[node]}\"\n\n        node_trace.append(go.Scatter(\n            x=[x], y=[y],\n            mode=\"markers\" if node_type == \"actor\" else \"markers+text\",\n            marker=dict(size=size, color=color, opacity=0.9, showscale=False),\n            hovertext=hover_text,\n            textposition=\"top center\" if node_type == \"movie\" else None\n        ))\n\n    # Create edges for connections between movies and actors\n    edge_trace = []\n    for edge in G.edges():\n        x0, y0 = pos[edge[0]]\n        x1, y1 = pos[edge[1]]\n        edge_trace.append(go.Scatter(\n            x=[x0, x1, None], y=[y0, y1, None],\n            mode=\"lines\",\n            line=dict(width=1, color=\"gray\"),\n            hoverinfo=\"none\",\n            opacity=0.2  # Light transparency for better visibility\n        ))\n\n    # Add a color bar to indicate shared actors\n    colorbar_trace = go.Scatter(\n        x=[None], y=[None],\n        mode='markers',\n        marker=dict(\n            colorscale=\"viridis\",  # Thermal color scheme\n            cmin=min(movie_shared_counts.values(), default=0),\n            cmax=max_shared,\n            showscale=True,\n            colorbar=dict(\n                title=\"# of Shared Actors\",\n                titleside=\"right\",  # Aligns title to the right\n                tickmode=\"array\",\n                tickvals=list(range(0, int(max_shared) + 1, 10)),\n                ticktext=[str(tick) for tick in range(0, int(max_shared) + 1, 10)],\n                tickfont=dict(size=10),  \n                len=1,  \n                thickness=20,  \n                outlinewidth=1.3,\n                xpad=6,  \n                x=0.97  \n            )\n        ),\n        hoverinfo='none'\n    )\n\n    # Create final figure\n    fig = go.Figure(data=edge_trace + node_trace + [colorbar_trace])\n    fig.update_layout(\n        title=dict(\n            text=\"Movie Recs: Movie-Actor Network Graph\",\n            x=0.427,  \n            xanchor=\"center\",\n            yanchor=\"top\",\n            font=dict(size=20)  \n        ),\n        showlegend=False,\n        hovermode=\"closest\",\n        margin=dict(b=20, l=5, r=140, t=40),\n        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n        plot_bgcolor=\"white\",  \n        paper_bgcolor=\"white\"  \n    )\n\n    # Display the graph\n    return fig\n\nWe can call our plotting function with our CSV and minimum shared actors by running the following:\n\nfig = generate_movie_actor_network(csv_file = \"results.csv\", \n                                   min_shared_actors=5)\n\nfig.show()\n\n\n\n\n\n\nInterpretting the Visualization\nBy analyzing the graph, we can identify:\n\nHighly interconnected movies: Franchises and sequels (e.g., Harry Potter Series) tend to cluster together.\nUnexpected connections: Some movies share actors despite belonging to different genres.\nStrongest recommendations: The best recommendations are movies that share the most actors with the chosen starting movie.\n\nThis approach demonstrates how web scraping and network analysis can be combined to generate meaningful movie recommendations based purely on cast overlap, without using machine learning.\nResources Used (For NetworkX Visualization):\n\nhttps://plotly.com/python/network-graphs/\nhttps://community.plotly.com/t/is-plotly-able-to-integrate-with-networkx-graph/59115"
  },
  {
    "objectID": "blog/posts/Final_Project/index.html#introduction",
    "href": "blog/posts/Final_Project/index.html#introduction",
    "title": "YieldCurveForecaster: 3D Yield Curve Modeling and Forecasting Dashboard",
    "section": "Introduction",
    "text": "Introduction\nA yield curve is a graph that shows the interest rates (yields) of bonds with different maturity dates. It typically represents U.S. Treasury bonds and helps indicate the overall health of the economy.\n\nShort-term bonds (e.g., 3 months) usually have lower yields.\nLong-term bonds (e.g., 30 years) usually have higher yields (risk premium)\n\nShapes tell economic stories:\n\nNormal (upward): Healthy economy\nFlat: Economic transition\nInverted: Recession signal\n\nThis project aims to model the yield curve in three-dimensions, as well as forecast yields using exogenous variables to formulate “regime-awareness” for classic forecasting models (i.e. Nelson-Siegel Models).\nGitHub Repository Link"
  },
  {
    "objectID": "blog/posts/Final_Project/index.html#project-overview",
    "href": "blog/posts/Final_Project/index.html#project-overview",
    "title": "YieldCurveForecaster: 3D Yield Curve Modeling and Forecasting Dashboard",
    "section": "Project Overview",
    "text": "Project Overview\nThe project is split up into three main categories:\n\nData Management: The acquisition of data from FRED API (Yield data, CPI, VIX), SQLite database formation, and SQL query functions.\nForecasting Models (Mathematical Modeling): The forecasting models (dynamic exponential decay, AFNS, etc.).\nBacktesting & Analysis: The functions used to backtest each model and calculate the error metrics/statistics.\n\nThere are other important modules such as the plotting module and dash app module, but for the sake of the entirety of the project, the forecasting module was the main focus of work and research.\n\nAt the project’s foundation, lies an SQLite database that stores historical yield curve data, inflation metrics, and market volatility indices. The data layer (data.py) handles all the database operations using parameterized queries and includes ETL (Extract, Transform, Load) functionality. This ensures that time-based and maturity-related data is cleaned up and standardized, making it ready for use down the line. The mathematical modeling layer (models.py and utils.py) implements the Nelson-Siegel framework and “regime-awareness” forecasting algorithms, containing the complex mathematics of yield curve dynamics. The analytics layer (analysis.py) provides backtesting capabilities and statistical analysis functions that operate on both historical and forecasted data.\nThe project’s visualizations and user interactions are built on Dash, implementing a reactive programming model where UI components respond to user inputs through callback functions. This setup ensures a smooth and connected workflow, where any changes you make to the parameters in the interface automatically flow through the entire system. It triggers the necessary database queries, runs the right modeling functions, and updates the visualizations seamlessly—all in real time. The application’s state management system ensures that computational resources are used efficiently, with heavy operations like backtesting performed only when explicitly requested."
  },
  {
    "objectID": "blog/posts/Final_Project/index.html#technical-components",
    "href": "blog/posts/Final_Project/index.html#technical-components",
    "title": "YieldCurveForecaster: 3D Yield Curve Modeling and Forecasting Dashboard",
    "section": "Technical Components",
    "text": "Technical Components\nThe main technical components include:\n\nCreating and Interacting with a SQL Database\nComplex Data Visualization via Plotly\nMathematical Modeling\nBuilding a Dynamic Dash App\n\n\nCreating and Interacting with a SQL Database\nThis project sets up a data storage system using an SQLite database, complete with custom query functions designed to pull time-based financial data efficiently. Unlike simpler methods like relying on CSV files, this database is structured to fetch only the exact data required for specific analyses. This approach minimizes memory usage, especially when working with large datasets that cover several decades.\nThe database is organized into separate tables for yield curve data, market volatility metrics, and inflation, all using consistent date formats to ensure everything lines up correctly over time. To keep things secure and flexible, the data access layer uses parameterized queries, which guard against SQL injection risks while offering versatile ways to retrieve the data you need.\nThe first step in the project was to fetch the data from FRED API. The update_db.py module contains multiple functions to fetch data but we’ll focus on fetching the yield data:\n\ndef fetch_treasury_yields(start_date, end_date):\n    \"\"\"\n    Fetch Treasury yield data from FRED for various maturities.\n    \n    Returns:\n        DataFrame with date index and columns for different maturities.\n    \"\"\"\n    logger.info(f\"Fetching Treasury yields from {start_date} to {end_date}\")\n    \n    # FRED series IDs for different Treasury maturities\n    # Format: (maturity_label, series_id)\n    treasury_series = [\n        ('T1M', 'DGS1MO'),   # 1-Month Treasury Constant Maturity Rate\n        ('T3M', 'DGS3MO'),   # 3-Month Treasury Constant Maturity Rate\n        ('T6M', 'DGS6MO'),   # 6-Month Treasury Constant Maturity Rate\n        ('T1', 'DGS1'),      # 1-Year Treasury Constant Maturity Rate\n        ('T2', 'DGS2'),      # 2-Year Treasury Constant Maturity Rate\n        ('T5', 'DGS5'),      # 5-Year Treasury Constant Maturity Rate\n        ('T10', 'DGS10'),    # 10-Year Treasury Constant Maturity Rate\n        ('T30', 'DGS30')     # 30-Year Treasury Constant Maturity Rate\n    ]\n    \n    # Initialize dictionary to store series data\n    yields_data = {}\n    \n    # Fetch each series\n    for label, series_id in treasury_series:\n        try:\n            # Get data from FRED API\n            series = fred.get_series(series_id, start_date, end_date)\n            if not series.empty:\n                yields_data[label] = series\n                logger.info(f\"Successfully fetched {label} yields ({len(series)} observations)\")\n            else:\n                logger.warning(f\"No data returned for {label} (series ID: {series_id})\")\n        except Exception as e:\n            logger.error(f\"Error fetching {label} (series ID: {series_id}): {str(e)}\")\n    \n    # Convert to DataFrame\n    if yields_data:\n        df = pd.DataFrame(yields_data)\n        df.index.name = 'date'\n        \n        # Convert index to datetime if not already\n        if not isinstance(df.index, pd.DatetimeIndex):\n            df.index = pd.to_datetime(df.index)\n            \n        return df\n    else:\n        logger.error(\"Failed to fetch any Treasury yield data\")\n        return pd.DataFrame()\n\nThe main challenge stemmed from dealing with multiple yield maturities fetched from the FRED API. Since variable names can’t start with an integer, I had to rename each maturity by adding a “T” at the beginning. This adjustment later necessitated a custom function to convert these specially named maturity variables back into numerical values for analysis. Additionally, the data had to be concatenated and reshaped (using a “melt” operation) to ensure it was properly structured for the data.py module to query and process efficiently.\nThe data.py module houses all the query functions, some of which require more than just querying between two dates. An example of this is the get_historical_inflation() function, which not only retrieves CPI data but also computes year-over-year inflation rates directly in the query:\n\ndef get_historical_inflation(db_file: str, start_date: str, end_date: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Fetch historical inflation data and compute Year-over-Year (YoY) CPI change.\n\n    Args:\n        db_file (str): Path to SQLite database.\n        start_date (str): Start date in 'YYYY-MM-DD' format.\n        end_date (str): End date in 'YYYY-MM-DD' format.\n\n    Returns:\n        pd.DataFrame: DataFrame with ['date', 'CPI', 'inflation_rate'].\n    \"\"\"\n    query = \"\"\"\n        SELECT date, CPI\n        FROM Inflation\n        WHERE date BETWEEN ? AND ?\n        ORDER BY date ASC\n    \"\"\"\n    try:\n        with sqlite3.connect(db_file) as conn:\n            df = pd.read_sql_query(query, conn, params=(start_date, end_date))\n        \n        df = df.sort_values(\"date\")\n        df[\"inflation_rate\"] = df[\"CPI\"].pct_change(12) * 100  # Compute YoY inflation rate\n        \n        return df.dropna()\n    except Exception as e:\n        print(f\"Error querying inflation data: {e}\")\n        return pd.DataFrame()\n\n\nAll in all, the data management architecture is relatively straightforward, with importing and handling our financial time-series data as the main goal. The database currently has three tables for yields, CPI, and VIX data but can easily be expanded to include more exogenous variables to incorporate into improving the “regime-awareness” hypothesis.\n\n\nComplex Data Visualization via Plotly\nThe project implements sophisticated three-dimensional visualizations using Plotly, creating interactive representations of yield curve surfaces that evolve over time. The 3D visualization features both historical as well as the forecasted yields. Utilizing the 3D surface visualizations in this way, allow us to go beyond two-dimensional charts to represent the multi-dimensional nature of the yield curve.\nThe core visualization function creates a 3D surface representation where:\n\nThe X-axis represents different maturities (from 1-month to 30-year)\nThe Y-axis represents time (both historical and forecasted periods)\nThe Z-axis represents yield values\n\nThe visualization module plotting.py implements a custom 3D surface generation function that creates topographical representations of yield curves for both forecasted and historical data:\n\ndef generate_surface_plot(forecast_model, selected_maturities, db_file, start_date, end_date, vix_data=None):\n    \"\"\"Creates a 3D surface visualization of historical and forecasted yield curves.\n    \n    Purpose:\n    Generates an interactive 3D surface plot showing the evolution of the yield curve\n    over time, including both historical data and forward-looking model forecasts.\n    Selected maturities can be highlighted with individual time series lines.\n    \n    Args:\n        forecast_model: Model name to use for forecasting (\"Heuristic\", \n            \"ImprovedHeuristic\", or \"RLS\").\n        selected_maturities: List of maturities to highlight with individual lines.\n        db_file: Path to SQLite database with yield data.\n        start_date: Start date for historical data in 'YYYY-MM-DD' format.\n        end_date: End date for historical data in 'YYYY-MM-DD' format.\n        vix_data: Optional DataFrame with VIX data for forecast enhancement.\n        \n    Returns:\n        go.Figure: Plotly figure object containing the 3D yield curve visualization.\n        \n    Raises:\n        Exception: If an error occurs during plot generation.\n    \"\"\"\n    try:\n        # Process historical data with memory efficiency in mind\n        historical_df = data.prepare_data(db_file, start_date, end_date)\n        historical_df.index = pd.to_datetime(historical_df.index)\n        \n        # For large datasets, subsample to improve performance while preserving trends\n        if len(historical_df) &gt; 200:\n            historical_df = historical_df.iloc[::2]  # Take every other row\n            \n        # Prepare maturity data for consistent plotting\n        available_maturities = [round(float(col), 4) for col in historical_df.columns]\n        sorted_maturities = sorted(available_maturities, reverse=True)\n        historical_df = historical_df.reindex(columns=config.MATURITIES_LIST)[sorted_maturities]\n        \n        # Convert dates to numeric values for 3D plotting\n        base_date = historical_df.index.min()\n        hist_dates_numeric = (historical_df.index - base_date).days\n        \n        # Create meshgrid for 3D surface - positions represent maturity indices\n        positions = np.arange(len(sorted_maturities))\n        X_hist, Y_hist = np.meshgrid(positions, hist_dates_numeric)\n        Z_hist = historical_df.fillna(0).values\n        \n        # Adapt forecast length based on visualization complexity\n        forecast_steps = 24  # Standard value: 2-year forecast\n        if len(selected_maturities) &gt; 5:\n            # Shorter forecast horizon when displaying many maturity lines\n            forecast_steps = 12\n            \n        # Generate forecast data using the selected model\n        forecast_df = models.forecast_yield_curve(\n            forecast_model, db_file, start_date, end_date, \n            sorted_maturities, forecast_steps=forecast_steps, vix_data=vix_data\n        )\n        \n        # Prepare forecast data for 3D visualization\n        forecast_df[\"maturity_numeric\"] = forecast_df[\"maturity_numeric\"].astype(float)\n        forecast_pivot = forecast_df.pivot(index=\"date\", columns=\"maturity_numeric\", values=\"yield\")\n        forecast_pivot = forecast_pivot.reindex(columns=sorted_maturities).ffill().bfill().fillna(0)\n        \n        # Convert forecast dates to numeric values for plotting\n        forecast_dates_numeric = (forecast_pivot.index - base_date).days.values\n        X_fore, Y_fore = np.meshgrid(positions, forecast_dates_numeric)\n        Z_fore = forecast_pivot.values\n\n        # Adapt surface opacity based on selection mode\n        surface_opacity = 0.05 if selected_maturities else 0.6\n\n        # Create the base figure\n        fig = go.Figure()\n        \n        # Add historical yield curve surface (blue gradient)\n        fig.add_trace(go.Surface(\n            x=X_hist, y=Y_hist, z=Z_hist,\n            colorscale=\"Blues\",\n            opacity=surface_opacity,\n            name=\"Historical Yield Curve\",\n            showscale=False\n        ))\n        \n        # Add forecast yield curve surface (orange-cyan-blue gradient)\n        custom_scale = [[0, 'darkorange'], [0.5, 'cyan'], [1, 'darkblue']]\n        fig.add_trace(go.Surface(\n            x=X_fore, y=Y_fore, z=Z_fore,\n            colorscale=custom_scale,\n            opacity=surface_opacity,\n            name=\"Forecast Yield Curve\",\n            showscale=False\n        ))\n        \n        # Add individual time series lines for selected maturities\n        if selected_maturities:\n            for m in selected_maturities:\n                m_val = round(float(m), 4)\n                if m_val not in sorted_maturities:\n                    continue\n                \n                # Extract historical data for this maturity\n                hist_ts = historical_df[m_val].reset_index()\n                hist_ts.rename(columns={m_val: \"yield\"}, inplace=True)\n                \n                # Get forecast for this specific maturity\n                forecast_ts = models.forecast_individual_maturity(\n                    db_file, start_date, end_date, m_val, \n                    model=forecast_model, vix_data=vix_data\n                )\n                \n                # Combine historical and forecast for continuous visualization\n                combined_ts = pd.concat([hist_ts, forecast_ts], ignore_index=True)\n                combined_ts.sort_values(\"date\", inplace=True)\n                combined_ts['date_str'] = combined_ts['date'].dt.strftime(\"%m-%Y\")\n                combined_ts['date_numeric'] = (combined_ts['date'] - base_date).dt.days\n                \n                # Add the maturity-specific line to the 3D plot\n                pos = sorted_maturities.index(m_val)\n                fig.add_trace(go.Scatter3d(\n                    x=[pos] * len(combined_ts),  # Fixed x position for this maturity\n                    y=combined_ts['date_numeric'],  # Date position\n                    z=combined_ts['yield'],  # Yield values\n                    mode='lines',\n                    line=dict(color='black', width=1.1),\n                    name=f\"{int(m_val)}Y\" if m_val &gt;= 1 else f\"{int(round(m_val*12))}M\",\n                    # Enhanced hover information for financial analysis\n                    hovertemplate=\"&lt;b&gt;Maturity:&lt;/b&gt; \" + f\"{m_val} years\" +\n                                \"&lt;br&gt;&lt;b&gt;Date:&lt;/b&gt; %{customdata[0]}\" +\n                                \"&lt;br&gt;&lt;b&gt;Yield:&lt;/b&gt; %{z:.2f}%\",\n                    customdata=combined_ts[['date_str']].values\n                ))\n        \n        # Configure time axis with year labels\n        end_date_val = forecast_pivot.index.max() if not forecast_pivot.empty else historical_df.index.max()\n        year_ticks = pd.date_range(start=base_date, end=end_date_val, freq='YS')\n        y_tick_vals = [(date - base_date).days for date in year_ticks]\n        y_tick_text = [date.strftime(\"%Y\") for date in year_ticks]\n            \n        # Set layout with optimized aspect ratio and clear axis labels\n        fig.update_layout(\n            scene=dict(\n                xaxis=dict(\n                    title=\"Maturity\",\n                    tickvals=list(range(len(sorted_maturities))),\n                    ticktext=[f\"{int(m)}Y\" if m &gt;= 1 else f\"{int(round(m*12))}M\" for m in sorted_maturities]\n                ),\n                yaxis=dict(\n                    title=\"Time\",\n                    tickvals=y_tick_vals,\n                    ticktext=y_tick_text\n                ),\n                zaxis=dict(title=\"Yield (%)\"),\n                aspectratio=dict(x=1, y=2, z=0.7)\n            ),\n            title_text=f\"Historical & Forecast Yield Curves ({forecast_model} Model)\",\n            margin=dict(l=0, r=0, b=10, t=30),\n            legend=dict(\n                yanchor=\"top\",\n                y=0.99,\n                xanchor=\"left\",\n                x=0.01\n            )\n        )\n        \n        # Clean up large intermediate objects to free memory\n        del X_hist, Y_hist, Z_hist, X_fore, Y_fore, Z_fore\n        gc.collect()\n        \n        return fig\n    except Exception as e:\n        # Log the error for debugging\n        print(f\"Error in generate_surface_plot: {str(e)}\")\n        raise\n\nSome of the challenges that arose when plotting the 3D yield curve came from concatenating the historical and forecasted yield graphs. Making sure that the forecasting graph exhibited continuity (no large gap) was a reoccurring issue.\nOne of the features I wanted the 3D visualization to have was the ability for users to select a specific maturity and be able to view it without the obstruction of the entire three-dimensional plot. Thus, the rendering engine also dynamically adjusts opacity levels based on user-selected maturity, creating a focus effect where the surface becomes semi-transparent when specific yield curves are highlighted.\nThis complex visualization approach transforms numerical yield data into an intuitive visual format that makes patterns, anomalies, and trends immediately apparent (financial shocks like COVID-19 can be easily visible).\nBeyond the 3D visualizations, the plotting.py module contains specialized plot functions for yield curve spreads, inversion analysis, and volatility correlations. These visualizations implement custom color-coding schemes that map domain-specific concepts like recession probabilities to visual elements that financial analysts can readily interpret. These visualizations accompany the main 3D yield curve to better understand how they work with yield curve dynamics.\n\n\n\nMathematical Modeling\nThe core of the application lies in its mathematical modeling capabilities. The system implements multiple forecasting approaches, each with specific strengths for different market environments.\nWe’ll focus on the “Improved-Heuristic” model which utilized CPI and VIX data to create factors used in a simple decay function:\n\\[Y_{t+h,m} = Y_{t,m} \\cdot (1-d_{m,t})^h + \\alpha_{m,h} \\cdot (M_{m,t} - Y_{t,m} \\cdot (1-d_{m,t})^h) + \\epsilon_{t,h,m}\\]\nWhere: - \\(Y_{t,m}\\) is the yield at time \\(t\\) for maturity \\(m\\) - \\(h\\) is the forecast horizon in months - \\(d_{m,t}\\) is the effective decay rate, varying by maturity and time - \\(\\alpha_{m,h}\\) is the horizon-dependent mean reversion strength - \\(M_{m,t}\\) is the maturity-specific mean reversion target - \\(\\epsilon_{t,h,m}\\) is a noise term based on historical volatility\nMaturity-Specific Decay Rates\nThe model implements non-linear scaling of decay rates across the curve:\n\\[d_{m,t} = d_{max} - (d_{max} - d_{min}) \\cdot \\left(\\frac{m - m_{min}}{m_{max} - m_{min}}\\right)^{\\gamma_m} + \\beta_{infl,t}\\]\nWhere: - \\(d_{max}\\) is the maximum decay rate (for shortest maturities) - \\(d_{min}\\) is the minimum decay rate (for longest maturities) - \\(\\gamma_m\\) is a maturity-dependent power factor (0.7 for short maturities, 0.9 for longer ones) - \\(\\beta_{infl,t}\\) is an inflation-based adjustment\nThe model adjusts decay rates across the curve, applying faster decay for short-term rates and slower decay for long-term rates:\n\n# Compute base decay rate with non-linear scaling for short-term rates\nif m &lt;= 1:  # 1 year or less - faster decay initially\n    base_decay = d_max - (d_max - d_min) * np.power((m - m_min) / (m_max - m_min), 0.7)\nelse:\n    base_decay = d_max - (d_max - d_min) * np.power((m - m_min) / (m_max - m_min), 0.9)\n\nInflation and Volatility Adjustments\nThe inflation adjustment term \\(\\beta_{infl,t}\\) is calculated as:\n\\[\\beta_{infl,t} = \\begin{cases}\n-0.08 - 0.02 \\cdot I'_t & \\text{if } I_t &gt; 4\\% \\text{ and } I'_t &gt; 0 \\\\\n-0.05 - 0.01 \\cdot I'_t & \\text{if } I_t &gt; 3\\% \\text{ and } I'_t &gt; 0 \\\\\n-0.03 & \\text{if } 1\\% \\leq I_t &lt; 3\\% \\\\\n-0.015 & \\text{if } 0 \\leq I_t &lt; 1\\% \\\\\n-0.005 + 0.005 \\cdot |I'_t| & \\text{if } I_t &lt; 0\\% \\text{ and } I'_t &lt; 0\n\\end{cases}\\]\nWhere: - \\(I_t\\) is the current inflation rate - \\(I'_t\\) is the inflation momentum (change over recent periods)\nThis is further adjusted by market volatility:\n\\[\\beta_{infl,t} = \\beta_{infl,t} \\cdot V_f\\]\nWhere \\(V_f\\) is a volatility adjustment factor:\n\\[V_f = \\begin{cases}\n1.3 & \\text{if VIX} &gt; 30 \\\\\n1.1 & \\text{if VIX} &gt; 20 \\\\\n0.9 & \\text{if VIX} &lt; 15 \\\\\n1.0 & \\text{otherwise}\n\\end{cases}\\]\nHere, we implement the math using simple else-if statements to refine our bias estimator:\n\n# Base bias calculation with momentum factor - higher inflation = more negative bias\nif recent_inflation &gt; 4:  # Very high inflation\n    base_bias = -0.08 - (0.02 * inflation_trend if inflation_trend &gt; 0 else 0)\nelif recent_inflation &gt; 3:  # High inflation\n    base_bias = -0.05 - (0.01 * inflation_trend if inflation_trend &gt; 0 else 0)\nelif recent_inflation &gt;= 1:  # Normal inflation\n    base_bias = -0.03\n\nDynamic Mean Reversion\nThe mean reversion target \\(M_{m,t}\\) combines historical averages with economic factors:\n\\[M_{m,t} = w_{s,m} \\cdot \\mu_{s,m} + w_{m,m} \\cdot \\mu_{m,m} + w_{l,m} \\cdot \\mu_{l,m}\\]\nWhere: - \\(\\mu_{s,m}\\), \\(\\mu_{m,m}\\), \\(\\mu_{l,m}\\) are short, medium, and long-term historical means - \\(w_{s,m}\\), \\(w_{m,m}\\), \\(w_{l,m}\\) are maturity-specific weights\nThis target is then adjusted by economic factors:\n\\[M_{m,t} = M_{m,t} \\cdot (1 + \\delta_m \\cdot E_t)\\]\nWhere: - \\(E_t\\) is a composite economic factor score - \\(\\delta_m\\) is a maturity-dependent sensitivity (ranging from 0.01 to 0.05)\nNoise Generation\nThe noise term incorporates: 1. Historical volatility 2. Maturity-specific scaling 3. Time horizon effects 4. Volatility regime adjustments\n\\[\\epsilon_{t,h,m} = \\sigma_{m} \\cdot e^{-0.2m} \\cdot \\sqrt{\\min(1, \\frac{h}{12})} \\cdot V_f \\cdot \\mathcal{N}(0.002, 1)\\]\nWhere: - \\(\\sigma_m\\) is the historical standard deviation for maturity \\(m\\) - \\(e^{-0.2m}\\) provides exponential decay in noise with maturity - The square root term increases noise with forecast horizon - \\(V_f\\) is the volatility regime factor - \\(\\mathcal{N}(0.002, 1)\\) is a slightly biased normal distribution\nThis mathematical framework enables the model to adapt to changing market conditions while maintaining economically reasonable forecasts across different maturities and time horizons.\nIn addition to all this, we can classify the variables into simple regimes by incorporating the yield curve spreads (calculated by taking the difference of yields between long and short term maturities):\n\n# Determine curve regime based on inversion status and change direction\nif is_inverted:\n    if spread_change &gt; 0.05:\n        # Inverted but steepening - often signals end of tightening\n        regime_info['curve_regime'] = \"inverted_steepening\"\n    else:\n        # Strongly inverted - typical of late-cycle\n        regime_info['curve_regime'] = \"inverted_flat\"\nelse:\n    if spread_change &gt; 0.1:\n        # Rapidly steepening - often signals easing\n        regime_info['curve_regime'] = \"steepening\"\n    elif spread_change &lt; -0.1:\n        # Flattening - typical of tightening\n        regime_info['curve_regime'] = \"flattening\"\n    else:\n        regime_info['curve_regime'] = \"normal\"\n\nThis regime awareness allows the model to account for business cycle dynamics, adjusting forecasts based on whether the market is in a tightening cycle, easing cycle, or transitioning between regimes.\nThus, combining all the exogenous variables with an easier-to-work-with model (exponential decay) was the focus of research throughout the majority of this project. While results are still not on par with the widespread statistical forecasting models (AFNS), the error metric results are seemingly decent enough that further research likely could reveal a statistically significant improvement in yield curve forecasting.\n\n\nBuilding a Dynamic Dash App\nThe Dash application integrates all the components into a cohesive, interactive user interface. It’s built using a modular architecture that separates concerns between data access, model computation, and presentation, allowing for easier maintenance and future enhancements. The application features three main tabs:\n\nVisualization Tab: Interactive 3D visualization of historical and forecasted yield curves\nBacktesting Tab: Model evaluation and performance metrics across different time horizons\nAnalysis Tab: Statistical analysis, spread visualization, and market regime detection\n\nThe application uses a modular design pattern that separates concerns between different functional areas:\n(Note: The code for the Dash app in dashboard.py is long, thus snippets will be highlighted)\n\ndef create_app():\n    \"\"\"Creates and configures the Dash application for yield curve analysis.\"\"\"\n    # Initialize Dash app with Bootstrap styling\n    app = Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n\n    # Define the application layout with multiple tabs\n    app.layout = html.Div([\n        # Navigation header\n        dbc.NavbarSimple(\n            brand=\"Yield Curve Analysis & Forecasting Dashboard\",\n            brand_href=\"#\",\n            color=\"primary\",\n            dark=True,\n        ),\n        \n        # State storage components\n        dcc.Store(id='current-model-store', data='Heuristic'),\n        dcc.Store(id='selected-maturities-store', data=[]),\n        dcc.Store(id='backtest-state-store', data={}),\n        dcc.Store(id='viz-needs-refresh', data=False),\n        \n        # Main tab structure\n        dbc.Tabs([\n            # Tab 1: 3D Visualization\n            dbc.Tab(label=\"Yield Curve Visualization\", tab_id=\"tab-visualization\", children=[...]),\n            # Tab 2: Backtesting\n            dbc.Tab(label=\"Backtesting & Evaluation\", tab_id=\"tab-backtest\", children=[...]),\n            # Tab 3: Analysis\n            dbc.Tab(label=\"Statistics & Analysis\", tab_id=\"tab-analysis\", children=[...])\n        ], id=\"main-tabs\")\n    ])\n    \n    # Further code\n    # ...\n    \n    return app\n\nThe layout structure follows a hierarchical organization with tabs, rows, columns, and individual components, all styled with Bootstrap for a professional appearance.\nOne of the most important aspects of the Dash app is how it manages state between user interactions:\n\n# State storage components\ndcc.Store(id='current-model-store', data='Heuristic'),\ndcc.Store(id='selected-maturities-store', data=[]),\ndcc.Store(id='backtest-state-store', data={}),\ndcc.Store(id='viz-needs-refresh', data=False),\n\nThese dcc.Store components serve as client-side state containers that persist between callback executions. This approach:\n\nReduces redundant computations by storing user selections and current state\nPrevents unnecessary re-rendering of complex visualizations\nEnables communication between different parts of the application\n\nFor instance, when a user runs a backtest, the application stores the results in backtest-state-store and sets viz-needs-refresh to True, which triggers a refresh of the visualization when the user returns to that tab.\nThis was implemented in order to prevent issues with backtests potentially interfering with the 3D visualization.\nThe application uses a sophisticated callback system to handle user interactions. We’ll look at one of the key callbacks used to change visualizations based on user-selected forecasting models:\n\n@app.callback(\n    [Output(\"yield-curve-graph\", \"figure\"),\n     Output(\"current-model-store\", \"data\"),\n     Output(\"selected-maturities-store\", \"data\")],\n    [Input(\"forecast-model-dropdown\", \"value\"),\n     Input(\"maturities-dropdown\", \"value\")],\n    [State(\"current-model-store\", \"data\"),\n     State(\"selected-maturities-store\", \"data\")],\n    prevent_initial_call=False\n)\ndef update_figure(forecast_model, selected_maturities, current_model, stored_maturities):\n    \"\"\"Updates the 3D yield curve visualization based on user selections.\"\"\"\n    # Use fallback values if inputs are empty/None\n    if forecast_model is None or forecast_model == \"\":\n        forecast_model = current_model or \"Heuristic\"\n    \n    if selected_maturities is None:\n        selected_maturities = stored_maturities or []\n    \n    # Generate the 3D visualization with error handling\n    try:\n        fig = plotting.generate_surface_plot(\n            forecast_model, \n            selected_maturities, \n            config.DB_FILE, \n            config.DEFAULT_START_DATE, \n            config.DEFAULT_END_DATE, \n            vix_data\n        )\n        return fig, forecast_model, selected_maturities\n    except Exception as e:\n        # Create error message figure\n        fig = go.Figure()\n        fig.add_annotation(text=f\"Error: {str(e)}\", \n                          xref=\"paper\", yref=\"paper\", \n                          x=0.5, y=0.5, showarrow=False)\n        return fig, forecast_model, selected_maturities\n\nThis callback demonstrates several important principles:\n\nMultiple outputs: The callback updates both the visualization and store components in a single operation\nState usage: It uses current state values as fallbacks, ensuring the application behaves predictably\nError handling: The callback catches exceptions and provides user-friendly error messages\nParameter delegation: The actual visualization work is delegated to the plotting module\n\n\nThis interactive application makes complex yield curve analysis accessible and intuitive, allowing users to quickly switch between different views, models, and analytical tools without requiring deep technical knowledge of the underlying implementation details."
  },
  {
    "objectID": "blog/posts/Final_Project/index.html#conclusion-and-ethical-considerations",
    "href": "blog/posts/Final_Project/index.html#conclusion-and-ethical-considerations",
    "title": "YieldCurveForecaster: 3D Yield Curve Modeling and Forecasting Dashboard",
    "section": "Conclusion and Ethical Considerations",
    "text": "Conclusion and Ethical Considerations\nThe Yield Curve Forecasting tool provides a comprehensive platform for fixed income analysis, combining sophisticated mathematical modeling with interactive visualizations. The system’s ability to incorporate market regimes, inflation data, and volatility metrics produces forecasts that adapt to changing economic conditions. The backtesting framework provides transparency about model performance, allowing users to make informed decisions based on historical accuracy.\n\nEthics\nWhile the tool provides valuable insights for users ranging from intermediate investors to fixed income analysts, several ethical considerations are important to acknowledge:\n\nModel Transparency: The application makes clear that all forecasts are probabilistic in nature and subject to uncertainty. The backtesting framework helps users understand the limitations of each model, preventing overconfidence in predictions.\nFinancial Access Implications: Tools that enhance fixed income analysis primarily benefit institutional investors and may widen the knowledge gap between professional and retail investors. This raises questions about equitable access to financial technology.\nMarket Impact: If widely adopted, model-based forecasting tools could potentially contribute to herding behavior in markets if many participants use similar models or strategies. The application’s multiple models with different approaches help mitigate this risk.\nData Privacy: While the application uses public data, expansion to include proprietary or alternative data sources would require careful consideration of data privacy and usage rights.\nRegulatory Compliance: Users must ensure that any investment decisions made using the tool comply with relevant regulations, particularly regarding model validation requirements for financial institutions."
  },
  {
    "objectID": "blog/posts/Coin Toss Paradox/index.html#introduction",
    "href": "blog/posts/Coin Toss Paradox/index.html#introduction",
    "title": "A Coin Mixture Paradox: The Interesting Case of Constant Variance",
    "section": "Introduction",
    "text": "Introduction\nRecently, in my stochastic processes discussion, we went over a coin-flipping problem. In my probability course, we’ve covered many coin-toss problems, but I found this particular problem especially interesting—so much so that it sent me down a rabbit hole trying to understand it. The problem went as follows:\nImagine you have two coins. One is perfectly fair (50% chance of heads), and the other is biased, with a 60% chance of heads. Visually, the coins are indistinguishable. We then flip the left coin twice (not knowing if it is fair or biased), thus what is the variance of the number of heads denoted by \\(\\text{Var}(\\text{\\# heads})\\)?"
  },
  {
    "objectID": "blog/posts/Coin Toss Paradox/index.html#coin-flip-variance-solution",
    "href": "blog/posts/Coin Toss Paradox/index.html#coin-flip-variance-solution",
    "title": "A Coin Mixture Paradox: The Interesting Case of Constant Variance",
    "section": "Coin Flip Variance Solution",
    "text": "Coin Flip Variance Solution\nThe solution goes as follows:\nLet\n\\[\nX = X_1 + X_2\n\\]\nwhere \\(X_1\\) and \\(X_2\\) represent the results of the first and second coin flips, respectively (1 = heads, 0 = tails).\nWe want to compute: \\[\n\\operatorname{Var}(X) = \\operatorname{Var}(X_1 + X_2)\n\\]\nUsing the identity: \\[\n\\operatorname{Var}(X_1 + X_2) = \\operatorname{Var}(X_1) + \\operatorname{Var}(X_2) + 2 \\cdot \\operatorname{Cov}(X_1, X_2)\n\\]\n\nNote: The two coin flips are not independent, since flipping one head increases the likelihood that the coin is biased. Hence, we must account for covariance.\n\n\n\nStep 1: Compute \\(\\operatorname{Var}(X_1)\\)\n\\[\n\\operatorname{Var}(X_1) = \\mathbb{E}[X_1^2] - (\\mathbb{E}[X_1])^2\n\\]\nSince \\(X_1 \\in \\{0, 1\\}\\) (Bernoulli random variable), \\(X_1^2 = X_1\\), so: \\[\n\\mathbb{E}[X_1^2] = \\mathbb{E}[X_1]\n\\]\nWe compute \\(\\mathbb{E}[X_1]\\) using the law of total expectation: \\[\n\\mathbb{E}[X_1] = \\mathbb{P}(\\text{Heads}) = \\frac{1}{2}(0.5) + \\frac{1}{2}(0.6) = 0.55\n\\]\n\\[\n\\operatorname{Var}(X_1) = 0.55 - (0.55)^2 = 0.55 - 0.3025 = 0.2475\n\\]\nBy symmetry: \\[\n\\operatorname{Var}(X_2) = \\operatorname{Var}(X_1) = 0.2475\n\\]\n\n\n\nStep 2: Compute \\(\\operatorname{Cov}(X_1, X_2)\\)\nWe use: \\[\n\\operatorname{Cov}(X_1, X_2) = \\mathbb{E}[X_1 X_2] - \\mathbb{E}[X_1]\\mathbb{E}[X_2]\n\\]\n\nCompute \\(\\mathbb{E}[X_1 X_2]\\)\nThis is the probability that both flips are heads: \\[\n\\mathbb{P}(\\text{both H}) = \\mathbb{P}(\\text{both H} \\mid \\text{fair}) \\cdot \\mathbb{P}(\\text{fair}) + \\mathbb{P}(\\text{both H} \\mid \\text{biased}) \\cdot \\mathbb{P}(\\text{biased})\n\\]\n\\[\n= (0.5)(0.5)^2 + (0.5)(0.6)^2 = 0.5(0.25) + 0.5(0.36) = 0.125 + 0.18 = 0.305\n\\]\n\n\nCompute \\(\\mathbb{E}[X_1]\\mathbb{E}[X_2]\\)\n\\[\n= (0.55)^2 = 0.3025\n\\]\nSo, \\[\n\\operatorname{Cov}(X_1, X_2) = 0.305 - 0.3025 = 0.0025\n\\]\n\n\n\n\nFinal Step: Combine All\n\\[\n\\operatorname{Var}(X) = \\operatorname{Var}(X_1) + \\operatorname{Var}(X_2) + 2 \\cdot \\operatorname{Cov}(X_1, X_2)\n\\]\n\\[\n= 0.2475 + 0.2475 + 2(0.0025) = 0.495 + 0.005 = \\boxed{0.5}\n\\]"
  },
  {
    "objectID": "blog/posts/Coin Toss Paradox/index.html#interpretting-our-answer",
    "href": "blog/posts/Coin Toss Paradox/index.html#interpretting-our-answer",
    "title": "A Coin Mixture Paradox: The Interesting Case of Constant Variance",
    "section": "Interpretting Our Answer",
    "text": "Interpretting Our Answer\nI don’t know about you, but I found it hard to believe that the answer was actually 1/2.\nFor example, we know:\n\nA fair coin (p=0.5) flipped twice has variance 0.5\nA biased coin (p=0.6) flipped twice has variance 0.48\nWe have a 50/50 chance of using either coin\n\nWe can also graph the relationship of variance to probability of heads:\n\n\n\n\n\nSo, kind of like a magic trick, how is it possible that with a biased coin, our variance is the same as if we flipped two fair coins?\nThe answer is actually in the wording of the problem. By adding the uncertainty about which coin we’re using (we picked the left coin not knowing whether it’s fair or biased), we perfectly compensate for the reduced variance of the biased coin. On top of that, it wouldn’t matter whether the probability of heads was 0.6 or 0.7 or 0.8. For any probability of heads for the biased coin, our variance of heads for the two coin flips will always be 0.5.\nIt was at this point that I left the discussion knowing the answer to the paradox, however, I still wasn’t fully convinced. Thus, as any curious mathematician would do, I sought to understand why our solution works the way it does by generalizing the problem."
  },
  {
    "objectID": "blog/posts/Coin Toss Paradox/index.html#generalizing-the-problem",
    "href": "blog/posts/Coin Toss Paradox/index.html#generalizing-the-problem",
    "title": "A Coin Mixture Paradox: The Interesting Case of Constant Variance",
    "section": "Generalizing the Problem",
    "text": "Generalizing the Problem\nImagine you have two coins. One is perfectly fair (50% chance of heads), and the other is biased, but you don’t necessarily know its exact bias – just that it has some probability p of landing heads.\nNow, consider this experiment:\n\nYou randomly pick one of the two coins (with equal probability, 0.5 each).\nYou flip the chosen coin twice.\nYou count the total number of heads, let’s call this Y.\n\nLet’s define our variables more formally:\n\n\\(C\\): The random variable representing the chosen coin.\n\n\\(C = F\\) (Fair coin) with \\(P(C=F) = 0.5\\)\n\\(C = B\\) (Biased coin) with \\(P(C=B) = 0.5\\)\n\nProbabilities of Heads:\n\n\\(P(\\text{Heads} | C=F) = p_F = 0.5\\)\n\\(P(\\text{Heads} | C=B) = p_B = p\\) (where \\(0 \\leq p \\leq 1\\))\n\n\\(X_1, X_2\\): The outcomes of the two flips (1 for heads, 0 for tails). These are Bernoulli trials given the chosen coin.\n\\(Y\\): The total number of heads in the two flips. \\(Y = X_1 + X_2\\). Given the coin, \\(Y\\) follows a Binomial distribution \\(\\text{Bin}(n=2, \\text{probability}=p_C)\\).\n\nOur goal is to calculate \\(\\text{Var}(Y)\\), the variance of the total number of heads before we know which coin was chosen.\n\nExplaining the Constant Variance\nThe key to understanding this result lies in the Law of Total Variance (also known as Eve’s Law). For random variables \\(Y\\) and \\(C\\), it states:\n\\[\\text{Var}(Y) = E[\\text{Var}(Y | C)] + \\text{Var}(E[Y | C])\\]\nLet’s break this down:\n\n\\(E[\\text{Var}(Y | C)]\\): The expected value of the conditional variance. This is the average variance within each coin type. We calculate the variance of \\(Y\\) assuming we know which coin was picked (\\(\\text{Var}(Y | C=F)\\) and \\(\\text{Var}(Y | C=B)\\)) and then find the weighted average of these variances based on the probability of picking each coin.\n\\(\\text{Var}(E[Y | C])\\): The variance of the conditional expectation. This measures the variability between the average outcomes of the different coin types. We calculate the expected value of \\(Y\\) assuming we know which coin was picked (\\(E[Y | C=F]\\) and \\(E[Y | C=B]\\)) and then find the variance of these expected values, treating \\(E[Y | C]\\) itself as a random variable that depends on \\(C\\).\n\nLet’s calculate each term.\n\n\nStep 1: Conditional Expectations and Variances\nFirst, let’s find the expected value and variance of \\(Y\\), conditional on knowing which coin was chosen.\n\nGiven the Fair Coin (\\(C=F\\)):\n\nThe number of heads \\(Y\\) follows \\(\\text{Bin}(n=2, p=0.5)\\).\n\\(E[Y | C=F] = n \\cdot p_F = 2 \\times 0.5 = 1\\)\n\\(\\text{Var}(Y | C=F) = n \\cdot p_F \\cdot (1 - p_F) = 2 \\cdot 0.5 \\cdot (1 - 0.5) = 2 \\cdot 0.5 \\times 0.5 = 0.5\\)\n\nGiven the Biased Coin (\\(C=B\\)):\n\nThe number of heads \\(Y\\) follows \\(\\text{Bin}(n=2, p=p)\\).\n\\(E[Y | C=B] = n \\cdot p_B = 2 \\times p\\)\n\\(\\text{Var}(Y | C=B) = n \\cdot p_B \\cdot (1 - p_B) = 2 \\cdot p \\times (1 - p) = 2p(1-p)\\)\n\n\n\n\nStep 2: Calculate \\(E[\\text{Var}(Y | C)]\\)\nThis is the average of the conditional variances, weighted by the probability of choosing each coin:\n\\[E[\\text{Var}(Y | C)] = \\text{Var}(Y | C=F) \\cdot P(C=F) + \\text{Var}(Y | C=B) \\cdot P(C=B)\\]\n\\[E[\\text{Var}(Y | C)] = (0.5) \\cdot (0.5) + (2p(1-p)) \\cdot (0.5)\\]\n\\[E[\\text{Var}(Y | C)] = 0.25 + p(1-p)\\]\n\\[E[\\text{Var}(Y | C)] = 0.25 + p - p^2\\]\n\n\nStep 3: Calculate \\(\\text{Var}(E[Y | C])\\)\nThis is the variance of the conditional means. We have a random variable \\(E[Y | C]\\) which takes the value \\(E[Y | C=F] = 1\\) with probability 0.5, and the value \\(E[Y | C=B] = 2p\\) with probability 0.5.\nTo find its variance, we use \\(\\text{Var}(X) = E[X^2] - (E[X])^2\\).\n\nFirst, find the mean \\(E[E[Y | C]]\\):\n\\[E[E[Y | C]] = E[Y | C=F] \\cdot P(C=F) + E[Y | C=B] \\cdot P(C=B)\\]\n\\[E[E[Y | C]] = (1) \\cdot (0.5) + (2p) \\cdot (0.5)\\]\n\\[E[E[Y | C]] = 0.5 + p\\]\n(Note: By the law of total expectation, this is also \\(E[Y]\\)).\nNext, find the expected value of the square \\(E[(E[Y | C])^2]\\):\n\\[E[(E[Y | C])^2] = (E[Y | C=F])^2 \\cdot P(C=F) + (E[Y | C=B])^2 \\cdot P(C=B)\\]\n\\[E[(E[Y | C])^2] = (1)^2 \\cdot (0.5) + (2p)^2 \\cdot (0.5)\\]\n\\[E[(E[Y | C])^2] = 1 \\cdot 0.5 + 4p^2 \\cdot 0.5\\]\n\\[E[(E[Y | C])^2] = 0.5 + 2p^2\\]\nNow, calculate the variance:\n\\[\\text{Var}(E[Y | C]) = E[(E[Y | C])^2] - (E[E[Y | C]])^2\\]\n\\[\\text{Var}(E[Y | C]) = (0.5 + 2p^2) - (0.5 + p)^2\\]\n\\[\\text{Var}(E[Y | C]) = 0.5 + 2p^2 - (0.25 + p + p^2)\\]\n\\[\\text{Var}(E[Y | C]) = 0.5 + 2p^2 - 0.25 - p - p^2\\]\n\\[\\text{Var}(E[Y | C]) = 0.25 - p + p^2\\]\n\n\n\nStep 4: Combine the Terms\nNow we add the two components according to the Law of Total Variance:\n\\[\\text{Var}(Y) = E[\\text{Var}(Y | C)] + \\text{Var}(E[Y | C])\\]\n\\[\\text{Var}(Y) = (0.25 + p - p^2) + (0.25 - p + p^2)\\]\nNotice how the terms involving \\(p\\) and \\(p^2\\) cancel out!\n\\[\\text{Var}(Y) = 0.25 + 0.25 + (p - p) + (-p^2 + p^2)\\]\n\\[\\text{Var}(Y) = 0.5\\]\nThe variance \\(\\text{Var}(Y)\\) is indeed \\(0.5\\), regardless of the value of \\(p\\).\n\n\nIntuition: Why Does \\(p\\) Cancel Out?\nThe cancellation happens because the two components of the total variance move in opposite directions as the bias \\(p\\) changes:\n\nAverage Within-Coin Variance (\\(E[\\text{Var}(Y | C)] = 0.25 + p(1-p)\\)): This term represents the inherent randomness within each coin type. The variance of a single Bernoulli or Binomial trial is maximized when \\(p=0.5\\). So, as the biased coin’s \\(p\\) moves away from 0.5 (towards 0 or 1), its individual variance \\(2p(1-p)\\) decreases. This makes the average variance term smaller when \\(p\\) is far from 0.5.\nVariance Between Coin Averages (\\(\\text{Var}(E[Y | C]) = 0.25 - p + p^2\\)): This term represents how different the average outcomes are for the two coins. The expected values are \\(E[Y|C=F]=1\\) and \\(E[Y|C=B]=2p\\). When \\(p=0.5\\), both expectations are 1, so there’s no variance between them (\\(\\text{Var}(E[Y|C]) = 0.25 - 0.5 + 0.25 = 0\\)). As \\(p\\) moves away from 0.5, the difference between the average outcomes (1 and 2p) increases, leading to a larger variance between the conditional expectations.\n\nThese two effects perfectly offset each other. As the bias \\(p\\) makes one term smaller, it makes the other term larger by exactly the same amount, keeping their sum constant at 0.5."
  },
  {
    "objectID": "blog/posts/Coin Toss Paradox/index.html#conclusion",
    "href": "blog/posts/Coin Toss Paradox/index.html#conclusion",
    "title": "A Coin Mixture Paradox: The Interesting Case of Constant Variance",
    "section": "Conclusion",
    "text": "Conclusion\nThe constant variance of 0.5 in this coin mixture problem is a fascinating result that stems directly from the Law of Total Variance. While it seems paradoxical that the overall variability doesn’t depend on the specific bias \\(p\\) of the second coin, the mathematical breakdown shows a perfect cancellation effect. The average variance within the coin types and the variance between the coin types’ average outcomes compensate for each other precisely.\nThis example highlights how decomposing variance can reveal underlying structures and sometimes lead to surprising, constant results even when parameters within the mixture model are changing. It also serves as a reminder that our intuition about how probability distributions combine can sometimes be misleading!"
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio Performance Analysis",
    "section": "",
    "text": "This visualization provides a year-to-date analysis of my actively managed investment portfolio’s performance, comparing it to the S&P 500 benchmark (SPY). Portfolio metrics are computed based on a non-annualized framework.\nNote: The data is updated daily via an automated GitHub Action.\n\n\n\nYTD Performance\n\n\nRisk Analysis\n\n\nAlpha Evolution\n\n\n\n\n\n\n\n\n\nLoading risk analysis…\n\n\n\n\n\n\n\n\nNote: Rolling metrics shown in this chart use 20-day windows and will differ from the YTD cumulative metrics displayed in the summary cards below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey Metrics Explained\n\n\n\n\nAlpha: YTD excess returns after accounting for market risk (positive = outperformance)\nBeta: Measures portfolio volatility relative to market (1 = market risk)\nSharpe Ratio: Risk-adjusted absolute returns (higher = better)\nInformation Ratio: Risk-adjusted relative returns vs benchmark (higher = better)\nRolling Metrics: Computed using a 20-day rolling window to show evolution over time"
  },
  {
    "objectID": "posts/HW4/index.html#introduction",
    "href": "posts/HW4/index.html#introduction",
    "title": "JAX-Driven Heat Diffusion: Sparse and Vectorized PDE Simulation",
    "section": "Introduction",
    "text": "Introduction\nHeat diffusion is a fundamental process in physics and engineering, described by the heat equation, a partial differential equation (PDE) modeling thermal energy transfer. In this project, we simulate 2D heat diffusion using multiple computational approaches:\n\nMatrix-vector multiplication (explicit finite difference method).\nSparse matrix representation in JAX (efficient storage and computation).\nNumPy vectorized operations (direct array manipulation with np.roll).\nJAX-optimized just-in-time (JIT) compilation (high-speed execution).\n\nThe objective is to compare performance and efficiency across these methods while maintaining correct boundary conditions (to simulate heat diffusion properly) and stable numerical updates. Sparse representations and JIT-optimized computations significantly reduce runtime complexity compared to basic implementations. The results will be visualized using heatmaps at regular intervals to analyze diffusion patterns. Additionally, results will be accompanied by runtimes to compare code efficiency."
  },
  {
    "objectID": "posts/HW4/index.html#the-math-of-heat-diffusion-simulations",
    "href": "posts/HW4/index.html#the-math-of-heat-diffusion-simulations",
    "title": "JAX-Driven Heat Diffusion: Sparse and Vectorized PDE Simulation",
    "section": "The Math of Heat Diffusion Simulations",
    "text": "The Math of Heat Diffusion Simulations\nBefore we dive into the code that simulates heat diffusion, we must understand the math behind it.\n\nThe Continuous 2D Heat Equation\nThe heat diffusion equation describes how heat spreads over time in a two-dimensional space. The general form is:\n\\[\\frac{\\partial u}{\\partial t} = \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\]\nwhere:\n\n\\(u(x, y, t)\\) represents the temperature at position \\((x, y)\\) at time \\(t\\).\nThe right-hand side represents the Laplacian operator, which models how heat spreads from a point to its neighbors.\n\n\n\nDiscretization Using Finite Differences\nWe approximate the second derivatives using the finite difference method. Using a uniform grid with spacing \\(\\Delta x = \\Delta y\\) we approximate:\n\\[\\frac{\\partial^2 u}{\\partial x^2} \\approx \\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{\\Delta x^2}\\]\n\\[\\frac{\\partial^2 u}{\\partial y^2} \\approx \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{\\Delta y^2}\\]\nSubstituting these into the heat equation:\n\\[\\frac{u_{i,j}^{k+1} - u_{i,j}^{k}}{\\Delta t} = \\frac{u_{i+1,j}^{k} - 2u_{i,j}^{k} + u_{i-1,j}^{k}}{\\Delta x^2} + \\frac{u_{i,j+1}^{k} - 2u_{i,j}^{k} + u_{i,j-1}^{k}}{\\Delta y^2} \\]\nRearranging for \\(u_{i,j}^{k+1}\\):\n\\[u_{i,j}^{k+1} = u_{i,j}^{k} + \\epsilon \\left( u_{i+1,j}^{k} + u_{i-1,j}^{k} + u_{i,j+1}^{k} + u_{i,j-1}^{k} - 4u_{i,j}^{k} \\right)\\]\nwhere \\(\\epsilon = \\frac{ \\Delta t}{\\Delta x^2}\\) is the stability parameter.\n\n\nMatrix Representation of the Finite Difference Method\nWe represent the system in matrix form:\n\\[\nu_{k+1} = u_k + \\epsilon A u_k\n\\]\nwhere:\n\n\\(u_k\\) is the flattened temperature grid at time \\(k\\).\n\\(A\\) is the finite difference matrix that applies the heat diffusion stencil.\n\nThe finite difference matrix \\(A\\) for a grid of size \\(N \\times N\\) is:\n\\[\nA =\n\\begin{bmatrix}\nT & I & 0 & 0 & \\dots & 0  \\\\\nI & T & I & 0 & \\dots & 0  \\\\\n0 & I & T & I & \\dots & 0  \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\dots & T & I  \\\\\n0 & 0 & 0 & \\dots & I & T\n\\end{bmatrix}\n\\]\nwhere: - \\(T\\) is an \\(N \\times N\\) tridiagonal matrix representing left-right interactions:\n\\[\n  T =\n  \\begin{bmatrix}\n  -4 & 1  & 0  & \\dots & 0  \\\\\n  1  & -4 & 1  & \\dots & 0  \\\\\n  0  & 1  & -4 & \\dots & 0  \\\\\n  \\vdots & \\vdots & \\vdots & \\ddots & \\vdots  \\\\\n  0  & 0  & 0  & \\dots & -4\n  \\end{bmatrix}\n  \\]\n\n\\(I\\) is an \\(N \\times N\\) identity matrix that accounts for top/bottom interactions.\n\nThis matrix enforces:\n\nSelf-weight (-4) at each grid point.\nNeighboring weights (+1) for adjacent grid points."
  },
  {
    "objectID": "posts/HW4/index.html#initializing-our-heat-diffusion-simulation",
    "href": "posts/HW4/index.html#initializing-our-heat-diffusion-simulation",
    "title": "JAX-Driven Heat Diffusion: Sparse and Vectorized PDE Simulation",
    "section": "Initializing Our Heat Diffusion Simulation",
    "text": "Initializing Our Heat Diffusion Simulation\nFor our simulations we will use a grid size N = 101 and stability parameter \\(\\epsilon = 0.2\\).\n\nN = 101\nepsilon = 0.2\n\nWe initialize the simulation with a single heat source at the center.\nWe’ll use the following initial condition:\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n# construct initial condition: 1 unit of heat at midpoint. \nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)"
  },
  {
    "objectID": "posts/HW4/index.html#heat-diffusion-with-matrix-vector-multiplication",
    "href": "posts/HW4/index.html#heat-diffusion-with-matrix-vector-multiplication",
    "title": "JAX-Driven Heat Diffusion: Sparse and Vectorized PDE Simulation",
    "section": "Heat Diffusion With Matrix-Vector Multiplication",
    "text": "Heat Diffusion With Matrix-Vector Multiplication\nIn order to begin our 2D heat simulations we’ll create a file heat_equation.py with the following packages:\n\nimport numpy as np\nimport jax.numpy as jnp\nimport jax\nfrom scipy.sparse import diags\nfrom jax.experimental.sparse import BCOO\n\nUsing the math above we use matrix-vector multiplication to simulate heat diffusion. The grid is flattened into a 1D vector to apply the finite difference matrix efficiently:\n\\[\nu_{k+1} = u_k + \\epsilon A u_k\n\\]\nwhere:\n\n\\(A\\) is the finite difference matrix of size \\(N^2 \\times N^2\\).\n\\(U^k\\) is the flattened temperature grid at time \\(k\\).\n\\(\\epsilon\\) is the stability constant.\n\nThis formula is represented by the following function:\n\nfrom heat_equation import advance_time_matvecmul\nimport inspect\nprint(inspect.getsource(advance_time_matvecmul))\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix, N^2 x N^2. \n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    N = u.shape[0]\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\n\n\n\nIn order for the function above to run we also need to write a function get_A() that constructs the matrix A:\n\nfrom heat_equation import get_A\nprint(inspect.getsource(get_A))\n\ndef get_A(N):\n    \"\"\"Constructs the finite difference matrix A for the heat equation.\n    Args:\n        N: Grid size\n\n    Returns:\n        A: Finite difference matrix (N^2 x N^2).\n    \"\"\"\n    n = N * N\n    diagonals = [\n        -4 * np.ones(n),  # Main diagonal\n        np.ones(n - 1),    # Right neighbor\n        np.ones(n - 1),    # Left neighbor\n        np.ones(n - N),    # Upper neighbor\n        np.ones(n - N)     # Lower neighbor\n    ]\n    # Apply boundary conditions (preventing wrap-around)\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0  \n\n    # Construct the finite difference matrix\n    A = (\n        np.diag(diagonals[0]) +\n        np.diag(diagonals[1], 1) +\n        np.diag(diagonals[2], -1) +\n        np.diag(diagonals[3], N) +\n        np.diag(diagonals[4], -N)\n    )\n    return A\n\n\n\nTo test our functions and create a heat simulation, we’ll write a function that allows us to specify our type of function that updates the heat distribution from heat_equations.py and the version of matrix A we’ll use in each test.\nThe following function wraps our testing and visualization for all test cases:\n\nimport matplotlib.pyplot as plt\nimport time\n\ndef heat_diffusion_test(func, \n                        u_init, \n                        epsilon, \n                        iterations=2700, \n                        snapshot_interval=300, \n                        matrix_A=None):\n    \"\"\"\n    Runs and visualizes the heat diffusion simulation.\n\n    Args:\n        - func (function): The function that updates the heat distribution.\n        - u_init (np.ndarray or jnp.ndarray): Initial heat distribution (N x N grid).\n        - epsilon: Stability constant.\n        - iterations: Total number of iterations to run.\n        - snapshot_interval: Interval at which to capture snapshots.\n        - matrix_A: matrix A.\n    \n    Returns:\n        - Heat simulation visualizations\n        - Total execution time of the simulation.\n    \"\"\"\n    # Copy the initial condition\n    u = np.copy(u_init) if isinstance(u_init, np.ndarray) else jnp.array(u_init)\n\n    # Store snapshots for visualization\n    snapshots = []\n\n    # Precompile JAX function if applicable\n    if \"jax\" in str(type(func)):\n        u = func(u, epsilon)  # JAX functions require precompilation\n        u.block_until_ready()\n\n    # Run the simulation and time it\n    start_time = time.time()\n    for i in range(1, iterations + 1):\n        if matrix_A is not None:  # For matrix-vector multiplication\n            u = func(A, u, epsilon)\n        else:\n            u = func(u, epsilon)\n\n        if i % snapshot_interval == 0:\n            snapshots.append(np.array(u))  # Convert JAX arrays to NumPy if needed\n\n    # Ensure JAX computations complete\n    if isinstance(u, jnp.ndarray):\n        u.block_until_ready()\n    \n    end_time = time.time()\n    elapsed_time = end_time - start_time\n\n    # Plot results in a 3x3 grid\n    fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n    for idx, snapshot in enumerate(snapshots):\n        ax = axes[idx // 3, idx % 3]\n        ax.imshow(snapshot, cmap=\"viridis\", interpolation=\"nearest\")\n        ax.set_title(f\"Iteration {(idx + 1) * snapshot_interval}\")\n\n    plt.tight_layout()\n    plt.show()\n\n    return elapsed_time\n\nWe can now test our first heat simulation using get_A() and heat_diffusion_test() with advance_time_matvecmul() passed as the function in the test arguments:\n\nimport heat_equation\n\nA = get_A(N)\ntime = heat_diffusion_test(advance_time_matvecmul, u0, epsilon, matrix_A=A)\nprint(f\"Total execution time: {time:.2f} seconds\")\n\n\n\n\n\n\n\n\nTotal execution time: 35.84 seconds\n\n\nThis method, while mathematically correct, is computationally expensive (as seen by the runtime of approximately 36 seconds). In the next sections, we optimize it using sparse matrices and vectorized operations."
  },
  {
    "objectID": "posts/HW4/index.html#optimizing-computation-with-sparse-matrices",
    "href": "posts/HW4/index.html#optimizing-computation-with-sparse-matrices",
    "title": "JAX-Driven Heat Diffusion: Sparse and Vectorized PDE Simulation",
    "section": "Optimizing Computation With Sparse Matrices",
    "text": "Optimizing Computation With Sparse Matrices\nThe finite difference matrix \\(A\\) is mostly zeros, leading to wasted computations. Using JAX sparse matrices, we store only nonzero values, reducing memory usage and improving efficiency.\nWe’ll write a function get_sparse_A() that computes the sparse matrix using the batched coordinate BCOO format to only use \\(O(N^2)\\) space for the matrix, and only take \\(O(N^2)\\) time for each update:\n\nfrom heat_equation import get_sparse_A\nprint(inspect.getsource(get_sparse_A))\n\ndef get_sparse_A(N):\n    \"\"\"Constructs the sparse matrix A using JAX sparse format.\"\"\"\n    A = get_A(N) # Get the dense matrix A\n    A_sp_matrix = BCOO.fromdense(jnp.array(A)) # Convert dense matrix to JAX sparse format (BCOO)\n    return A_sp_matrix\n\n\n\nHere, A_sp_matrix is the sparse matrix of A which should make the compute more efficient. We can test this by rerunning the heat simulation using the get_sparse_A() matrix and the same heat_diffusion_test() function, comparing the runtimes:\n\nimport heat_equation\n\nA = get_sparse_A(N)\ntime = heat_diffusion_test(advance_time_matvecmul, u0, epsilon, matrix_A=A)\nprint(f\"Total execution time: {time:.2f} seconds\")\n\n\n\n\n\n\n\n\nTotal execution time: 1.86 seconds\n\n\nRuntime for the heat simulation using the sparse matrix of A is more than 10x faster (19.27x) at approximately 2 seconds. Still, we can make the heat simulation more efficient by replacing advance_time_matvecmul(), which utilizes matrix-vector multiplications with vectorized array operations like np.roll()."
  },
  {
    "objectID": "posts/HW4/index.html#direct-numpy-vectorization",
    "href": "posts/HW4/index.html#direct-numpy-vectorization",
    "title": "JAX-Driven Heat Diffusion: Sparse and Vectorized PDE Simulation",
    "section": "Direct NumPy Vectorization",
    "text": "Direct NumPy Vectorization\nThe matrix-vector multiplication approach is useful, particularly in other PDE problems, such as Poisson equations, where the matrix equation must be solved. However, for the heat equation, it is not necessary in terms of computation. Instead, we can use NumPy’s np.roll() to compute updates directly.\nWe’ll do this by writing the function advance_time_numpy() in heat_equation.py:\n\nfrom heat_equation import advance_time_numpy\nprint(inspect.getsource(advance_time_numpy))\n\ndef advance_time_numpy(u, epsilon):\n    \"\"\"Advances the simulation using NumPy's np.roll for boundary handling.\"\"\"\n    u_padded = np.pad(u, pad_width=1, mode='constant', constant_values=0)  # Pad u with zeros\n    u_next = u + epsilon * (  # Update u using finite differences\n        np.roll(u_padded, 1, axis=0)[1:-1, 1:-1] +  # Shift up\n        np.roll(u_padded, -1, axis=0)[1:-1, 1:-1] +  # Shift down\n        np.roll(u_padded, 1, axis=1)[1:-1, 1:-1] +  # Shift left\n        np.roll(u_padded, -1, axis=1)[1:-1, 1:-1] -  # Shift right\n        4 * u  # Subtract central value\n    )\n    return u_next  # Return updated interior values\n\n\n\nThe function above works by padding the grid with zeros and computes the transformation using np.roll(). We can understand the function with the visual below:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext, we can rerun the simulation using the advance_time_numpy() function as an argument in the visualize_heat_diffusion() function and compare runtimes:\n\nimport heat_equation\n\ntime = heat_diffusion_test(advance_time_numpy, u0, epsilon)\nprint(f\"Total execution time: {time:.2f} seconds\")\n\n\n\n\n\n\n\n\nTotal execution time: 0.21 seconds\n\n\nRuntime for the heat simulation using the direct NumPy vectorization is more than 100x faster (170.67x) than the base matrix-vector simulation we did at 0.21 seconds. While already fast, we can achieve a quicker runtime by applying the jit function to essentially the same function advance_time_numpy(), using the JAX package."
  },
  {
    "objectID": "posts/HW4/index.html#jax-jit-compilation-for-maximum-speed",
    "href": "posts/HW4/index.html#jax-jit-compilation-for-maximum-speed",
    "title": "JAX-Driven Heat Diffusion: Sparse and Vectorized PDE Simulation",
    "section": "JAX JIT Compilation for Maximum Speed",
    "text": "JAX JIT Compilation for Maximum Speed\nUsing JAX’s @jax.jit, we can compile the function into optimized machine code, further reducing execution time.\nWe’ll do this by writing the function advance_time_jax() in heat_equation.py with @jax.jit above the function:\n\nfrom heat_equation import advance_time_jax\nprint(inspect.getsource(advance_time_jax))\n\ndef advance_time_jax(u, epsilon):\n    \"\"\"Advances the simulation using JAX and just-in-time compilation.\"\"\"\n    u_padded = np.pad(u, pad_width=1, mode='constant', constant_values=0)  # Pad u with zeros\n    u_next = u + epsilon * (  # Update u using finite differences\n        jnp.roll(u_padded, 1, axis=0)[1:-1, 1:-1] +  # Shift up\n        jnp.roll(u_padded, -1, axis=0)[1:-1, 1:-1] +  # Shift down\n        jnp.roll(u_padded, 1, axis=1)[1:-1, 1:-1] +  # Shift left\n        jnp.roll(u_padded, -1, axis=1)[1:-1, 1:-1] -  # Shift right\n        4 * u  # Subtract central value\n    )\n    return u_next  # Return updated interior values\n\n\n\nThe only difference between the direct NumPy vectorization function is that we use @jax.jit above our function and replace the NumPy calls np with the JAX NumPy calls jnp.\nWe’ll run the heat diffusion test one last time using the JAX optimized function and compare execution speed:\n\nimport heat_equation\n\ntime = heat_diffusion_test(advance_time_jax, u0, epsilon)\nprint(f\"Total execution time: {time:.2f} seconds\")\n\n\n\n\n\n\n\n\nTotal execution time: 0.06 seconds\n\n\nRuntime for the heat simulation using the JAX jit operation is more than 2x faster (3.5x) than the direct NumPy vectorization simulation we did at 0.06 seconds. This is nearly 600x faster than the base matrix-vector simulation."
  },
  {
    "objectID": "posts/HW4/index.html#conclusion-applications-to-deep-learning",
    "href": "posts/HW4/index.html#conclusion-applications-to-deep-learning",
    "title": "JAX-Driven Heat Diffusion: Sparse and Vectorized PDE Simulation",
    "section": "Conclusion: Applications to Deep Learning",
    "text": "Conclusion: Applications to Deep Learning\nAlthough this blog focuses on simulating heat diffusion, the computational techniques used are directly applicable to deep learning and neural networks. The methods explored, matrix operations, sparse representations, vectorized computations, and JIT compilation, are fundamental to training and optimizing deep learning models."
  },
  {
    "objectID": "posts/HW2/index.html",
    "href": "posts/HW2/index.html",
    "title": "Data-Driven Movie Recommendations: Scrapy Web Scraping and Actor Network Graphs with NetworkX",
    "section": "",
    "text": "Network Graph of Movie Data"
  },
  {
    "objectID": "posts/HW2/index.html#introduction",
    "href": "posts/HW2/index.html#introduction",
    "title": "Data-Driven Movie Recommendations: Scrapy Web Scraping and Actor Network Graphs with NetworkX",
    "section": "Introduction",
    "text": "Introduction\nThe TMDB (The Movie Database) is a comprehensive online database that provides detailed information on movies, TV shows, actors, and production crews. In this project, we will develop a Scrapy-based web crawler to extract movie and actor data from TMDB. The crawler is designed to navigate from a movie’s main page to its full cast list, retrieve actor profiles, and extract their filmographies. We will employ CSS selectors for precise data extraction, implement depth-controlled crawling to avoid unnecessary requests, and optimize request handling to improve performance. Finally, we will use the extracted data to identify patterns in shared actor collaborations across movies using NetworkX. Through this approach, we demonstrate how Scrapy can be leveraged to build a structured dataset for analyzing relationships within the film industry."
  },
  {
    "objectID": "posts/HW2/index.html#setting-up-the-webscraper",
    "href": "posts/HW2/index.html#setting-up-the-webscraper",
    "title": "Data-Driven Movie Recommendations: Scrapy Web Scraping and Actor Network Graphs with NetworkX",
    "section": "Setting Up the Webscraper",
    "text": "Setting Up the Webscraper\nIn this section, we’ll first initialize the Scrapy framework, examine how TMDB organizes its data, and prepare our project for extracting meaningful movie-actor relationships.\n\nInitializing the Scrapy Project\nWe’ll start by creating a new Scrapy project.\nWe can do this by opening the terminal and activating the relevant Python environment:\n\nconda activate PIC16B-25W\n\nNext, create a Scrapy project by running:\n\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\n\nRunning this will create a folder named “TMDB_scraper” with the following contents inside:\n\nTMDB_scraper/\n│── scrapy.cfg                \n└── TMDB_scraper/\n    ├── spiders/              \n    │   └── tmdb_spider.py    # Our main spider script\n    ├── settings.py           # Scrapy settings file\n    ├── middlewares.py        \n    ├── pipelines.py          \n    └── __init__.py           \n\nWe’ll only really need the tmdb_spider.py file, where we will write the logic for our scraper. We’ll also make minor changes to the setting of our spider in settings.py.\n\n\nUnderstanding the TMDB Website Structure\nIn order to extract the correct data, we need to analyze how TMDB organizes and displays its information. A movie page typically exhibits the following structure:\nMovie Starting Page Once on a movie’s starting page, we observe that the URL contains a movie ID. Our project will focus on Harry Potter and the Sorcerer’s Stone. the TMDB URL for Harry Potter and the Sorcerer’s Stone is:\n\nhttps://www.themoviedb.org/movie/671-harry-potter-and-the-philosopher-s-stone/\n\nThe movie ID 671-harry-potter-and-the-philosopher-s-stone will be passed as an argument to our scraper.\nNavigate to the Cast & Crew Page Next we want to click on the “Full Cast & Crew” link which takes us to the following page:\n\nhttps://www.themoviedb.org/movie/671-harry-potter-and-the-philosopher-s-stone/cast\n\nNotice that the URL is the same as the previous page, except for the /cast appended to the end. This page is where our spider will extract the list of actors (ignoring crew members).\nExtracting Actor Profiles Clicking on an actor’s name leads to the actor profile page:\n\nhttps://www.themoviedb.org/person/10980-daniel-radcliffe\n\nOn this page, we need to find the “Acting” section, which contains a list of movies and TV shows the actor has performed in. There are multiple sections such as Production, Writing, and Acting. We must only extract movie titles listed under “Acting”.\n\n\nConfiguring the Scrapy Crawler Settings\nTo prevent excessive data requests while we are testing the crawler, we’ll temporarily add the following line in settings.py:\n\nCLOSESPIDER_PAGECOUNT = 20\n\nThis stops the spider after 20 pages so we don’t accidentally send too many requests while debugging.\nWhen scraping, we’ll look for the 200 status. If we run into the 403 Forbidden error when TMDB detects that we’re a bot. A simple fix is changing the User-Agent in settings.py:\n\nUSER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36\"\n\nThis makes our scraper appear as a normal browser request, reducing the chance of being blocked."
  },
  {
    "objectID": "posts/HW2/index.html#building-the-scrapy-spider",
    "href": "posts/HW2/index.html#building-the-scrapy-spider",
    "title": "Data-Driven Movie Recommendations: Scrapy Web Scraping and Actor Network Graphs with NetworkX",
    "section": "Building the Scrapy Spider",
    "text": "Building the Scrapy Spider\nNow that our Scrapy project is set up and we have analyzed TMDB website structure, we can implement the three parsing methods in our spider:\n\nparse(): Navigates from the movie page to the Full Cast & Crew page.\nparse_full_credits(): Extracts the list of actors in the movie.\nparse_actor_page(): Extracts movies from the actor’s Acting section.\n\nEach of these methods will systematically extract the data we need. We’ll write them into the tmdb_spider.py file.\n\nNavigating to the Cast & Crew Page\nparse(): The parse() method starts from a movie’s main page and navigates to its Full Cast & Crew page. Since the cast page always follows the pattern &lt;movie_url&gt;/cast, we can simply append /cast to the current URL.\nImplementation:\n\ndef parse(self, response):\n    \"\"\"\n    Navigates from the movie page to the Full Cast & Crew page.\n    \"\"\"\n    cast_url = response.url + \"/cast\" # Appends \"/cast\" to the end of the current URL.\n    \n    # Creates a Scrapy request to Cast URL and calls parse_full_credits().\n    yield scrapy.Request(url=cast_url, callback=self.parse_full_credits)\n\nThis method works by:\n\nExtracting the Full Cast & Crew URL by appending “/cast” to the movie’s base URL.\nCreating a Scrapy request to that URL and calling parse_full_credits() once the page is loaded.\n\nThis method does not extract any data but ensures we reach the correct page for scraping actors.\n\n\nExtracting Actor Links\nparse_full_credits(): Once we reach the Full Cast & Crew page, we need to extract only the actors listed in the cast section.\nImplementation:\n\ndef parse_full_credits(self, response):\n    \"\"\"\n    Extracts actors from the Full Cast & Crew page \n    and yields requests for each actor’s page.\n    \"\"\"\n    # Selects all actor profile links &lt;a&gt; elements \n    # inside the cast list &lt;ol class=\"people credits\"&gt;.\n    # Extracts the href attribute, which contains the link to the actor’s TMDB page.\n    actor_links = response.css('ol.people.credits li[data-order] div.info a[href^=\"/person/\"]::attr(href)').getall()\n\n    # Yields a new request for each actor’s profile page, calling parse_actor_page().\n    for link in actor_links:\n        yield scrapy.Request(url=response.urljoin(link), callback=self.parse_actor_page)\n\nThis method works by:\n\nSelecting all actor profile links &lt;a&gt; elements inside the cast list &lt;ol class=\"people credits\"&gt;.\nExtracting the href attribute, containing the link to the actor’s TMDB page.\nYielding a new request for each actor’s profile page, calling parse_actor_page() for further processing.\n\nThis method ensures that only actors (not crew members) are processed.\n\n\nExtracting Movie and TV Show Titles\nparse_actor_page() On an actor’s TMDB page, movies and TV shows are categorized under multiple sections like Acting, Writing, Directing, and Producing. We must extract only titles under the “Acting” section.\nImplementation:\n\ndef parse_actor_page(self, response):\n        \"\"\"\n        Extracts actor's name and their acting roles, yielding unique movie/TV titles.\n        \"\"\"\n        # Selector extracts actor's name\n        actor_name = response.css(\"h2.title a::text\").get()\n        \n        # Selector extracts section headers\n        section_headers = response.css(\"div.credits_list h3::text\").getall()\n\n        # Locates the \"Acting\" section\n        acting_index = section_headers.index(\"Acting\")\n\n        # Get all tables and extract titles from the Acting section\n        all_tables = response.css(\"div.credits_list table.card\")\n        acting_table = all_tables[acting_index]\n        titles = acting_table.css(\"a.tooltip bdi::text\").getall()\n\n        for movie_or_TV_name in titles:\n            yield {\"actor\": actor_name, \"movie_or_TV_name\": movie_or_TV_name}\n\nThis method works by:\n\nExtracting the actor’s name from the &lt;h2 class=\"title\"&gt; section.\nFinding all section headers (e.g., “Production”, “Acting”) to locate the Acting section.\nFinding the matching table for “Acting” and extracting all movie/TV show titles inside.\nYielding a dictionary containing (actor, movie/TV show) pairs.\n\nThis method ensures that only acting credits are included.\n\n\nRunning the Scraper\nBefore we run our scraper, we can check that our CSS selectors are working by running this in the terminal:\n\nscrapy shell \"your_page_link\"\n\nWithin the shell for each page, we can run each selector in our three parse methods to see if it yields the desired output.\nAfter checking our selectors, we are ready to run the scraper with the following line:\n\nscrapy crawl tmdb_spider -o results.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\n\nThis command will:\n\nStart at the Harry Potter and the Sorcerer’s Stone movie page.\nExtract all actors from the Full Cast & Crew page.\nVisit each actor’s profile and extract all movies in which they have acted.\nSave the results in results.csv.\n\nOur CSV file should look like:\n\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nPaul Marc Davis\nArthur & Merlin: Knights of Camelot\n\n\n1\nPaul Marc Davis\nClass\n\n\n2\nPaul Marc Davis\nSon of God\n\n\n3\nPaul Marc Davis\nThe Bible\n\n\n4\nPaul Marc Davis\nThe Sky in Bloom\n\n\n...\n...\n...\n\n\n3121\nRupert Grint\nThe View\n\n\n3122\nRupert Grint\nGMTV\n\n\n3123\nRupert Grint\nThe Tonight Show with Jay Leno\n\n\n3124\nRupert Grint\nAn Audience with...\n\n\n3125\nRupert Grint\nToday\n\n\n\n\n3126 rows × 2 columns\n\n\n\nNow that we have successfully scraped the data, the next step is to analyze the results and create a visualization of shared actors across movies. In the next section, we will:\n\nFilter the data to find movies with the most shared actors.\nCreate a network graph to visually represent relationships between movies and actors."
  },
  {
    "objectID": "posts/HW2/index.html#visualizing-movie-recommendations",
    "href": "posts/HW2/index.html#visualizing-movie-recommendations",
    "title": "Data-Driven Movie Recommendations: Scrapy Web Scraping and Actor Network Graphs with NetworkX",
    "section": "Visualizing Movie Recommendations",
    "text": "Visualizing Movie Recommendations\nOnce the scraper is complete, we need to analyze the results and present them in a meaningful way. Since our dataset consists of movies and actors, a network graph is an intuitive choice for visualization. A network graph allows us to see which movies share actors and how strongly they are connected.\n\nProcessing the Scraped Data\nThe CSV output from our Scrapy spider contains two columns:\n\nActor: The name of the actor.\nMovie or TV Show: The name of the movie or TV show they appeared in.\n\nTo build a movie recommendation system, we’ll look for movies that share multiple actors by the following:\n\nGrouping actors by movie: Create a mapping of each movie to the list of actors who starred in it.\nCompute shared actors: Identify pairs of movies that share actors.\nFilter based on shared actor count: We only include movies that share at least 5 actors in the final visualization.\n\n\n\nBuilding the Network Graph\nWe’ll use NetworkX to create a graph where:\n\nNodes represent movies and actors.\nEdges exist between a movie node and an actor node if the actor starred in the movie.\nMovies with more shared actors are emphasized by larger node size and distinct colors.\n\nAdditionally, in order to enhance readability, we’ll apply:\n\nNode scaling to make movies with more shared actors appear larger.\nColor encoding to assign colors to movie nodes using a thermal colormap, where warmer colors (yellow) indicate more shared actors and cooler colors less.\n\n\n\nDeploying the Movie Network Graph\nFirst, we’ll need to import the necessary packages to create our visualization:\n\nimport networkx as nx\nimport plotly.graph_objects as go\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nNext, we will write a function that will take our CSV file and a minimum number of shared actors per movie as inputs (only recommend movies and TV shows with high actor correlation).\nThe following function plots our interactive movie network graph with our desired specs:\n\ndef generate_movie_actor_network(csv_file, min_shared_actors):\n    \"\"\"\n    Generates a movie-actor network graph from a CSV file containing movie and actor relationships.\n    \n    \"\"\"\n\n    # Loads in the dataset\n    df = pd.read_csv(csv_file)\n\n    # Create a graph\n    G = nx.Graph()\n\n    # Build the mapping of movies to actors\n    movie_actor_map = {}\n    for _, row in df.iterrows():\n        movie = row[\"movie_or_TV_name\"]\n        actor = row[\"actor\"]\n        if movie not in movie_actor_map:\n            movie_actor_map[movie] = set()\n        movie_actor_map[movie].add(actor)\n\n    # Identify valid movies that share at least `min_shared_actors` actors with another movie\n    valid_movies = set()\n    for movie1 in movie_actor_map:\n        for movie2 in movie_actor_map:\n            if movie1 != movie2:\n                shared_actors = movie_actor_map[movie1] & movie_actor_map[movie2]\n                if len(shared_actors) &gt;= min_shared_actors:\n                    valid_movies.add(movie1)\n                    valid_movies.add(movie2)\n\n    # Compute the number of unique actors in each valid movie\n    movie_shared_counts = {\n        movie: len(movie_actor_map[movie]) for movie in valid_movies\n    }\n    max_shared = max(movie_shared_counts.values(), default=1)  # Avoid division by zero\n\n    # Normalize sizes (scale between 10 and 55 for visualization purposes)\n    def scale_size(value, min_size=10, max_size=55):\n        return min_size + ((value - 3) / (max_shared - 3)) * (max_size - min_size) if max_shared &gt; 3 else min_size\n\n    # Assign movie colors based on shared actor count using thermal color scale.\n    norm = plt.Normalize(vmin=min(movie_shared_counts.values(), default=0), vmax=max_shared)\n    thermal_colormap = plt.get_cmap(\"viridis\")\n\n    movie_colors = {\n        movie: f'rgb({int(r*255)},{int(g*255)},{int(b*255)})'\n        for movie, (r, g, b, _) in zip(\n            movie_shared_counts.keys(),\n            [thermal_colormap(norm(v)) for v in movie_shared_counts.values()]\n        )\n    }\n\n    # Add nodes and edges to the graph\n    for movie in valid_movies:\n        G.add_node(movie, type=\"movie\", size=scale_size(movie_shared_counts[movie]), color=movie_colors[movie])\n\n    for movie in valid_movies:\n        for actor in movie_actor_map[movie]:\n            if actor not in G:\n                G.add_node(actor, type=\"actor\", size=8, color=\"lightgray\")  # Smaller nodes for actors\n            G.add_edge(movie, actor)\n\n    # Compute graph layout for better spacing\n    pos = nx.spring_layout(G, seed=42, k=1.7)  # 'k' controls the node spacing\n\n    # Create scatter plot for nodes\n    node_trace = []\n    for node in G.nodes():\n        node_type = G.nodes[node][\"type\"]\n        x, y = pos[node]\n        color = G.nodes[node][\"color\"]\n        size = G.nodes[node][\"size\"]\n\n        # Set hover text for nodes\n        hover_text = node if node_type == \"actor\" else f\"{node}&lt;br&gt;Shared Actors: {movie_shared_counts[node]}\"\n\n        node_trace.append(go.Scatter(\n            x=[x], y=[y],\n            mode=\"markers\" if node_type == \"actor\" else \"markers+text\",\n            marker=dict(size=size, color=color, opacity=0.9, showscale=False),\n            hovertext=hover_text,\n            textposition=\"top center\" if node_type == \"movie\" else None\n        ))\n\n    # Create edges for connections between movies and actors\n    edge_trace = []\n    for edge in G.edges():\n        x0, y0 = pos[edge[0]]\n        x1, y1 = pos[edge[1]]\n        edge_trace.append(go.Scatter(\n            x=[x0, x1, None], y=[y0, y1, None],\n            mode=\"lines\",\n            line=dict(width=1, color=\"gray\"),\n            hoverinfo=\"none\",\n            opacity=0.2  # Light transparency for better visibility\n        ))\n\n    # Add a color bar to indicate shared actors\n    colorbar_trace = go.Scatter(\n        x=[None], y=[None],\n        mode='markers',\n        marker=dict(\n            colorscale=\"viridis\",  # Thermal color scheme\n            cmin=min(movie_shared_counts.values(), default=0),\n            cmax=max_shared,\n            showscale=True,\n            colorbar=dict(\n                title=\"# of Shared Actors\",\n                titleside=\"right\",  # Aligns title to the right\n                tickmode=\"array\",\n                tickvals=list(range(0, int(max_shared) + 1, 10)),\n                ticktext=[str(tick) for tick in range(0, int(max_shared) + 1, 10)],\n                tickfont=dict(size=10),  \n                len=1,  \n                thickness=20,  \n                outlinewidth=1.3,\n                xpad=6,  \n                x=0.97  \n            )\n        ),\n        hoverinfo='none'\n    )\n\n    # Create final figure\n    fig = go.Figure(data=edge_trace + node_trace + [colorbar_trace])\n    fig.update_layout(\n        title=dict(\n            text=\"Movie Recs: Movie-Actor Network Graph\",\n            x=0.427,  \n            xanchor=\"center\",\n            yanchor=\"top\",\n            font=dict(size=20)  \n        ),\n        showlegend=False,\n        hovermode=\"closest\",\n        margin=dict(b=20, l=5, r=140, t=40),\n        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n        plot_bgcolor=\"white\",  \n        paper_bgcolor=\"white\"  \n    )\n\n    # Display the graph\n    return fig\n\nWe can call our plotting function with our CSV and minimum shared actors by running the following:\n\nfig = generate_movie_actor_network(csv_file = \"results.csv\", \n                                   min_shared_actors=5)\n\nfig.show()\n\n\n\n\n\n\nInterpretting the Visualization\nBy analyzing the graph, we can identify:\n\nHighly interconnected movies: Franchises and sequels (e.g., Harry Potter Series) tend to cluster together.\nUnexpected connections: Some movies share actors despite belonging to different genres.\nStrongest recommendations: The best recommendations are movies that share the most actors with the chosen starting movie.\n\nThis approach demonstrates how web scraping and network analysis can be combined to generate meaningful movie recommendations based purely on cast overlap, without using machine learning.\nResources Used (For NetworkX Visualization):\n\nhttps://plotly.com/python/network-graphs/\nhttps://community.plotly.com/t/is-plotly-able-to-integrate-with-networkx-graph/59115"
  },
  {
    "objectID": "posts/Final_Project/index.html#introduction",
    "href": "posts/Final_Project/index.html#introduction",
    "title": "YieldCurveForecaster: 3D Yield Curve Modeling and Forecasting Dashboard",
    "section": "Introduction",
    "text": "Introduction\nA yield curve is a graph that shows the interest rates (yields) of bonds with different maturity dates. It typically represents U.S. Treasury bonds and helps indicate the overall health of the economy.\n\nShort-term bonds (e.g., 3 months) usually have lower yields.\nLong-term bonds (e.g., 30 years) usually have higher yields (risk premium)\n\nShapes tell economic stories:\n\nNormal (upward): Healthy economy\nFlat: Economic transition\nInverted: Recession signal\n\nThis project aims to model the yield curve in three-dimensions, as well as forecast yields using exogenous variables to formulate “regime-awareness” for classic forecasting models (i.e. Nelson-Siegel Models).\nGitHub Repository Link"
  },
  {
    "objectID": "posts/Final_Project/index.html#project-overview",
    "href": "posts/Final_Project/index.html#project-overview",
    "title": "YieldCurveForecaster: 3D Yield Curve Modeling and Forecasting Dashboard",
    "section": "Project Overview",
    "text": "Project Overview\nThe project is split up into three main categories:\n\nData Management: The acquisition of data from FRED API (Yield data, CPI, VIX), SQLite database formation, and SQL query functions.\nForecasting Models (Mathematical Modeling): The forecasting models (dynamic exponential decay, AFNS, etc.).\nBacktesting & Analysis: The functions used to backtest each model and calculate the error metrics/statistics.\n\nThere are other important modules such as the plotting module and dash app module, but for the sake of the entirety of the project, the forecasting module was the main focus of work and research.\n\nAt the project’s foundation, lies an SQLite database that stores historical yield curve data, inflation metrics, and market volatility indices. The data layer (data.py) handles all the database operations using parameterized queries and includes ETL (Extract, Transform, Load) functionality. This ensures that time-based and maturity-related data is cleaned up and standardized, making it ready for use down the line. The mathematical modeling layer (models.py and utils.py) implements the Nelson-Siegel framework and “regime-awareness” forecasting algorithms, containing the complex mathematics of yield curve dynamics. The analytics layer (analysis.py) provides backtesting capabilities and statistical analysis functions that operate on both historical and forecasted data.\nThe project’s visualizations and user interactions are built on Dash, implementing a reactive programming model where UI components respond to user inputs through callback functions. This setup ensures a smooth and connected workflow, where any changes you make to the parameters in the interface automatically flow through the entire system. It triggers the necessary database queries, runs the right modeling functions, and updates the visualizations seamlessly—all in real time. The application’s state management system ensures that computational resources are used efficiently, with heavy operations like backtesting performed only when explicitly requested."
  },
  {
    "objectID": "posts/Final_Project/index.html#technical-components",
    "href": "posts/Final_Project/index.html#technical-components",
    "title": "YieldCurveForecaster: 3D Yield Curve Modeling and Forecasting Dashboard",
    "section": "Technical Components",
    "text": "Technical Components\nThe main technical components include:\n\nCreating and Interacting with a SQL Database\nComplex Data Visualization via Plotly\nMathematical Modeling\nBuilding a Dynamic Dash App\n\n\nCreating and Interacting with a SQL Database\nThis project sets up a data storage system using an SQLite database, complete with custom query functions designed to pull time-based financial data efficiently. Unlike simpler methods like relying on CSV files, this database is structured to fetch only the exact data required for specific analyses. This approach minimizes memory usage, especially when working with large datasets that cover several decades.\nThe database is organized into separate tables for yield curve data, market volatility metrics, and inflation, all using consistent date formats to ensure everything lines up correctly over time. To keep things secure and flexible, the data access layer uses parameterized queries, which guard against SQL injection risks while offering versatile ways to retrieve the data you need.\nThe first step in the project was to fetch the data from FRED API. The update_db.py module contains multiple functions to fetch data but we’ll focus on fetching the yield data:\n\ndef fetch_treasury_yields(start_date, end_date):\n    \"\"\"\n    Fetch Treasury yield data from FRED for various maturities.\n    \n    Returns:\n        DataFrame with date index and columns for different maturities.\n    \"\"\"\n    logger.info(f\"Fetching Treasury yields from {start_date} to {end_date}\")\n    \n    # FRED series IDs for different Treasury maturities\n    # Format: (maturity_label, series_id)\n    treasury_series = [\n        ('T1M', 'DGS1MO'),   # 1-Month Treasury Constant Maturity Rate\n        ('T3M', 'DGS3MO'),   # 3-Month Treasury Constant Maturity Rate\n        ('T6M', 'DGS6MO'),   # 6-Month Treasury Constant Maturity Rate\n        ('T1', 'DGS1'),      # 1-Year Treasury Constant Maturity Rate\n        ('T2', 'DGS2'),      # 2-Year Treasury Constant Maturity Rate\n        ('T5', 'DGS5'),      # 5-Year Treasury Constant Maturity Rate\n        ('T10', 'DGS10'),    # 10-Year Treasury Constant Maturity Rate\n        ('T30', 'DGS30')     # 30-Year Treasury Constant Maturity Rate\n    ]\n    \n    # Initialize dictionary to store series data\n    yields_data = {}\n    \n    # Fetch each series\n    for label, series_id in treasury_series:\n        try:\n            # Get data from FRED API\n            series = fred.get_series(series_id, start_date, end_date)\n            if not series.empty:\n                yields_data[label] = series\n                logger.info(f\"Successfully fetched {label} yields ({len(series)} observations)\")\n            else:\n                logger.warning(f\"No data returned for {label} (series ID: {series_id})\")\n        except Exception as e:\n            logger.error(f\"Error fetching {label} (series ID: {series_id}): {str(e)}\")\n    \n    # Convert to DataFrame\n    if yields_data:\n        df = pd.DataFrame(yields_data)\n        df.index.name = 'date'\n        \n        # Convert index to datetime if not already\n        if not isinstance(df.index, pd.DatetimeIndex):\n            df.index = pd.to_datetime(df.index)\n            \n        return df\n    else:\n        logger.error(\"Failed to fetch any Treasury yield data\")\n        return pd.DataFrame()\n\nThe main challenge stemmed from dealing with multiple yield maturities fetched from the FRED API. Since variable names can’t start with an integer, I had to rename each maturity by adding a “T” at the beginning. This adjustment later necessitated a custom function to convert these specially named maturity variables back into numerical values for analysis. Additionally, the data had to be concatenated and reshaped (using a “melt” operation) to ensure it was properly structured for the data.py module to query and process efficiently.\nThe data.py module houses all the query functions, some of which require more than just querying between two dates. An example of this is the get_historical_inflation() function, which not only retrieves CPI data but also computes year-over-year inflation rates directly in the query:\n\ndef get_historical_inflation(db_file: str, start_date: str, end_date: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Fetch historical inflation data and compute Year-over-Year (YoY) CPI change.\n\n    Args:\n        db_file (str): Path to SQLite database.\n        start_date (str): Start date in 'YYYY-MM-DD' format.\n        end_date (str): End date in 'YYYY-MM-DD' format.\n\n    Returns:\n        pd.DataFrame: DataFrame with ['date', 'CPI', 'inflation_rate'].\n    \"\"\"\n    query = \"\"\"\n        SELECT date, CPI\n        FROM Inflation\n        WHERE date BETWEEN ? AND ?\n        ORDER BY date ASC\n    \"\"\"\n    try:\n        with sqlite3.connect(db_file) as conn:\n            df = pd.read_sql_query(query, conn, params=(start_date, end_date))\n        \n        df = df.sort_values(\"date\")\n        df[\"inflation_rate\"] = df[\"CPI\"].pct_change(12) * 100  # Compute YoY inflation rate\n        \n        return df.dropna()\n    except Exception as e:\n        print(f\"Error querying inflation data: {e}\")\n        return pd.DataFrame()\n\n\nAll in all, the data management architecture is relatively straightforward, with importing and handling our financial time-series data as the main goal. The database currently has three tables for yields, CPI, and VIX data but can easily be expanded to include more exogenous variables to incorporate into improving the “regime-awareness” hypothesis.\n\n\nComplex Data Visualization via Plotly\nThe project implements sophisticated three-dimensional visualizations using Plotly, creating interactive representations of yield curve surfaces that evolve over time. The 3D visualization features both historical as well as the forecasted yields. Utilizing the 3D surface visualizations in this way, allow us to go beyond two-dimensional charts to represent the multi-dimensional nature of the yield curve.\nThe core visualization function creates a 3D surface representation where:\n\nThe X-axis represents different maturities (from 1-month to 30-year)\nThe Y-axis represents time (both historical and forecasted periods)\nThe Z-axis represents yield values\n\nThe visualization module plotting.py implements a custom 3D surface generation function that creates topographical representations of yield curves for both forecasted and historical data:\n\ndef generate_surface_plot(forecast_model, selected_maturities, db_file, start_date, end_date, vix_data=None):\n    \"\"\"Creates a 3D surface visualization of historical and forecasted yield curves.\n    \n    Purpose:\n    Generates an interactive 3D surface plot showing the evolution of the yield curve\n    over time, including both historical data and forward-looking model forecasts.\n    Selected maturities can be highlighted with individual time series lines.\n    \n    Args:\n        forecast_model: Model name to use for forecasting (\"Heuristic\", \n            \"ImprovedHeuristic\", or \"RLS\").\n        selected_maturities: List of maturities to highlight with individual lines.\n        db_file: Path to SQLite database with yield data.\n        start_date: Start date for historical data in 'YYYY-MM-DD' format.\n        end_date: End date for historical data in 'YYYY-MM-DD' format.\n        vix_data: Optional DataFrame with VIX data for forecast enhancement.\n        \n    Returns:\n        go.Figure: Plotly figure object containing the 3D yield curve visualization.\n        \n    Raises:\n        Exception: If an error occurs during plot generation.\n    \"\"\"\n    try:\n        # Process historical data with memory efficiency in mind\n        historical_df = data.prepare_data(db_file, start_date, end_date)\n        historical_df.index = pd.to_datetime(historical_df.index)\n        \n        # For large datasets, subsample to improve performance while preserving trends\n        if len(historical_df) &gt; 200:\n            historical_df = historical_df.iloc[::2]  # Take every other row\n            \n        # Prepare maturity data for consistent plotting\n        available_maturities = [round(float(col), 4) for col in historical_df.columns]\n        sorted_maturities = sorted(available_maturities, reverse=True)\n        historical_df = historical_df.reindex(columns=config.MATURITIES_LIST)[sorted_maturities]\n        \n        # Convert dates to numeric values for 3D plotting\n        base_date = historical_df.index.min()\n        hist_dates_numeric = (historical_df.index - base_date).days\n        \n        # Create meshgrid for 3D surface - positions represent maturity indices\n        positions = np.arange(len(sorted_maturities))\n        X_hist, Y_hist = np.meshgrid(positions, hist_dates_numeric)\n        Z_hist = historical_df.fillna(0).values\n        \n        # Adapt forecast length based on visualization complexity\n        forecast_steps = 24  # Standard value: 2-year forecast\n        if len(selected_maturities) &gt; 5:\n            # Shorter forecast horizon when displaying many maturity lines\n            forecast_steps = 12\n            \n        # Generate forecast data using the selected model\n        forecast_df = models.forecast_yield_curve(\n            forecast_model, db_file, start_date, end_date, \n            sorted_maturities, forecast_steps=forecast_steps, vix_data=vix_data\n        )\n        \n        # Prepare forecast data for 3D visualization\n        forecast_df[\"maturity_numeric\"] = forecast_df[\"maturity_numeric\"].astype(float)\n        forecast_pivot = forecast_df.pivot(index=\"date\", columns=\"maturity_numeric\", values=\"yield\")\n        forecast_pivot = forecast_pivot.reindex(columns=sorted_maturities).ffill().bfill().fillna(0)\n        \n        # Convert forecast dates to numeric values for plotting\n        forecast_dates_numeric = (forecast_pivot.index - base_date).days.values\n        X_fore, Y_fore = np.meshgrid(positions, forecast_dates_numeric)\n        Z_fore = forecast_pivot.values\n\n        # Adapt surface opacity based on selection mode\n        surface_opacity = 0.05 if selected_maturities else 0.6\n\n        # Create the base figure\n        fig = go.Figure()\n        \n        # Add historical yield curve surface (blue gradient)\n        fig.add_trace(go.Surface(\n            x=X_hist, y=Y_hist, z=Z_hist,\n            colorscale=\"Blues\",\n            opacity=surface_opacity,\n            name=\"Historical Yield Curve\",\n            showscale=False\n        ))\n        \n        # Add forecast yield curve surface (orange-cyan-blue gradient)\n        custom_scale = [[0, 'darkorange'], [0.5, 'cyan'], [1, 'darkblue']]\n        fig.add_trace(go.Surface(\n            x=X_fore, y=Y_fore, z=Z_fore,\n            colorscale=custom_scale,\n            opacity=surface_opacity,\n            name=\"Forecast Yield Curve\",\n            showscale=False\n        ))\n        \n        # Add individual time series lines for selected maturities\n        if selected_maturities:\n            for m in selected_maturities:\n                m_val = round(float(m), 4)\n                if m_val not in sorted_maturities:\n                    continue\n                \n                # Extract historical data for this maturity\n                hist_ts = historical_df[m_val].reset_index()\n                hist_ts.rename(columns={m_val: \"yield\"}, inplace=True)\n                \n                # Get forecast for this specific maturity\n                forecast_ts = models.forecast_individual_maturity(\n                    db_file, start_date, end_date, m_val, \n                    model=forecast_model, vix_data=vix_data\n                )\n                \n                # Combine historical and forecast for continuous visualization\n                combined_ts = pd.concat([hist_ts, forecast_ts], ignore_index=True)\n                combined_ts.sort_values(\"date\", inplace=True)\n                combined_ts['date_str'] = combined_ts['date'].dt.strftime(\"%m-%Y\")\n                combined_ts['date_numeric'] = (combined_ts['date'] - base_date).dt.days\n                \n                # Add the maturity-specific line to the 3D plot\n                pos = sorted_maturities.index(m_val)\n                fig.add_trace(go.Scatter3d(\n                    x=[pos] * len(combined_ts),  # Fixed x position for this maturity\n                    y=combined_ts['date_numeric'],  # Date position\n                    z=combined_ts['yield'],  # Yield values\n                    mode='lines',\n                    line=dict(color='black', width=1.1),\n                    name=f\"{int(m_val)}Y\" if m_val &gt;= 1 else f\"{int(round(m_val*12))}M\",\n                    # Enhanced hover information for financial analysis\n                    hovertemplate=\"&lt;b&gt;Maturity:&lt;/b&gt; \" + f\"{m_val} years\" +\n                                \"&lt;br&gt;&lt;b&gt;Date:&lt;/b&gt; %{customdata[0]}\" +\n                                \"&lt;br&gt;&lt;b&gt;Yield:&lt;/b&gt; %{z:.2f}%\",\n                    customdata=combined_ts[['date_str']].values\n                ))\n        \n        # Configure time axis with year labels\n        end_date_val = forecast_pivot.index.max() if not forecast_pivot.empty else historical_df.index.max()\n        year_ticks = pd.date_range(start=base_date, end=end_date_val, freq='YS')\n        y_tick_vals = [(date - base_date).days for date in year_ticks]\n        y_tick_text = [date.strftime(\"%Y\") for date in year_ticks]\n            \n        # Set layout with optimized aspect ratio and clear axis labels\n        fig.update_layout(\n            scene=dict(\n                xaxis=dict(\n                    title=\"Maturity\",\n                    tickvals=list(range(len(sorted_maturities))),\n                    ticktext=[f\"{int(m)}Y\" if m &gt;= 1 else f\"{int(round(m*12))}M\" for m in sorted_maturities]\n                ),\n                yaxis=dict(\n                    title=\"Time\",\n                    tickvals=y_tick_vals,\n                    ticktext=y_tick_text\n                ),\n                zaxis=dict(title=\"Yield (%)\"),\n                aspectratio=dict(x=1, y=2, z=0.7)\n            ),\n            title_text=f\"Historical & Forecast Yield Curves ({forecast_model} Model)\",\n            margin=dict(l=0, r=0, b=10, t=30),\n            legend=dict(\n                yanchor=\"top\",\n                y=0.99,\n                xanchor=\"left\",\n                x=0.01\n            )\n        )\n        \n        # Clean up large intermediate objects to free memory\n        del X_hist, Y_hist, Z_hist, X_fore, Y_fore, Z_fore\n        gc.collect()\n        \n        return fig\n    except Exception as e:\n        # Log the error for debugging\n        print(f\"Error in generate_surface_plot: {str(e)}\")\n        raise\n\nSome of the challenges that arose when plotting the 3D yield curve came from concatenating the historical and forecasted yield graphs. Making sure that the forecasting graph exhibited continuity (no large gap) was a reoccurring issue.\nOne of the features I wanted the 3D visualization to have was the ability for users to select a specific maturity and be able to view it without the obstruction of the entire three-dimensional plot. Thus, the rendering engine also dynamically adjusts opacity levels based on user-selected maturity, creating a focus effect where the surface becomes semi-transparent when specific yield curves are highlighted.\nThis complex visualization approach transforms numerical yield data into an intuitive visual format that makes patterns, anomalies, and trends immediately apparent (financial shocks like COVID-19 can be easily visible).\nBeyond the 3D visualizations, the plotting.py module contains specialized plot functions for yield curve spreads, inversion analysis, and volatility correlations. These visualizations implement custom color-coding schemes that map domain-specific concepts like recession probabilities to visual elements that financial analysts can readily interpret. These visualizations accompany the main 3D yield curve to better understand how they work with yield curve dynamics.\n\n\n\nMathematical Modeling\nThe core of the application lies in its mathematical modeling capabilities. The system implements multiple forecasting approaches, each with specific strengths for different market environments.\nWe’ll focus on the “Improved-Heuristic” model which utilized CPI and VIX data to create factors used in a simple decay function:\n\\[Y_{t+h,m} = Y_{t,m} \\cdot (1-d_{m,t})^h + \\alpha_{m,h} \\cdot (M_{m,t} - Y_{t,m} \\cdot (1-d_{m,t})^h) + \\epsilon_{t,h,m}\\]\nWhere: - \\(Y_{t,m}\\) is the yield at time \\(t\\) for maturity \\(m\\) - \\(h\\) is the forecast horizon in months - \\(d_{m,t}\\) is the effective decay rate, varying by maturity and time - \\(\\alpha_{m,h}\\) is the horizon-dependent mean reversion strength - \\(M_{m,t}\\) is the maturity-specific mean reversion target - \\(\\epsilon_{t,h,m}\\) is a noise term based on historical volatility\nMaturity-Specific Decay Rates\nThe model implements non-linear scaling of decay rates across the curve:\n\\[d_{m,t} = d_{max} - (d_{max} - d_{min}) \\cdot \\left(\\frac{m - m_{min}}{m_{max} - m_{min}}\\right)^{\\gamma_m} + \\beta_{infl,t}\\]\nWhere: - \\(d_{max}\\) is the maximum decay rate (for shortest maturities) - \\(d_{min}\\) is the minimum decay rate (for longest maturities) - \\(\\gamma_m\\) is a maturity-dependent power factor (0.7 for short maturities, 0.9 for longer ones) - \\(\\beta_{infl,t}\\) is an inflation-based adjustment\nThe model adjusts decay rates across the curve, applying faster decay for short-term rates and slower decay for long-term rates:\n\n# Compute base decay rate with non-linear scaling for short-term rates\nif m &lt;= 1:  # 1 year or less - faster decay initially\n    base_decay = d_max - (d_max - d_min) * np.power((m - m_min) / (m_max - m_min), 0.7)\nelse:\n    base_decay = d_max - (d_max - d_min) * np.power((m - m_min) / (m_max - m_min), 0.9)\n\nInflation and Volatility Adjustments\nThe inflation adjustment term \\(\\beta_{infl,t}\\) is calculated as:\n\\[\\beta_{infl,t} = \\begin{cases}\n-0.08 - 0.02 \\cdot I'_t & \\text{if } I_t &gt; 4\\% \\text{ and } I'_t &gt; 0 \\\\\n-0.05 - 0.01 \\cdot I'_t & \\text{if } I_t &gt; 3\\% \\text{ and } I'_t &gt; 0 \\\\\n-0.03 & \\text{if } 1\\% \\leq I_t &lt; 3\\% \\\\\n-0.015 & \\text{if } 0 \\leq I_t &lt; 1\\% \\\\\n-0.005 + 0.005 \\cdot |I'_t| & \\text{if } I_t &lt; 0\\% \\text{ and } I'_t &lt; 0\n\\end{cases}\\]\nWhere: - \\(I_t\\) is the current inflation rate - \\(I'_t\\) is the inflation momentum (change over recent periods)\nThis is further adjusted by market volatility:\n\\[\\beta_{infl,t} = \\beta_{infl,t} \\cdot V_f\\]\nWhere \\(V_f\\) is a volatility adjustment factor:\n\\[V_f = \\begin{cases}\n1.3 & \\text{if VIX} &gt; 30 \\\\\n1.1 & \\text{if VIX} &gt; 20 \\\\\n0.9 & \\text{if VIX} &lt; 15 \\\\\n1.0 & \\text{otherwise}\n\\end{cases}\\]\nHere, we implement the math using simple else-if statements to refine our bias estimator:\n\n# Base bias calculation with momentum factor - higher inflation = more negative bias\nif recent_inflation &gt; 4:  # Very high inflation\n    base_bias = -0.08 - (0.02 * inflation_trend if inflation_trend &gt; 0 else 0)\nelif recent_inflation &gt; 3:  # High inflation\n    base_bias = -0.05 - (0.01 * inflation_trend if inflation_trend &gt; 0 else 0)\nelif recent_inflation &gt;= 1:  # Normal inflation\n    base_bias = -0.03\n\nDynamic Mean Reversion\nThe mean reversion target \\(M_{m,t}\\) combines historical averages with economic factors:\n\\[M_{m,t} = w_{s,m} \\cdot \\mu_{s,m} + w_{m,m} \\cdot \\mu_{m,m} + w_{l,m} \\cdot \\mu_{l,m}\\]\nWhere: - \\(\\mu_{s,m}\\), \\(\\mu_{m,m}\\), \\(\\mu_{l,m}\\) are short, medium, and long-term historical means - \\(w_{s,m}\\), \\(w_{m,m}\\), \\(w_{l,m}\\) are maturity-specific weights\nThis target is then adjusted by economic factors:\n\\[M_{m,t} = M_{m,t} \\cdot (1 + \\delta_m \\cdot E_t)\\]\nWhere: - \\(E_t\\) is a composite economic factor score - \\(\\delta_m\\) is a maturity-dependent sensitivity (ranging from 0.01 to 0.05)\nNoise Generation\nThe noise term incorporates: 1. Historical volatility 2. Maturity-specific scaling 3. Time horizon effects 4. Volatility regime adjustments\n\\[\\epsilon_{t,h,m} = \\sigma_{m} \\cdot e^{-0.2m} \\cdot \\sqrt{\\min(1, \\frac{h}{12})} \\cdot V_f \\cdot \\mathcal{N}(0.002, 1)\\]\nWhere: - \\(\\sigma_m\\) is the historical standard deviation for maturity \\(m\\) - \\(e^{-0.2m}\\) provides exponential decay in noise with maturity - The square root term increases noise with forecast horizon - \\(V_f\\) is the volatility regime factor - \\(\\mathcal{N}(0.002, 1)\\) is a slightly biased normal distribution\nThis mathematical framework enables the model to adapt to changing market conditions while maintaining economically reasonable forecasts across different maturities and time horizons.\nIn addition to all this, we can classify the variables into simple regimes by incorporating the yield curve spreads (calculated by taking the difference of yields between long and short term maturities):\n\n# Determine curve regime based on inversion status and change direction\nif is_inverted:\n    if spread_change &gt; 0.05:\n        # Inverted but steepening - often signals end of tightening\n        regime_info['curve_regime'] = \"inverted_steepening\"\n    else:\n        # Strongly inverted - typical of late-cycle\n        regime_info['curve_regime'] = \"inverted_flat\"\nelse:\n    if spread_change &gt; 0.1:\n        # Rapidly steepening - often signals easing\n        regime_info['curve_regime'] = \"steepening\"\n    elif spread_change &lt; -0.1:\n        # Flattening - typical of tightening\n        regime_info['curve_regime'] = \"flattening\"\n    else:\n        regime_info['curve_regime'] = \"normal\"\n\nThis regime awareness allows the model to account for business cycle dynamics, adjusting forecasts based on whether the market is in a tightening cycle, easing cycle, or transitioning between regimes.\nThus, combining all the exogenous variables with an easier-to-work-with model (exponential decay) was the focus of research throughout the majority of this project. While results are still not on par with the widespread statistical forecasting models (AFNS), the error metric results are seemingly decent enough that further research likely could reveal a statistically significant improvement in yield curve forecasting.\n\n\nBuilding a Dynamic Dash App\nThe Dash application integrates all the components into a cohesive, interactive user interface. It’s built using a modular architecture that separates concerns between data access, model computation, and presentation, allowing for easier maintenance and future enhancements. The application features three main tabs:\n\nVisualization Tab: Interactive 3D visualization of historical and forecasted yield curves\nBacktesting Tab: Model evaluation and performance metrics across different time horizons\nAnalysis Tab: Statistical analysis, spread visualization, and market regime detection\n\nThe application uses a modular design pattern that separates concerns between different functional areas:\n(Note: The code for the Dash app in dashboard.py is long, thus snippets will be highlighted)\n\ndef create_app():\n    \"\"\"Creates and configures the Dash application for yield curve analysis.\"\"\"\n    # Initialize Dash app with Bootstrap styling\n    app = Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n\n    # Define the application layout with multiple tabs\n    app.layout = html.Div([\n        # Navigation header\n        dbc.NavbarSimple(\n            brand=\"Yield Curve Analysis & Forecasting Dashboard\",\n            brand_href=\"#\",\n            color=\"primary\",\n            dark=True,\n        ),\n        \n        # State storage components\n        dcc.Store(id='current-model-store', data='Heuristic'),\n        dcc.Store(id='selected-maturities-store', data=[]),\n        dcc.Store(id='backtest-state-store', data={}),\n        dcc.Store(id='viz-needs-refresh', data=False),\n        \n        # Main tab structure\n        dbc.Tabs([\n            # Tab 1: 3D Visualization\n            dbc.Tab(label=\"Yield Curve Visualization\", tab_id=\"tab-visualization\", children=[...]),\n            # Tab 2: Backtesting\n            dbc.Tab(label=\"Backtesting & Evaluation\", tab_id=\"tab-backtest\", children=[...]),\n            # Tab 3: Analysis\n            dbc.Tab(label=\"Statistics & Analysis\", tab_id=\"tab-analysis\", children=[...])\n        ], id=\"main-tabs\")\n    ])\n    \n    # Further code\n    # ...\n    \n    return app\n\nThe layout structure follows a hierarchical organization with tabs, rows, columns, and individual components, all styled with Bootstrap for a professional appearance.\nOne of the most important aspects of the Dash app is how it manages state between user interactions:\n\n# State storage components\ndcc.Store(id='current-model-store', data='Heuristic'),\ndcc.Store(id='selected-maturities-store', data=[]),\ndcc.Store(id='backtest-state-store', data={}),\ndcc.Store(id='viz-needs-refresh', data=False),\n\nThese dcc.Store components serve as client-side state containers that persist between callback executions. This approach:\n\nReduces redundant computations by storing user selections and current state\nPrevents unnecessary re-rendering of complex visualizations\nEnables communication between different parts of the application\n\nFor instance, when a user runs a backtest, the application stores the results in backtest-state-store and sets viz-needs-refresh to True, which triggers a refresh of the visualization when the user returns to that tab.\nThis was implemented in order to prevent issues with backtests potentially interfering with the 3D visualization.\nThe application uses a sophisticated callback system to handle user interactions. We’ll look at one of the key callbacks used to change visualizations based on user-selected forecasting models:\n\n@app.callback(\n    [Output(\"yield-curve-graph\", \"figure\"),\n     Output(\"current-model-store\", \"data\"),\n     Output(\"selected-maturities-store\", \"data\")],\n    [Input(\"forecast-model-dropdown\", \"value\"),\n     Input(\"maturities-dropdown\", \"value\")],\n    [State(\"current-model-store\", \"data\"),\n     State(\"selected-maturities-store\", \"data\")],\n    prevent_initial_call=False\n)\ndef update_figure(forecast_model, selected_maturities, current_model, stored_maturities):\n    \"\"\"Updates the 3D yield curve visualization based on user selections.\"\"\"\n    # Use fallback values if inputs are empty/None\n    if forecast_model is None or forecast_model == \"\":\n        forecast_model = current_model or \"Heuristic\"\n    \n    if selected_maturities is None:\n        selected_maturities = stored_maturities or []\n    \n    # Generate the 3D visualization with error handling\n    try:\n        fig = plotting.generate_surface_plot(\n            forecast_model, \n            selected_maturities, \n            config.DB_FILE, \n            config.DEFAULT_START_DATE, \n            config.DEFAULT_END_DATE, \n            vix_data\n        )\n        return fig, forecast_model, selected_maturities\n    except Exception as e:\n        # Create error message figure\n        fig = go.Figure()\n        fig.add_annotation(text=f\"Error: {str(e)}\", \n                          xref=\"paper\", yref=\"paper\", \n                          x=0.5, y=0.5, showarrow=False)\n        return fig, forecast_model, selected_maturities\n\nThis callback demonstrates several important principles:\n\nMultiple outputs: The callback updates both the visualization and store components in a single operation\nState usage: It uses current state values as fallbacks, ensuring the application behaves predictably\nError handling: The callback catches exceptions and provides user-friendly error messages\nParameter delegation: The actual visualization work is delegated to the plotting module\n\n\nThis interactive application makes complex yield curve analysis accessible and intuitive, allowing users to quickly switch between different views, models, and analytical tools without requiring deep technical knowledge of the underlying implementation details."
  },
  {
    "objectID": "posts/Final_Project/index.html#conclusion-and-ethical-considerations",
    "href": "posts/Final_Project/index.html#conclusion-and-ethical-considerations",
    "title": "YieldCurveForecaster: 3D Yield Curve Modeling and Forecasting Dashboard",
    "section": "Conclusion and Ethical Considerations",
    "text": "Conclusion and Ethical Considerations\nThe Yield Curve Forecasting tool provides a comprehensive platform for fixed income analysis, combining sophisticated mathematical modeling with interactive visualizations. The system’s ability to incorporate market regimes, inflation data, and volatility metrics produces forecasts that adapt to changing economic conditions. The backtesting framework provides transparency about model performance, allowing users to make informed decisions based on historical accuracy.\n\nEthics\nWhile the tool provides valuable insights for users ranging from intermediate investors to fixed income analysts, several ethical considerations are important to acknowledge:\n\nModel Transparency: The application makes clear that all forecasts are probabilistic in nature and subject to uncertainty. The backtesting framework helps users understand the limitations of each model, preventing overconfidence in predictions.\nFinancial Access Implications: Tools that enhance fixed income analysis primarily benefit institutional investors and may widen the knowledge gap between professional and retail investors. This raises questions about equitable access to financial technology.\nMarket Impact: If widely adopted, model-based forecasting tools could potentially contribute to herding behavior in markets if many participants use similar models or strategies. The application’s multiple models with different approaches help mitigate this risk.\nData Privacy: While the application uses public data, expansion to include proprietary or alternative data sources would require careful consideration of data privacy and usage rights.\nRegulatory Compliance: Users must ensure that any investment decisions made using the tool comply with relevant regulations, particularly regarding model validation requirements for financial institutions."
  },
  {
    "objectID": "posts/Coin Toss Paradox/index.html#introduction",
    "href": "posts/Coin Toss Paradox/index.html#introduction",
    "title": "A Coin Mixture Paradox: The Interesting Case of Constant Variance",
    "section": "Introduction",
    "text": "Introduction\nRecently, in my stochastic processes discussion, we went over a coin-flipping problem. In my probability course, we’ve covered many coin-toss problems, but I found this particular problem especially interesting—so much so that it sent me down a rabbit hole trying to understand it. The problem went as follows:\nImagine you have two coins. One is perfectly fair (50% chance of heads), and the other is biased, with a 60% chance of heads. Visually, the coins are indistinguishable. We then flip the left coin twice (not knowing if it is fair or biased), thus what is the variance of the number of heads denoted by \\(\\text{Var}(\\text{\\# heads})\\)?"
  },
  {
    "objectID": "posts/Coin Toss Paradox/index.html#coin-flip-variance-solution",
    "href": "posts/Coin Toss Paradox/index.html#coin-flip-variance-solution",
    "title": "A Coin Mixture Paradox: The Interesting Case of Constant Variance",
    "section": "Coin Flip Variance Solution",
    "text": "Coin Flip Variance Solution\nThe solution goes as follows:\nLet\n\\[\nX = X_1 + X_2\n\\]\nwhere \\(X_1\\) and \\(X_2\\) represent the results of the first and second coin flips, respectively (1 = heads, 0 = tails).\nWe want to compute: \\[\n\\operatorname{Var}(X) = \\operatorname{Var}(X_1 + X_2)\n\\]\nUsing the identity: \\[\n\\operatorname{Var}(X_1 + X_2) = \\operatorname{Var}(X_1) + \\operatorname{Var}(X_2) + 2 \\cdot \\operatorname{Cov}(X_1, X_2)\n\\]\n\nNote: The two coin flips are not independent, since flipping one head increases the likelihood that the coin is biased. Hence, we must account for covariance.\n\n\n\nStep 1: Compute \\(\\operatorname{Var}(X_1)\\)\n\\[\n\\operatorname{Var}(X_1) = \\mathbb{E}[X_1^2] - (\\mathbb{E}[X_1])^2\n\\]\nSince \\(X_1 \\in \\{0, 1\\}\\) (Bernoulli random variable), \\(X_1^2 = X_1\\), so: \\[\n\\mathbb{E}[X_1^2] = \\mathbb{E}[X_1]\n\\]\nWe compute \\(\\mathbb{E}[X_1]\\) using the law of total expectation: \\[\n\\mathbb{E}[X_1] = \\mathbb{P}(\\text{Heads}) = \\frac{1}{2}(0.5) + \\frac{1}{2}(0.6) = 0.55\n\\]\n\\[\n\\operatorname{Var}(X_1) = 0.55 - (0.55)^2 = 0.55 - 0.3025 = 0.2475\n\\]\nBy symmetry: \\[\n\\operatorname{Var}(X_2) = \\operatorname{Var}(X_1) = 0.2475\n\\]\n\n\n\nStep 2: Compute \\(\\operatorname{Cov}(X_1, X_2)\\)\nWe use: \\[\n\\operatorname{Cov}(X_1, X_2) = \\mathbb{E}[X_1 X_2] - \\mathbb{E}[X_1]\\mathbb{E}[X_2]\n\\]\n\nCompute \\(\\mathbb{E}[X_1 X_2]\\)\nThis is the probability that both flips are heads: \\[\n\\mathbb{P}(\\text{both H}) = \\mathbb{P}(\\text{both H} \\mid \\text{fair}) \\cdot \\mathbb{P}(\\text{fair}) + \\mathbb{P}(\\text{both H} \\mid \\text{biased}) \\cdot \\mathbb{P}(\\text{biased})\n\\]\n\\[\n= (0.5)(0.5)^2 + (0.5)(0.6)^2 = 0.5(0.25) + 0.5(0.36) = 0.125 + 0.18 = 0.305\n\\]\n\n\nCompute \\(\\mathbb{E}[X_1]\\mathbb{E}[X_2]\\)\n\\[\n= (0.55)^2 = 0.3025\n\\]\nSo, \\[\n\\operatorname{Cov}(X_1, X_2) = 0.305 - 0.3025 = 0.0025\n\\]\n\n\n\n\nFinal Step: Combine All\n\\[\n\\operatorname{Var}(X) = \\operatorname{Var}(X_1) + \\operatorname{Var}(X_2) + 2 \\cdot \\operatorname{Cov}(X_1, X_2)\n\\]\n\\[\n= 0.2475 + 0.2475 + 2(0.0025) = 0.495 + 0.005 = \\boxed{0.5}\n\\]"
  },
  {
    "objectID": "posts/Coin Toss Paradox/index.html#interpretting-our-answer",
    "href": "posts/Coin Toss Paradox/index.html#interpretting-our-answer",
    "title": "A Coin Mixture Paradox: The Interesting Case of Constant Variance",
    "section": "Interpretting Our Answer",
    "text": "Interpretting Our Answer\nI don’t know about you, but I found it hard to believe that the answer was actually 1/2.\nFor example, we know:\n\nA fair coin (p=0.5) flipped twice has variance 0.5\nA biased coin (p=0.6) flipped twice has variance 0.48\nWe have a 50/50 chance of using either coin\n\nWe can also graph the relationship of variance to probability of heads:\n\n\n\n\n\nSo, kind of like a magic trick, how is it possible that with a biased coin, our variance is the same as if we flipped two fair coins?\nThe answer is actually in the wording of the problem. By adding the uncertainty about which coin we’re using (we picked the left coin not knowing whether it’s fair or biased), we perfectly compensate for the reduced variance of the biased coin. On top of that, it wouldn’t matter whether the probability of heads was 0.6 or 0.7 or 0.8. For any probability of heads for the biased coin, our variance of heads for the two coin flips will always be 0.5.\nIt was at this point that I left the discussion knowing the answer to the paradox, however, I still wasn’t fully convinced. Thus, as any curious mathematician would do, I sought to understand why our solution works the way it does by generalizing the problem."
  },
  {
    "objectID": "posts/Coin Toss Paradox/index.html#generalizing-the-problem",
    "href": "posts/Coin Toss Paradox/index.html#generalizing-the-problem",
    "title": "A Coin Mixture Paradox: The Interesting Case of Constant Variance",
    "section": "Generalizing the Problem",
    "text": "Generalizing the Problem\nImagine you have two coins. One is perfectly fair (50% chance of heads), and the other is biased, but you don’t necessarily know its exact bias – just that it has some probability p of landing heads.\nNow, consider this experiment:\n\nYou randomly pick one of the two coins (with equal probability, 0.5 each).\nYou flip the chosen coin twice.\nYou count the total number of heads, let’s call this Y.\n\nLet’s define our variables more formally:\n\n\\(C\\): The random variable representing the chosen coin.\n\n\\(C = F\\) (Fair coin) with \\(P(C=F) = 0.5\\)\n\\(C = B\\) (Biased coin) with \\(P(C=B) = 0.5\\)\n\nProbabilities of Heads:\n\n\\(P(\\text{Heads} | C=F) = p_F = 0.5\\)\n\\(P(\\text{Heads} | C=B) = p_B = p\\) (where \\(0 \\leq p \\leq 1\\))\n\n\\(X_1, X_2\\): The outcomes of the two flips (1 for heads, 0 for tails). These are Bernoulli trials given the chosen coin.\n\\(Y\\): The total number of heads in the two flips. \\(Y = X_1 + X_2\\). Given the coin, \\(Y\\) follows a Binomial distribution \\(\\text{Bin}(n=2, \\text{probability}=p_C)\\).\n\nOur goal is to calculate \\(\\text{Var}(Y)\\), the variance of the total number of heads before we know which coin was chosen.\n\nExplaining the Constant Variance\nThe key to understanding this result lies in the Law of Total Variance (also known as Eve’s Law). For random variables \\(Y\\) and \\(C\\), it states:\n\\[\\text{Var}(Y) = E[\\text{Var}(Y | C)] + \\text{Var}(E[Y | C])\\]\nLet’s break this down:\n\n\\(E[\\text{Var}(Y | C)]\\): The expected value of the conditional variance. This is the average variance within each coin type. We calculate the variance of \\(Y\\) assuming we know which coin was picked (\\(\\text{Var}(Y | C=F)\\) and \\(\\text{Var}(Y | C=B)\\)) and then find the weighted average of these variances based on the probability of picking each coin.\n\\(\\text{Var}(E[Y | C])\\): The variance of the conditional expectation. This measures the variability between the average outcomes of the different coin types. We calculate the expected value of \\(Y\\) assuming we know which coin was picked (\\(E[Y | C=F]\\) and \\(E[Y | C=B]\\)) and then find the variance of these expected values, treating \\(E[Y | C]\\) itself as a random variable that depends on \\(C\\).\n\nLet’s calculate each term.\n\n\nStep 1: Conditional Expectations and Variances\nFirst, let’s find the expected value and variance of \\(Y\\), conditional on knowing which coin was chosen.\n\nGiven the Fair Coin (\\(C=F\\)):\n\nThe number of heads \\(Y\\) follows \\(\\text{Bin}(n=2, p=0.5)\\).\n\\(E[Y | C=F] = n \\cdot p_F = 2 \\times 0.5 = 1\\)\n\\(\\text{Var}(Y | C=F) = n \\cdot p_F \\cdot (1 - p_F) = 2 \\cdot 0.5 \\cdot (1 - 0.5) = 2 \\cdot 0.5 \\times 0.5 = 0.5\\)\n\nGiven the Biased Coin (\\(C=B\\)):\n\nThe number of heads \\(Y\\) follows \\(\\text{Bin}(n=2, p=p)\\).\n\\(E[Y | C=B] = n \\cdot p_B = 2 \\times p\\)\n\\(\\text{Var}(Y | C=B) = n \\cdot p_B \\cdot (1 - p_B) = 2 \\cdot p \\times (1 - p) = 2p(1-p)\\)\n\n\n\n\nStep 2: Calculate \\(E[\\text{Var}(Y | C)]\\)\nThis is the average of the conditional variances, weighted by the probability of choosing each coin:\n\\[E[\\text{Var}(Y | C)] = \\text{Var}(Y | C=F) \\cdot P(C=F) + \\text{Var}(Y | C=B) \\cdot P(C=B)\\]\n\\[E[\\text{Var}(Y | C)] = (0.5) \\cdot (0.5) + (2p(1-p)) \\cdot (0.5)\\]\n\\[E[\\text{Var}(Y | C)] = 0.25 + p(1-p)\\]\n\\[E[\\text{Var}(Y | C)] = 0.25 + p - p^2\\]\n\n\nStep 3: Calculate \\(\\text{Var}(E[Y | C])\\)\nThis is the variance of the conditional means. We have a random variable \\(E[Y | C]\\) which takes the value \\(E[Y | C=F] = 1\\) with probability 0.5, and the value \\(E[Y | C=B] = 2p\\) with probability 0.5.\nTo find its variance, we use \\(\\text{Var}(X) = E[X^2] - (E[X])^2\\).\n\nFirst, find the mean \\(E[E[Y | C]]\\):\n\\[E[E[Y | C]] = E[Y | C=F] \\cdot P(C=F) + E[Y | C=B] \\cdot P(C=B)\\]\n\\[E[E[Y | C]] = (1) \\cdot (0.5) + (2p) \\cdot (0.5)\\]\n\\[E[E[Y | C]] = 0.5 + p\\]\n(Note: By the law of total expectation, this is also \\(E[Y]\\)).\nNext, find the expected value of the square \\(E[(E[Y | C])^2]\\):\n\\[E[(E[Y | C])^2] = (E[Y | C=F])^2 \\cdot P(C=F) + (E[Y | C=B])^2 \\cdot P(C=B)\\]\n\\[E[(E[Y | C])^2] = (1)^2 \\cdot (0.5) + (2p)^2 \\cdot (0.5)\\]\n\\[E[(E[Y | C])^2] = 1 \\cdot 0.5 + 4p^2 \\cdot 0.5\\]\n\\[E[(E[Y | C])^2] = 0.5 + 2p^2\\]\nNow, calculate the variance:\n\\[\\text{Var}(E[Y | C]) = E[(E[Y | C])^2] - (E[E[Y | C]])^2\\]\n\\[\\text{Var}(E[Y | C]) = (0.5 + 2p^2) - (0.5 + p)^2\\]\n\\[\\text{Var}(E[Y | C]) = 0.5 + 2p^2 - (0.25 + p + p^2)\\]\n\\[\\text{Var}(E[Y | C]) = 0.5 + 2p^2 - 0.25 - p - p^2\\]\n\\[\\text{Var}(E[Y | C]) = 0.25 - p + p^2\\]\n\n\n\nStep 4: Combine the Terms\nNow we add the two components according to the Law of Total Variance:\n\\[\\text{Var}(Y) = E[\\text{Var}(Y | C)] + \\text{Var}(E[Y | C])\\]\n\\[\\text{Var}(Y) = (0.25 + p - p^2) + (0.25 - p + p^2)\\]\nNotice how the terms involving \\(p\\) and \\(p^2\\) cancel out!\n\\[\\text{Var}(Y) = 0.25 + 0.25 + (p - p) + (-p^2 + p^2)\\]\n\\[\\text{Var}(Y) = 0.5\\]\nThe variance \\(\\text{Var}(Y)\\) is indeed \\(0.5\\), regardless of the value of \\(p\\).\n\n\nIntuition: Why Does \\(p\\) Cancel Out?\nThe cancellation happens because the two components of the total variance move in opposite directions as the bias \\(p\\) changes:\n\nAverage Within-Coin Variance (\\(E[\\text{Var}(Y | C)] = 0.25 + p(1-p)\\)): This term represents the inherent randomness within each coin type. The variance of a single Bernoulli or Binomial trial is maximized when \\(p=0.5\\). So, as the biased coin’s \\(p\\) moves away from 0.5 (towards 0 or 1), its individual variance \\(2p(1-p)\\) decreases. This makes the average variance term smaller when \\(p\\) is far from 0.5.\nVariance Between Coin Averages (\\(\\text{Var}(E[Y | C]) = 0.25 - p + p^2\\)): This term represents how different the average outcomes are for the two coins. The expected values are \\(E[Y|C=F]=1\\) and \\(E[Y|C=B]=2p\\). When \\(p=0.5\\), both expectations are 1, so there’s no variance between them (\\(\\text{Var}(E[Y|C]) = 0.25 - 0.5 + 0.25 = 0\\)). As \\(p\\) moves away from 0.5, the difference between the average outcomes (1 and 2p) increases, leading to a larger variance between the conditional expectations.\n\nThese two effects perfectly offset each other. As the bias \\(p\\) makes one term smaller, it makes the other term larger by exactly the same amount, keeping their sum constant at 0.5."
  },
  {
    "objectID": "posts/Coin Toss Paradox/index.html#conclusion",
    "href": "posts/Coin Toss Paradox/index.html#conclusion",
    "title": "A Coin Mixture Paradox: The Interesting Case of Constant Variance",
    "section": "Conclusion",
    "text": "Conclusion\nThe constant variance of 0.5 in this coin mixture problem is a fascinating result that stems directly from the Law of Total Variance. While it seems paradoxical that the overall variability doesn’t depend on the specific bias \\(p\\) of the second coin, the mathematical breakdown shows a perfect cancellation effect. The average variance within the coin types and the variance between the coin types’ average outcomes compensate for each other precisely.\nThis example highlights how decomposing variance can reveal underlying structures and sometimes lead to surprising, constant results even when parameters within the mixture model are changing. It also serves as a reminder that our intuition about how probability distributions combine can sometimes be misleading!"
  },
  {
    "objectID": "posts/HW5/index.html",
    "href": "posts/HW5/index.html",
    "title": "Cat or Dog?: Image Classification in Keras with TensorFlow Datasets, Data Augmentation, & Transfer Learning",
    "section": "",
    "text": "Source: Chan’s Jupyter"
  },
  {
    "objectID": "posts/HW5/index.html#introduction",
    "href": "posts/HW5/index.html#introduction",
    "title": "Cat or Dog?: Image Classification in Keras with TensorFlow Datasets, Data Augmentation, & Transfer Learning",
    "section": "Introduction",
    "text": "Introduction\nImage classification is a fundamental problem in deep learning, enabling medical diagnosis, self-driving cars, and automated image tagging applications. In this blog post, we’ll explore how to build Convolutional Neural Networks (CNNs) to distinguish between cats and dogs using Keras and the TensorFlow Datasets API. We’ll progressively enhance our models using data augmentation, preprocessing, and transfer learning. The goal is to build a model that achieves at least 93% validation accuracy using a pre-trained MobileNetV3 model."
  },
  {
    "objectID": "posts/HW5/index.html#getting-started",
    "href": "posts/HW5/index.html#getting-started",
    "title": "Cat or Dog?: Image Classification in Keras with TensorFlow Datasets, Data Augmentation, & Transfer Learning",
    "section": "Getting Started…",
    "text": "Getting Started…\n\nImporting Relevant Packages\nWe’ll start by importing the necessary packages for our CNN model and adjacent needs. Our imports include libraries such as Keras, TensorFlow, and TensorFlow Datasets (TFDS).\n\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import utils\nimport tensorflow_datasets as tfds\n\n\n\nObtaining & Preparing The Data\nNow, we’ll access the data using a sample data set from Kaggle that contains labeled images of cats and dogs:\n\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\n\n\nThe cats_vs_dogs dataset is loaded and split into training (40%), validation (10%), and test (10%) sets.\nThe dataset contains images of different sizes, so we resize all images to (150, 150), as CNNs require a consistent input shape.\n\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\n\nTo optimize performance, we batch and cache the dataset using prefetch(), ensuring efficient memory usage during training.\n\nfrom tensorflow import data as tf_data\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n\nThe batch_size determines how many data points are gathered from the directory at once.\n\n\nWorking With The Dataset\nWe’ll visualize random samples of cats and dogs to gain insight into our dataset. The dataset is unbatched and iterated over to separate images by label. We can get a piece of a data set using the take method; e.g. train_ds.take(1) will retrieve one batch (32 images with labels) from the training data.\nThe following function creates a two-row visualization where the first row shows three random pictures of cats and the second row shows three random pictures of dogs:\n\ndef visualize_data(dataset, num_images=3):\n    # Separate cats and dogs\n    cats, dogs = [], []\n    \n    # Shuffle the dataset before selecting images\n    shuffled_ds = dataset.unbatch().shuffle(1000)\n    \n    # Collect images for visualization\n    for image, label in shuffled_ds.take(30):  # Sample from dataset\n        if label.numpy() == 0 and len(cats) &lt; num_images:\n            cats.append(image.numpy())\n        elif label.numpy() == 1 and len(dogs) &lt; num_images:\n            dogs.append(image.numpy())\n    \n    # Create the plot\n    fig, axes = plt.subplots(2, num_images, figsize=(15, 8))\n\n    # Plot cats in first row\n    for i in range(num_images):\n        axes[0, i].imshow(cats[i].astype(\"uint8\"))\n        axes[0, i].set_title(\"Cat\")\n        axes[0, i].axis(\"off\")\n\n    # Plot dogs in second row\n    for i in range(num_images):\n        axes[1, i].imshow(dogs[i].astype(\"uint8\"))\n        axes[1, i].set_title(\"Dog\")\n        axes[1, i].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n\nshow_images(train_ds)\n\n2025-03-04 18:10:33.288487: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n\n\n\n\n\n\n\n\n\n\n\nChecking Label Frequencies\nNext, we’ll compute label frequencies, determining how many images belong to each class (cats = 0, dogs = 1). This allows us to establish a baseline accuracy, which is the accuracy of always predicting the most frequent class. If the dataset is imbalanced, this affects model performance, making class balancing or augmentation necessary.\nThe following line of code will create an iterator called labels_iterator:\n\n# Create an iterator for labels\nlabels_iterator = train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\nWe’ll next compute the number of images in the training data with label 0 (corresponding to \"cat\") and label 1 (corresponding to \"dog\"):\n\n# Count occurrences of each label\nlabel_counts = {0: 0, 1: 0}  # 0 for cats, 1 for dogs\nfor label in labels_iterator:\n    label_counts[label] += 1\n\nprint(f\"Number of cat images (label 0): {label_counts[0]}\")\nprint(f\"Number of dog images (label 1): {label_counts[1]}\")\n\nNumber of cat images (label 0): 4637\nNumber of dog images (label 1): 4668\n\n\nFinally, we’ll compute the baseline by taking the majority class and dividing it by the total images:\n\n# Calculate baseline accuracy\ntotal_images = label_counts[0] + label_counts[1]\nmajority_class = max(label_counts, key=label_counts.get)\nbaseline_accuracy = label_counts[majority_class] / total_images\n\nprint(f\"Baseline accuracy (always guessing {majority_class}): {baseline_accuracy:.2%}\")\n\nBaseline accuracy (always guessing 1): 50.17%\n\n\nThe baseline model always predicts the most frequent class. If the dataset is balanced (approximately equal numbers of cats and dogs), the baseline accuracy would be around 50%. If there’s an imbalance, the baseline would be slightly higher. This serves as our benchmark, thus our models should perform significantly better to be considered useful."
  },
  {
    "objectID": "posts/HW5/index.html#first-model-cnn-from-scratch",
    "href": "posts/HW5/index.html#first-model-cnn-from-scratch",
    "title": "Cat or Dog?: Image Classification in Keras with TensorFlow Datasets, Data Augmentation, & Transfer Learning",
    "section": "First Model: CNN from Scratch",
    "text": "First Model: CNN from Scratch\nOur first approach involves training a CNN from scratch. The model consists of:\n\nThree convolutional layers (Conv2D) with ReLU activation to extract features.\nMax pooling layers (MaxPooling2D) to reduce spatial dimensions.\nA fully connected (Dense) layer for classification.\nA dropout layer to prevent overfitting.\n\nWe’ll build the model (model1) using keras.Sequential and the layers mentioned above:\n\n# Create the first model\nmodel1 = keras.Sequential([\n    # Input Layer: Defines the input shape for the model (150x150, 3 channel RGB)\n    keras.layers.Input(shape=(150, 150, 3)),\n    \n    # First Convolutional Layer\n    keras.layers.Conv2D(32, (3, 3), activation='relu'),  # 32 filters, 3x3 kernel, ReLU \n    keras.layers.MaxPooling2D((2, 2)),  # Max pooling to reduce spatial dimensions\n    \n    # Second Convolutional Layer\n    keras.layers.Conv2D(64, (3, 3), activation='relu'),  # 64 filters, 3x3 kernel\n    keras.layers.MaxPooling2D((2, 2)),  # Max pooling again to reduce dimensions\n    \n    # Third Convolutional Layer\n    keras.layers.Conv2D(128, (3, 3), activation='relu'),  # 128 filters, 3x3 kernel\n    keras.layers.MaxPooling2D((2, 2)),  # Further downsampling\n\n    # Flatten layer converts 2D feature maps into a 1D feature vector\n    keras.layers.Flatten(),\n\n    # Fully Connected (Dense) Layer\n    keras.layers.Dense(128, activation='relu'),  # 128 neurons, ReLU activation\n\n    # Dropout layer helps prevent overfitting by randomly dropping connections\n    keras.layers.Dropout(0.5),  # 50% of neurons are dropped during training\n\n    # Output layer - 2 neurons (for cats and dogs) \n    keras.layers.Dense(2, activation='softmax') # Softmax for normalizing output \n])\n\nWe can inspect the model summary by running:\n\nmodel1.summary()\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv2d (Conv2D)                 │ (None, 148, 148, 32)   │           896 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d (MaxPooling2D)    │ (None, 74, 74, 32)     │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_1 (Conv2D)               │ (None, 72, 72, 64)     │        18,496 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_1 (MaxPooling2D)  │ (None, 36, 36, 64)     │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_2 (Conv2D)               │ (None, 34, 34, 128)    │        73,856 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_2 (MaxPooling2D)  │ (None, 17, 17, 128)    │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (Flatten)               │ (None, 36992)          │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (Dense)                   │ (None, 128)            │     4,735,104 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (Dropout)               │ (None, 128)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ (None, 2)              │           258 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 4,828,610 (18.42 MB)\n\n\n\n Trainable params: 4,828,610 (18.42 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nNow we’re ready to train the model. We’ll compile the model using the Adam optimizer and binary cross-entropy loss since this is a binary classification task. The model is trained for 20 epochs, and we plot the training vs. validation accuracy.\nWe’ll train the model on the Dataset by running:\n\n# Compile the model\nmodel1.compile(\n    optimizer='adam',\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=['accuracy']\n)\n\n# Train the model\nhistory1 = model1.fit(\n    train_ds,\n    epochs=20,\n    validation_data=validation_ds\n)\n\nEpoch 1/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 15s 84ms/step - accuracy: 0.5454 - loss: 22.4823 - val_accuracy: 0.5709 - val_loss: 0.6765 Epoch 2/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 13s 38ms/step - accuracy: 0.5722 - loss: 0.6805 - val_accuracy: 0.5408 - val_loss: 0.6803 Epoch 3/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 38ms/step - accuracy: 0.5812 - loss: 0.6627 - val_accuracy: 0.6333 - val_loss: 0.6401 Epoch 4/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 38ms/step - accuracy: 0.6502 - loss: 0.6238 - val_accuracy: 0.5937 - val_loss: 0.6889 Epoch 5/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 40ms/step - accuracy: 0.6642 - loss: 0.5986 - val_accuracy: 0.6191 - val_loss: 0.6914 Epoch 6/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 39ms/step - accuracy: 0.6528 - loss: 0.6232 - val_accuracy: 0.6053 - val_loss: 0.6762 Epoch 7/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 41ms/step - accuracy: 0.7014 - loss: 0.5594 - val_accuracy: 0.6088 - val_loss: 0.7254 Epoch 8/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 41ms/step - accuracy: 0.7206 - loss: 0.5276 - val_accuracy: 0.6002 - val_loss: 0.8139 Epoch 9/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 39ms/step - accuracy: 0.7408 - loss: 0.5093 - val_accuracy: 0.6204 - val_loss: 0.9211 Epoch 10/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 40ms/step - accuracy: 0.7735 - loss: 0.4678 - val_accuracy: 0.6397 - val_loss: 0.8101 Epoch 11/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 40ms/step - accuracy: 0.8102 - loss: 0.4050 - val_accuracy: 0.6346 - val_loss: 0.8699 Epoch 12/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 39ms/step - accuracy: 0.8221 - loss: 0.3841 - val_accuracy: 0.6393 - val_loss: 0.9269 Epoch 13/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 41ms/step - accuracy: 0.8454 - loss: 0.3530 - val_accuracy: 0.6234 - val_loss: 0.8182 Epoch 14/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 41ms/step - accuracy: 0.8617 - loss: 0.3357 - val_accuracy: 0.6169 - val_loss: 0.9354 Epoch 15/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 41ms/step - accuracy: 0.8848 - loss: 0.2723 - val_accuracy: 0.6161 - val_loss: 1.0123 Epoch 16/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 39ms/step - accuracy: 0.9029 - loss: 0.2464 - val_accuracy: 0.6333 - val_loss: 1.0399 Epoch 17/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 39ms/step - accuracy: 0.9149 - loss: 0.2233 - val_accuracy: 0.6354 - val_loss: 1.1485 Epoch 18/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 41ms/step - accuracy: 0.9270 - loss: 0.1924 - val_accuracy: 0.6371 - val_loss: 1.0909 Epoch 19/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 41ms/step - accuracy: 0.9309 - loss: 0.1785 - val_accuracy: 0.6294 - val_loss: 1.1745 Epoch 20/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 40ms/step - accuracy: 0.9467 - loss: 0.1564 - val_accuracy: 0.6298 - val_loss: 1.2176a\nWe’ll need to visualize the training data with matplotlib. The following function will plot the training vs validation accuracy over the number of epochs:\n\ndef history_plot(history, title):\n    plt.figure(figsize=(10, 4))\n    plt.plot(history.history['accuracy'], label='Training Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title(title)\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nWe can now visualize the training vs validation accuracy for our first model:\n\n# Plot the training history\nhistory_plot(history1, 'Model 1: Basic CNN Performance')\n\n\n\n\n\n\n\n\nDiscussion of Model 1: The accuracy of model one stabilized between 60% and 65% during training.\n\nThrough experimentation, it was found that using three convolutional layers instead of two improved validation accuracy. This is likely because additional convolutional layers allow the model to learn more complex spatial features and extract hierarchical patterns from the images. Additionally, adding a fully connected dense layer before the output layer helped improve accuracy by capturing high-level features learned from the convolutional layers and enhancing the model’s ability to classify images correctly.\nThis is a sizeable improvement from the baseline model, performing at least 15% better than random guessing. While seeing some results, there’s significant room for improvement.\nRegarding overfitting, we can observe that the training accuracy is significantly higher than the validation accuracy, suggesting overfitting. The dropout layer helps mitigate this to some extent, but the model is still learning patterns that don’t generalize well to new data."
  },
  {
    "objectID": "posts/HW5/index.html#second-model-model-with-data-augmentation",
    "href": "posts/HW5/index.html#second-model-model-with-data-augmentation",
    "title": "Cat or Dog?: Image Classification in Keras with TensorFlow Datasets, Data Augmentation, & Transfer Learning",
    "section": "Second Model: Model with Data Augmentation",
    "text": "Second Model: Model with Data Augmentation\nIn our second model, we’ll add data augmentation to add some variability in the data during the training process. Data augmentation includes modified copies of the same image in the training set. For example, a picture of a cat is still a picture of a cat even if we flip it upside down or rotate it 90 degrees. We can include such transformed versions of the image in our training process to help our model learn the so-called invariant features of our input images.\nWe’ll use:\n\nRandom horizontal flipping (RandomFlip).\nRandom rotation (RandomRotation).\n\nWe’ll then create a new model (model2), where the data augmentation layers are placed before the CNN layers. The model is trained again for 20 epochs, and the accuracy is plotted.\nLet’s first create a visualization function for the augmentation so we can understand how the augmentation layer is transforming the images:\n\n# Function to visualize augmentation effects\ndef visualize_augmentation(image, augmentation_layer, num_examples=3):\n    plt.figure(figsize=(15, 4))\n\n    # Display original image\n    plt.subplot(1, num_examples + 1, 1)\n    plt.imshow(image.astype(\"uint8\"))\n    plt.title(\"Original\")\n    plt.axis(\"off\")\n\n    # Display a few augmented versions\n    for i in range(num_examples):\n        augmented = augmentation_layer(tf.expand_dims(image, 0))\n        plt.subplot(1, num_examples + 1, i + 2)\n        plt.imshow(tf.squeeze(augmented).numpy().astype(\"uint8\"))\n        plt.title(f\"Augmented {i+1}\")\n        plt.axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n\nRunning this will perform the augmentation and call the visualize_augmentation() function to visualize the transformations:\n\n# Get a sample image\nsample_image = None\nfor images, _ in train_ds.take(1):\n    sample_image = images[0].numpy()\n\n# Create and visualize random flip augmentation\nflip_layer = keras.layers.RandomFlip(\"horizontal\")\nvisualize_augmentation(sample_image, flip_layer)\n\n# Create and visualize random rotation augmentation\nrotation_layer = keras.layers.RandomRotation(0.1)  # 10% rotation range\nvisualize_augmentation(sample_image, rotation_layer)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow that we understand what the augmentation layer is doing, let’s build our model2 with the augmentation layer above our convolutional layers:\n\n# Define augmentation layer outside model to avoid errors\ndata_augmentation = keras.Sequential([\n    keras.layers.RandomFlip(\"horizontal\"),\n    keras.layers.RandomRotation(0.1),\n])\n\n# Creates a model with data augmentation\nmodel2 = keras.Sequential([\n    # Input Layer\n    keras.layers.Input(shape=(150, 150, 3)),\n\n    # Data Augmentation (Applied before CNN layers)\n    data_augmentation,\n\n    # Convolutional Layers (same as model1)\n    keras.layers.Conv2D(32, (3, 3), activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n\n    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n\n    keras.layers.Conv2D(128, (3, 3), activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n\n    # Classification Layers\n    keras.layers.Flatten(),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dropout(0.5),\n\n    # Output Layer\n    keras.layers.Dense(2, activation='softmax')\n])\n\nWe can inspect the model summary by running:\n\nmodel2.summary()\n\nModel: \"sequential_4\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ sequential_3 (Sequential)       │ (None, 150, 150, 3)    │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_6 (Conv2D)               │ (None, 148, 148, 32)   │           896 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_6 (MaxPooling2D)  │ (None, 74, 74, 32)     │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_7 (Conv2D)               │ (None, 72, 72, 64)     │        18,496 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_7 (MaxPooling2D)  │ (None, 36, 36, 64)     │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_8 (Conv2D)               │ (None, 34, 34, 128)    │        73,856 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_8 (MaxPooling2D)  │ (None, 17, 17, 128)    │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_2 (Flatten)             │ (None, 36992)          │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (Dense)                 │ (None, 128)            │     4,735,104 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (Dropout)             │ (None, 128)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_5 (Dense)                 │ (None, 2)              │           258 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 4,828,610 (18.42 MB)\n\n\n\n Trainable params: 4,828,610 (18.42 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nWe’re ready to train model2 and should expect an accuracy higher than model1 with validation accuracy &gt;60%:\n\n# Compile the model\nmodel2.compile(\n    optimizer='adam',\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=['accuracy']\n)\n\n# Train the model\nhistory2 = model2.fit(\n    train_ds,\n    epochs=20,\n    validation_data=validation_ds\n)\n\nEpoch 1/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 46ms/step - accuracy: 0.5288 - loss: 26.1849 - val_accuracy: 0.5310 - val_loss: 0.6883 Epoch 2/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 44ms/step - accuracy: 0.5370 - loss: 0.6912 - val_accuracy: 0.5262 - val_loss: 0.6878 Epoch 3/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 48ms/step - accuracy: 0.5346 - loss: 0.6887 - val_accuracy: 0.5696 - val_loss: 0.6737 Epoch 4/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 44ms/step - accuracy: 0.5464 - loss: 0.6838 - val_accuracy: 0.5778 - val_loss: 0.6710 Epoch 5/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 45ms/step - accuracy: 0.5735 - loss: 0.6753 - val_accuracy: 0.6101 - val_loss: 0.6592 Epoch 6/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 45ms/step - accuracy: 0.5937 - loss: 0.6628 - val_accuracy: 0.6066 - val_loss: 0.6607 Epoch 7/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 45ms/step - accuracy: 0.5960 - loss: 0.6614 - val_accuracy: 0.6462 - val_loss: 0.6308 Epoch 8/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 43ms/step - accuracy: 0.6311 - loss: 0.6440 - val_accuracy: 0.6698 - val_loss: 0.6193 Epoch 9/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 44ms/step - accuracy: 0.6422 - loss: 0.6365 - val_accuracy: 0.6582 - val_loss: 0.6262 Epoch 10/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 44ms/step - accuracy: 0.6358 - loss: 0.6383 - val_accuracy: 0.6724 - val_loss: 0.6090 Epoch 11/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 44ms/step - accuracy: 0.6678 - loss: 0.6161 - val_accuracy: 0.6831 - val_loss: 0.5882 Epoch 12/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 44ms/step - accuracy: 0.6732 - loss: 0.5992 - val_accuracy: 0.7077 - val_loss: 0.5582 Epoch 13/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 44ms/step - accuracy: 0.6849 - loss: 0.5906 - val_accuracy: 0.6758 - val_loss: 0.5839 Epoch 14/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 44ms/step - accuracy: 0.6887 - loss: 0.5934 - val_accuracy: 0.7120 - val_loss: 0.5631 Epoch 15/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 44ms/step - accuracy: 0.7143 - loss: 0.5631 - val_accuracy: 0.7266 - val_loss: 0.5406 Epoch 16/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 44ms/step - accuracy: 0.7324 - loss: 0.5391 - val_accuracy: 0.7373 - val_loss: 0.5249 Epoch 17/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 45ms/step - accuracy: 0.7430 - loss: 0.5221 - val_accuracy: 0.7601 - val_loss: 0.4919 Epoch 18/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 44ms/step - accuracy: 0.7584 - loss: 0.5030 - val_accuracy: 0.7623 - val_loss: 0.5070 Epoch 19/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 44ms/step - accuracy: 0.7579 - loss: 0.4986 - val_accuracy: 0.7786 - val_loss: 0.4747 Epoch 20/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 45ms/step - accuracy: 0.7687 - loss: 0.4849 - val_accuracy: 0.7670 - val_loss: 0.4911\nVisualize the training vs validation accuracy for the second model:\n\n# Plot the training history\nhistory_plot(history2, 'Model 2: CNN with Data Augmentation')\n\n\n\n\n\n\n\n\nDiscussion of Model 2: The accuracy of the second model stabilized between 75% and 80% during training.\n\nThrough experimentation, I found that using RandomFlip(\"horizontal\") instead of \"horizontal_and_vertical\" improved validation accuracy. This makes sense because flipping cat and dog images upside down likely introduced unrealistic variations that confused the model rather than helping it generalize. Similarly, setting RandomRotation to a low value (e.g., 0.1) led to better validation accuracy, as excessive rotation distorted important features, reducing the model’s ability to learn meaningful patterns.\nThis represents an improvement over Model 1. The data augmentation techniques help the model learn more robust features by creating variations of the training images. This allows the model generalize new data better.\nRegarding overfitting, there’s likely still some difference between the training and validation accuracies, but it should be less pronounced than in Model 1. Data augmentation is specifically designed to combat overfitting by introducing variability in the training data."
  },
  {
    "objectID": "posts/HW5/index.html#third-model-data-preprocessingnormalization",
    "href": "posts/HW5/index.html#third-model-data-preprocessingnormalization",
    "title": "Cat or Dog?: Image Classification in Keras with TensorFlow Datasets, Data Augmentation, & Transfer Learning",
    "section": "Third Model: Data Preprocessing/Normalization",
    "text": "Third Model: Data Preprocessing/Normalization\nSince pixel values range from [0, 255] (RGB values), we normalize them to [-1, 1] using the Rescaling layer. By handling the scaling before the training process, we can spend more of our training energy handling actual signals in the data and less energy having the weights adjusted to the data scale. This speeds up convergence and helps the model generalize better.\nThe normalization layer is added before data augmentation in a new model (model3):\n\n# Create preprocessing layer\ni = keras.Input(shape=(150, 150, 3))\n# Scale pixel values from (0, 255) to (-1, 1)\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\nx = scale_layer(i)\n# Create a preprocessing model that applies scaling\npreprocessor = keras.Model(inputs=i, outputs=x)\n\n# Same augmentation layer as in model2\ndata_augmentation = keras.Sequential([\n    keras.layers.RandomFlip(\"horizontal\"),\n    keras.layers.RandomRotation(0.1),\n])\n\n# Create a CNN model with preprocessing and augmentation\nmodel3 = keras.Sequential([\n    # Preprocessing layer\n    preprocessor,\n\n    # Data Augmentation\n    data_augmentation,\n\n    # Convolutional layers (mostly same as model1)\n    # Added padding='same' to preserve spatial features and double the filters\n    keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n    keras.layers.MaxPooling2D((2, 2)),\n\n    # Classification layers\n    keras.layers.Flatten(),\n    keras.layers.Dense(256, activation='relu'),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(2, activation='softmax')\n])\n\nWe can inspect the model summary by running:\n\nmodel3.summary()\n\nModel: \"sequential_6\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ functional_5 (Functional)       │ (None, 150, 150, 3)    │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ sequential_5 (Sequential)       │ (None, 150, 150, 3)    │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_9 (Conv2D)               │ (None, 150, 150, 64)   │         1,792 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_9 (MaxPooling2D)  │ (None, 75, 75, 64)     │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_10 (Conv2D)              │ (None, 75, 75, 128)    │        73,856 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_10 (MaxPooling2D) │ (None, 37, 37, 128)    │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_11 (Conv2D)              │ (None, 37, 37, 256)    │       295,168 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_11 (MaxPooling2D) │ (None, 18, 18, 256)    │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_3 (Flatten)             │ (None, 82944)          │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_6 (Dense)                 │ (None, 256)            │    21,233,920 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (Dropout)             │ (None, 256)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_7 (Dense)                 │ (None, 2)              │           514 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 21,605,250 (82.42 MB)\n\n\n\n Trainable params: 21,605,250 (82.42 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nWe’re ready to train model3 and should expect an accuracy higher than model2 with validation accuracy &gt;80%:\n\n# Compile the model\nmodel3.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Train the model\nhistory3 = model3.fit(\n    train_ds,\n    epochs=20,\n    validation_data=validation_ds\n)\n\nEpoch 1/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 23s 130ms/step - accuracy: 0.5569 - loss: 1.0618 - val_accuracy: 0.6556 - val_loss: 0.6299 Epoch 2/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 16s 111ms/step - accuracy: 0.6349 - loss: 0.6388 - val_accuracy: 0.7175 - val_loss: 0.5751 Epoch 3/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 16s 108ms/step - accuracy: 0.6914 - loss: 0.5869 - val_accuracy: 0.7438 - val_loss: 0.5328 Epoch 4/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 16s 108ms/step - accuracy: 0.6998 - loss: 0.5700 - val_accuracy: 0.7691 - val_loss: 0.5065 Epoch 5/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 16s 108ms/step - accuracy: 0.7189 - loss: 0.5424 - val_accuracy: 0.7812 - val_loss: 0.4774 Epoch 6/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 16s 108ms/step - accuracy: 0.7392 - loss: 0.5240 - val_accuracy: 0.7777 - val_loss: 0.4710 Epoch 7/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 21s 110ms/step - accuracy: 0.7476 - loss: 0.5074 - val_accuracy: 0.7837 - val_loss: 0.4683 Epoch 8/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 16s 111ms/step - accuracy: 0.7643 - loss: 0.4909 - val_accuracy: 0.8014 - val_loss: 0.4380 Epoch 9/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 16s 109ms/step - accuracy: 0.7748 - loss: 0.4756 - val_accuracy: 0.7941 - val_loss: 0.4403 Epoch 10/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 16s 109ms/step - accuracy: 0.7865 - loss: 0.4612 - val_accuracy: 0.7829 - val_loss: 0.4627 Epoch 11/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 20s 108ms/step - accuracy: 0.7889 - loss: 0.4437 - val_accuracy: 0.8121 - val_loss: 0.4130 Epoch 12/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 21s 110ms/step - accuracy: 0.7976 - loss: 0.4301 - val_accuracy: 0.8267 - val_loss: 0.4007 Epoch 13/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 16s 110ms/step - accuracy: 0.8018 - loss: 0.4288 - val_accuracy: 0.8199 - val_loss: 0.4015 Epoch 14/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 16s 111ms/step - accuracy: 0.8231 - loss: 0.4025 - val_accuracy: 0.8181 - val_loss: 0.4436 Epoch 15/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 16s 109ms/step - accuracy: 0.8305 - loss: 0.3857 - val_accuracy: 0.8332 - val_loss: 0.3856 Epoch 16/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 16s 109ms/step - accuracy: 0.8271 - loss: 0.3860 - val_accuracy: 0.8405 - val_loss: 0.3844 Epoch 17/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 20s 109ms/step - accuracy: 0.8322 - loss: 0.3781 - val_accuracy: 0.8487 - val_loss: 0.3722 Epoch 18/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 16s 109ms/step - accuracy: 0.8356 - loss: 0.3702 - val_accuracy: 0.8439 - val_loss: 0.3796 Epoch 19/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 20s 109ms/step - accuracy: 0.8372 - loss: 0.3544 - val_accuracy: 0.8547 - val_loss: 0.3628 Epoch 20/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 16s 109ms/step - accuracy: 0.8495 - loss: 0.3476 - val_accuracy: 0.8517 - val_loss: 0.3548\nVisualize the training vs validation accuracy for the third model:\n\n# Plot the training history\nhistory_plot(history3, 'Model 3: CNN with Preprocessing and Data Augmentation')\n\n\n\n\n\n\n\n\nDiscussion of Model 3: The validation accuracy of the third model reached approximately 80-85% during training.\n\nThrough experimentation, it was found that using \"same\" padding instead of \"valid\" helped preserve spatial features throughout the convolutional layers, leading to improved validation accuracy. This is because \"same\" padding ensures that the output feature maps retain the same spatial dimensions, preventing loss of important edge information. Additionally, doubling the number of filters in each convolutional layer increased accuracy by allowing the model to learn more complex and diverse feature representations, enhancing its ability to distinguish between cats and dogs.\nThis is a significant improvement over both Model 1 and Model 2. The preprocessing helps the model train more efficiently by normalizing the pixel values to the range [-1, 1]. This scaling makes the optimization process more stable and allows the model to learn more effectively.\nRegarding overfitting, data augmentation and proper preprocessing reduced the gap between training and validation accuracy. Now, the model learns more generalizable features rather than memorizing the training data."
  },
  {
    "objectID": "posts/HW5/index.html#fourth-model-transfer-learning-with-mobilenetv3",
    "href": "posts/HW5/index.html#fourth-model-transfer-learning-with-mobilenetv3",
    "title": "Cat or Dog?: Image Classification in Keras with TensorFlow Datasets, Data Augmentation, & Transfer Learning",
    "section": "Fourth Model: Transfer Learning with MobileNetV3",
    "text": "Fourth Model: Transfer Learning with MobileNetV3\nInstead of training a CNN from scratch, we’ll leverage MobileNetV3Large, a pre-trained CNN trained on millions of images. In this model, we’ll freeze the base model’s weights and only train the final classification layers.\nWe’ll download MobileNetV3Large as base_model_layer, adding it in between the augmentation layer and the convolutional layers:\n\n# Set up MobileNetV3Large as a base model\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(\n    input_shape=IMG_SHAPE,\n    include_top=False,\n    weights='imagenet'\n)\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training=False)\nbase_model_layer = keras.Model(inputs=i, outputs=x)\n\n# Same augmentation layer as in model2\ndata_augmentation = keras.Sequential([\n    keras.layers.RandomFlip(\"horizontal\"),\n    keras.layers.RandomRotation(0.1),\n])\n\n# Create a model using transfer learning\nmodel4 = keras.Sequential([\n    keras.Input(shape=IMG_SHAPE),  # Explicit input layer\n\n    # Data augmentation layers\n    data_augmentation,\n\n    # Pre-trained base model\n    base_model_layer,\n\n    # Classification layers\n    # Reduces each feature map to a single value by computing the average \n    keras.layers.GlobalAveragePooling2D(),\n    keras.layers.Dropout(0.3),\n    keras.layers.Dense(2, activation='softmax')\n])\n\nWe can inspect the model summary by running:\n\nmodel4.summary()\n\nModel: \"sequential_12\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ sequential_11 (Sequential)      │ (None, 150, 150, 3)    │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ functional_14 (Functional)      │ (None, 5, 5, 960)      │     2,996,352 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d_2      │ (None, 960)            │             0 │\n│ (GlobalAveragePooling2D)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_6 (Dropout)             │ (None, 960)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_10 (Dense)                │ (None, 2)              │         1,922 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 2,998,274 (11.44 MB)\n\n\n\n Trainable params: 1,922 (7.51 KB)\n\n\n\n Non-trainable params: 2,996,352 (11.43 MB)\n\n\n\nWe’re ready to train model4 and should expect an accuracy higher than model3 with validation accuracy &gt;93%:\n\n# Compile the model\nmodel4.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Train the model\nhistory4 = model4.fit(\n    train_ds,\n    epochs=20,\n    validation_data=validation_ds\n)\n\nEpoch 1/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 17s 72ms/step - accuracy: 0.6088 - loss: 0.7998 - val_accuracy: 0.8710 - val_loss: 0.3336 Epoch 2/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 56ms/step - accuracy: 0.8164 - loss: 0.4177 - val_accuracy: 0.9316 - val_loss: 0.2011 Epoch 3/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 9s 63ms/step - accuracy: 0.8709 - loss: 0.3027 - val_accuracy: 0.9527 - val_loss: 0.1528 Epoch 4/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 56ms/step - accuracy: 0.8982 - loss: 0.2482 - val_accuracy: 0.9596 - val_loss: 0.1279 Epoch 5/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 56ms/step - accuracy: 0.9032 - loss: 0.2350 - val_accuracy: 0.9604 - val_loss: 0.1139 Epoch 6/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 55ms/step - accuracy: 0.9165 - loss: 0.2006 - val_accuracy: 0.9626 - val_loss: 0.1049 Epoch 7/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 54ms/step - accuracy: 0.9232 - loss: 0.1981 - val_accuracy: 0.9626 - val_loss: 0.0998 Epoch 8/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 54ms/step - accuracy: 0.9286 - loss: 0.1818 - val_accuracy: 0.9660 - val_loss: 0.0932 Epoch 9/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 54ms/step - accuracy: 0.9291 - loss: 0.1787 - val_accuracy: 0.9665 - val_loss: 0.0904 Epoch 10/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 54ms/step - accuracy: 0.9289 - loss: 0.1705 - val_accuracy: 0.9665 - val_loss: 0.0875 Epoch 11/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 54ms/step - accuracy: 0.9312 - loss: 0.1674 - val_accuracy: 0.9690 - val_loss: 0.0845 Epoch 12/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 55ms/step - accuracy: 0.9336 - loss: 0.1618 - val_accuracy: 0.9703 - val_loss: 0.0833 Epoch 13/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 9s 62ms/step - accuracy: 0.9332 - loss: 0.1612 - val_accuracy: 0.9708 - val_loss: 0.0816 Epoch 14/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 54ms/step - accuracy: 0.9332 - loss: 0.1673 - val_accuracy: 0.9712 - val_loss: 0.0799 Epoch 15/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 55ms/step - accuracy: 0.9383 - loss: 0.1548 - val_accuracy: 0.9708 - val_loss: 0.0788 Epoch 16/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 55ms/step - accuracy: 0.9353 - loss: 0.1573 - val_accuracy: 0.9729 - val_loss: 0.0784 Epoch 17/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 54ms/step - accuracy: 0.9406 - loss: 0.1486 - val_accuracy: 0.9742 - val_loss: 0.0780 Epoch 18/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 55ms/step - accuracy: 0.9421 - loss: 0.1425 - val_accuracy: 0.9733 - val_loss: 0.0775 Epoch 19/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 55ms/step - accuracy: 0.9389 - loss: 0.1458 - val_accuracy: 0.9738 - val_loss: 0.0748 Epoch 20/20 146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 54ms/step - accuracy: 0.9457 - loss: 0.1372 - val_accuracy: 0.9742 - val_loss: 0.0728\nVisualize the training vs validation accuracy for the fourth model:\n\n# Plot the training history\nhistory_plot(history4, 'Model 4: Transfer Learning with MobileNetV3Large')\n\n\n\n\n\n\n\n\nDiscussion of Model 4: The validation accuracy of the fourth model reached over 95% during training.\n\nThis is a dramatic improvement over all previous models. The pre-trained MobileNetV3Large has already learned robust feature representations from millions of images in the ImageNet dataset. By leveraging these pre-learned features, we can achieve much higher accuracy with very little additional training.\nRegarding the model summary, we can observe that the base model contains the vast majority of the parameters, but since these are frozen (not trainable), we’re only training the final classification layers. This makes the training process very efficient.\nUnlike earlier models, the transfer learning approach generally reduces overfitting since the base model already captures generalizable features. The gap between training and validation accuracy is much smaller than in previous models, suggesting that the model generalizes well to new data.\nTo further optimize the training process, we set the optimizer to Adam with a learning rate of 0.0001. Since we are fine-tuning a pre-trained model, a high learning rate (e.g., 0.001) could overwrite important pre-trained features, leading to suboptimal performance. A lower learning rate (0.0001) ensures the model makes gradual updates, preserving the useful feature representations learned by MobileNetV3, achieving smoother convergence with higher validation accuracy."
  },
  {
    "objectID": "posts/HW5/index.html#score-on-test-data",
    "href": "posts/HW5/index.html#score-on-test-data",
    "title": "Cat or Dog?: Image Classification in Keras with TensorFlow Datasets, Data Augmentation, & Transfer Learning",
    "section": "Score On Test Data",
    "text": "Score On Test Data\nFinally, we’ll evaluate the accuracy of our most performant model (model4) on the unseen test_ds. We should expect an accuracy close to the validation accuracy we reached in the fourth model ~96-97%:\n\n# Evaluate on test data\ntest_loss, test_acc = model4.evaluate(test_ds)\nprint(f\"Test accuracy: {test_acc:.4f}\")\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 4s 94ms/step - accuracy: 0.9713 - loss: 0.0702 Test accuracy: 0.9673\nOur test accuracy falls within the expected range (96.73%), confirming that our most performant model (model4) was properly trained and generalizes well to unseen data. This suggests that the transfer learning approach successfully leveraged pre-trained features, allowing the model to maintain high accuracy on new inputs without overfitting."
  },
  {
    "objectID": "posts/HW0/index.html#introduction",
    "href": "posts/HW0/index.html#introduction",
    "title": "Exploring Palmer Penguins Data: Seaborn Data Visualization With Heat Maps",
    "section": "Introduction",
    "text": "Introduction\nThe Palmer Penguins dataset contains measurements of penguin species from the Palmer Archipelago in Antarctica, including numeric measurements like culmen(bill) length, flipper length, and body mass. In this blog, we will learn how to construct a heatmap to explore correlations between these numerical features across the three penguin species: Adelie, Chinstrap, and Gentoo."
  },
  {
    "objectID": "posts/HW0/index.html#read-in-and-inspect-the-data",
    "href": "posts/HW0/index.html#read-in-and-inspect-the-data",
    "title": "Exploring Palmer Penguins Data: Seaborn Data Visualization With Heat Maps",
    "section": "Read in and Inspect the Data",
    "text": "Read in and Inspect the Data\nWe will begin by reading the data into Python by running:\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\nThe first line imports the Pandas package into our project. We will use it to read the CSV file and manipulate/analyze data. After setting the variable “url” to our CSV URL, we can use the Pandas read CSV function to store the data frame as “penguins.”\nNext, we will inspect the data by running:\n\npenguins.head()\n\n\n\n\nPalmer Penguin data first 5 rows output"
  },
  {
    "objectID": "posts/HW0/index.html#cleaning-our-data",
    "href": "posts/HW0/index.html#cleaning-our-data",
    "title": "Exploring Palmer Penguins Data: Seaborn Data Visualization With Heat Maps",
    "section": "Cleaning Our Data",
    "text": "Cleaning Our Data\nAnalyzing the first five rows of the data reveals the columns we need to focus on. Since we want to find correlations between culmen length, depth, flipper length, and body mass for each species, we must manipulate the data frame to include only the relevant columns. We can achieve this using the iloc function.\nWe can create a new data frame with our selected columns by running the following code:\n\n# iloc selects columns from penguins data frame using column index positions\n# ':' selects all rows from the DataFrame\n#  '[2, 9, 10, 11, 12]' selects the columns at index positions 2, 9, 10, 11, and 12\npenguin_data = penguins.iloc[:,[2,9,10,11,12]]\n\nThe output will return a data frame, asigned to “penguin_data,” with only our desired columns. However, we still need to clean the data. Some rows in our data frame are missing inputs indicated by “NaN.” We can remove those rows with the “dropna” function.\nRunning the following code will remove all rows with missing data:\n\n# removes rows with missing values \"NaN\"\npenguin_data = penguin_data.dropna()\n\nWe can again check what our data looks like now by running:\n\npenguin_data.head()\n\n\n\n\nPalmer Penguin cleaned data first 5 rows output\n\n\nWith this, our data looks ready to be used."
  },
  {
    "objectID": "posts/HW0/index.html#create-correlation-heat-maps-by-species",
    "href": "posts/HW0/index.html#create-correlation-heat-maps-by-species",
    "title": "Exploring Palmer Penguins Data: Seaborn Data Visualization With Heat Maps",
    "section": "Create Correlation Heat Maps by Species",
    "text": "Create Correlation Heat Maps by Species\nFirst, we must import the relevant packages for our correlation heat maps:\n\nimport seaborn as sns # Used for plotting the heat map visualization\nimport matplotlib.pyplot as plt # Used to for annotating visualization and giving specs\n\nSince we want to create heat maps for each penguin species, we must write a function that 1.) groups the data by species, 2.) calculates the correlation matrix for each group, and 3.) plots the matrices of each group.\nLet’s name the function: “palmer_penguin_heatmap,” which takes in our data frame, a key that will group our data by (in our case, “Species”), and a list of columns that we would like to include in the correlation.\nRunning the following code will establish our function:\n\ndef palmer_penguin_heatmap(dataset, key, cols):\n    \"\"\"\n    Calculates the correlation matrices for each species and plots the heatmap.\n\n    Params:\n    -&gt; dataset (pandas df): The dataset (penguin_data).\n    -&gt; key (str): Column that will group data by (\"Species\").\n    -&gt; cols (list): List of columns for correlation analysis.\n    \"\"\"\n    grouped = dataset.groupby(key) # groups data set by species\n    \n    for species, group in grouped:\n        corr_matrix = group[cols].corr()  # Calculate correlation matrix\n        plt.figure(figsize=(8, 6))  # Create a new figure\n        sns.heatmap(corr_matrix, annot=True, cmap='crest', fmt=\".2f\")  # Plots heatmap\n        plt.title(f\"Correlation Matrix for {species} Penguins\")  # Add title for given species\n        plt.show()  # Display the heatmap\n\nAfter running our function, we are almost ready to call the function with our parameters. We have our data set and key right now, but we must define which columns we want to use for the correlation analysis.\nWe can do this by running:\n\n# Stores a list of column names that we wish to analyze \nnum_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nWe are ready to call our function “palmer_penguin_heatmap” with our three parameters. We should expect our output to be three heat maps for Adelie, Chinstrap, and Gentoo.\nWe can call our function by running:\n\n# Calls the function with our penguin data, groupby key, and target columns\npalmer_penguin_heatmap(dataset = penguin_data, key = 'Species', cols = num_col)\n\nOur outputs should look as follows:"
  },
  {
    "objectID": "posts/HW0/index.html#interpreting-the-heat-maps",
    "href": "posts/HW0/index.html#interpreting-the-heat-maps",
    "title": "Exploring Palmer Penguins Data: Seaborn Data Visualization With Heat Maps",
    "section": "Interpreting the Heat Maps",
    "text": "Interpreting the Heat Maps\nEach heat map is titled with the corresponding species and labeled with measurements to help us interpret the data. Heat maps visualize correlation matrices. The color-coded squares help the viewer interpret higher correlations (denoted by the color bar on the right of the heat map). Thus, each square corresponds to the correlation of two select columns (measurements). As seen by the dark squares, any measurement compared to itself correlates to 1.00. These squares help us see where specific measurements may be associated with others. For example, we can claim that flipper length corresponds to higher body masses for Gentoo penguins since we observe a strong positive correlation (0.72). For this reason, heat maps are a useful first visualization for large data sets to spot patterns that can be further explored."
  },
  {
    "objectID": "posts/HW6/index.html#introduction",
    "href": "posts/HW6/index.html#introduction",
    "title": "Detecting Fake News with Deep Learning: A Keras-Based Text Classification Approach",
    "section": "Introduction",
    "text": "Introduction\nIn this project, we’ll attempt to classify news articles as legitimate or fake news. This project uses deep learning techniques in Keras to classify news articles as real or fake based on their titles and content. The model pipeline includes text preprocessing with NLTK, text vectorization, and word embeddings to extract meaningful linguistic patterns. We’ll build and train three neural network architectures: one using only article titles, another using only article text, and a third combining both for improved accuracy."
  },
  {
    "objectID": "posts/HW6/index.html#preprocessing",
    "href": "posts/HW6/index.html#preprocessing",
    "title": "Detecting Fake News with Deep Learning: A Keras-Based Text Classification Approach",
    "section": "Preprocessing",
    "text": "Preprocessing\nIn this section, we’ll import and process the text to prepare the data for our models.\n\nSetup & Data Acquisition\nFirst, we’ll need to import the necessary libraries and load our dataset:\n\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nimport string\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import TextVectorization\nfrom nltk.corpus import stopwords\nimport nltk\n\n# Download NLTK stopwords\nnltk.download('stopwords', quiet = True)\n\n# Define our training data URL\ntrain_url = \"https://raw.githubusercontent.com/pic16b-ucla/25W/refs/heads/main/datasets/fake_news_train.csv\"\n\n# Read the data\ntrain = pd.read_csv(train_url)\n\n# Display the first few rows to understand the data\ntrain.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\ntitle\ntext\nfake\n\n\n\n\n0\n17366\nMerkel: Strong result for Austria's FPO 'big c...\nGerman Chancellor Angela Merkel said on Monday...\n0\n\n\n1\n5634\nTrump says Pence will lead voter fraud panel\nWEST PALM BEACH, Fla.President Donald Trump sa...\n0\n\n\n2\n17487\nJUST IN: SUSPECTED LEAKER and “Close Confidant...\nOn December 5, 2017, Circa s Sara Carter warne...\n1\n\n\n3\n12217\nThyssenkrupp has offered help to Argentina ove...\nGermany s Thyssenkrupp, has offered assistance...\n0\n\n\n4\n5535\nTrump say appeals court decision on travel ban...\nPresident Donald Trump on Thursday called the ...\n0\n\n\n\n\n\n\n\nOur dataset contains article titles, the full text content, and a binary label indicating whether each article is fake news (1) or real news (0).\n\n\nData Processing\nNext, we’ll create a function, make_dataset(), to process our data by removing stopwords and creating a TensorFlow dataset. The function will do three things:\n\nChange the text to lowercase: .lower().\nRemove stopwords (i.e. “the,” “and,” or “but”) from text and title using NLTK\nConstruct and return a tf.data.Dataset with two inputs (title, text) and one output (fake). The dataset is also batched with size 100.\n\n\ndef make_dataset(df):\n    # Convert to lowercase\n    df['title'] = df['title'].str.lower()\n    df['text'] = df['text'].str.lower()\n    \n    # Get stopwords from NLTK\n    stop_words = set(stopwords.words('english'))\n    \n    # Function to remove stopwords\n    def remove_stopwords(text):\n        if isinstance(text, str):\n            words = text.split()\n            filtered_words = [word for word in words if word not in stop_words]\n            return ' '.join(filtered_words)\n        return \"\"\n    \n    # Apply stopword removal\n    df['title'] = df['title'].apply(remove_stopwords)\n    df['text'] = df['text'].apply(remove_stopwords)\n    \n    # Create dataset with multiple inputs\n    titles = df['title'].values\n    texts = df['text'].values\n    labels = df['fake'].values\n    \n    # Create dictionary datasets for multiple inputs\n    dataset = tf.data.Dataset.from_tensor_slices(\n        ({\"title\": titles, \"text\": texts}, labels)\n    )\n    \n    return dataset.batch(100)  # Batch size of 100\n\n# Process the training data\ndataset = make_dataset(train)\n\n\n\nSplit Training and Validation Data\nNext, we’ll split our data into training and validation sets using a 80/20 split:\n\n# Calculate the size of the training set (80% of the data)\ntrain_size = int(0.8 * len(train))\nval_size = len(train) - train_size\n\n# Split the dataset\ntrain_dataset = dataset.take(train_size//100)  # Divide by batch size\nval_dataset = dataset.skip(train_size//100)\n\n# Calculate base rate\nbase_rate = train['fake'].mean() * 100\nprint(f\"Base rate of fake news: {base_rate:.2f}%\")\n\nBase rate of fake news: 52.30%\n\n\nThe base rate tells us what percentage of our dataset consists of fake news. This gives us a baseline accuracy to compare our models against.\n\n\nText Vectorization Layer\nNow we’ll create a text vectorization layer that will convert our text data into numerical vectors:\n\n# Preparing text vectorization layer for tf model\nsize_vocabulary = 2000\n\ndef standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase, \n                                 '[%s]' % re.escape(string.punctuation), '')\n    return no_punctuation \n\n# Create a shared vectorization layer for both title and text\nvectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary,  # only consider this many words (2000)\n    output_mode='int',\n    output_sequence_length=500) \n\n# Create text-only dataset for adaptation\ntext_dataset = train_dataset.unbatch().map(lambda x, y: x[\"text\"])\n# Create title-only dataset for adaptation\ntitle_dataset = train_dataset.unbatch().map(lambda x, y: x[\"title\"])\n\n# Combine title and text datasets for adaptation\ncombined_texts = text_dataset.concatenate(title_dataset)\nvectorize_layer.adapt(combined_texts)\n\nThe text vectorization layer transforms our text data into numerical representations that our neural network can process."
  },
  {
    "objectID": "posts/HW6/index.html#creating-models",
    "href": "posts/HW6/index.html#creating-models",
    "title": "Detecting Fake News with Deep Learning: A Keras-Based Text Classification Approach",
    "section": "Creating Models",
    "text": "Creating Models\nWe’ll create three different models (title only, text only, both) to compare their performance:\n\nTitle-Only Model\nWe’ll start with the title-only model, which classifies articles as Real or Fake based on the words in headlines:\n\n# Title-only model\ndef create_title_model():\n    # Input layer for title\n    title_input = keras.Input(shape=(1,), dtype=tf.string, name=\"title\")\n    \n    # Vectorize the title\n    title_vectorized = vectorize_layer(title_input)\n    \n    # Embedding layer\n    embedding = layers.Embedding(size_vocabulary, 10, name=\"embedding\")(title_vectorized)\n    \n    # Add dropout for regularization\n    x = layers.Dropout(0.2)(embedding)\n    \n    # Global pooling layer\n    x = layers.GlobalAveragePooling1D()(x)\n    \n    # Add another dropout layer\n    x = layers.Dropout(0.2)(x)\n    \n    # Output layer\n    output = layers.Dense(1, activation=\"sigmoid\")(x)\n    \n    # Create the model\n    model = keras.Model(inputs=title_input, outputs=output, name=\"title_model\")\n    \n    # Compile the model\n    model.compile(\n        loss=\"binary_crossentropy\",\n        optimizer=\"adam\",\n        metrics=[\"accuracy\"]\n    )\n    \n    return model\n\nLet’s create the model and summary:\n\n# Creatd title model\ntitle_model = create_title_model()\n\ntitle_model.summary()\n\nModel: \"title_model\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ title (InputLayer)              │ (None, 1)              │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ text_vectorization              │ (None, 500)            │             0 │\n│ (TextVectorization)             │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ embedding (Embedding)           │ (None, 500, 10)        │        20,000 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (Dropout)               │ (None, 500, 10)        │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling1d        │ (None, 10)             │             0 │\n│ (GlobalAveragePooling1D)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (Dropout)             │ (None, 10)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (Dense)                   │ (None, 1)              │            11 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 20,011 (78.17 KB)\n\n\n\n Trainable params: 20,011 (78.17 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nNow, we’re ready to train the title-only model:\n\n# Train title model\ntitle_history = title_model.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=10,\n    verbose=1\n)\n\n\nEpoch 1/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.7679 - loss: 0.5436 - val_accuracy: 0.7857 - val_loss: 0.5237\n\nEpoch 2/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.7705 - loss: 0.5262 - val_accuracy: 0.7791 - val_loss: 0.5069\n\nEpoch 3/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.7736 - loss: 0.5103 - val_accuracy: 0.7887 - val_loss: 0.4933\n\nEpoch 4/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.7833 - loss: 0.4962 - val_accuracy: 0.7991 - val_loss: 0.4821\n\nEpoch 5/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.7843 - loss: 0.4834 - val_accuracy: 0.7927 - val_loss: 0.4692\n\nEpoch 6/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.7882 - loss: 0.4755 - val_accuracy: 0.7918 - val_loss: 0.4593\n\nEpoch 7/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.7915 - loss: 0.4645 - val_accuracy: 0.8022 - val_loss: 0.4503\n\nEpoch 8/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.7971 - loss: 0.4549 - val_accuracy: 0.8033 - val_loss: 0.4417\n\nEpoch 9/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.8015 - loss: 0.4461 - val_accuracy: 0.8079 - val_loss: 0.4338\n\nEpoch 10/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.8031 - loss: 0.4395 - val_accuracy: 0.8074 - val_loss: 0.4267\n\n\n\n\n\n# Visualize the model\nfrom tensorflow.keras.utils import plot_model\nutils.plot_model(title_model, \"title_model.png\", \n                 show_shapes=True,\n                 show_layer_names=True)\n\n\n\n\n\n\n\n\n\n\nText-Only Model\nNext, we’ll build and run the text-only model:\n\n# Text-only model\ndef create_text_model():\n    # Input layer for text\n    text_input = keras.Input(shape=(1,), dtype=tf.string, name=\"text\")\n    \n    # Vectorize the text\n    text_vectorized = vectorize_layer(text_input)\n    \n    # Embedding layer\n    embedding = layers.Embedding(size_vocabulary, 10, name=\"embedding\")(text_vectorized)\n    \n    # Add dropout for regularization\n    x = layers.Dropout(0.2)(embedding)\n    \n    # Global pooling layer\n    x = layers.GlobalAveragePooling1D()(x)\n    \n    # Add another dropout layer\n    x = layers.Dropout(0.2)(x)\n    \n    # Output layer\n    output = layers.Dense(1, activation=\"sigmoid\")(x)\n    \n    # Create the model\n    model = keras.Model(inputs=text_input, outputs=output, name=\"text_model\")\n    \n    # Compile the model\n    model.compile(\n        loss=\"binary_crossentropy\",\n        optimizer=\"adam\",\n        metrics=[\"accuracy\"]\n    )\n    \n    return model\n\nLet’s create the model and summary:\n\n# Create text model\ntext_model = create_text_model()\n\ntext_model.summary()\n\nModel: \"text_model\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ text (InputLayer)               │ (None, 1)              │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ text_vectorization_8            │ (None, 500)            │             0 │\n│ (TextVectorization)             │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ embedding (Embedding)           │ (None, 500, 10)        │        20,000 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (Dropout)             │ (None, 500, 10)        │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling1d_1      │ (None, 10)             │             0 │\n│ (GlobalAveragePooling1D)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (Dropout)             │ (None, 10)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ (None, 1)              │            11 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 20,011 (78.17 KB)\n\n\n\n Trainable params: 20,011 (78.17 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nNow we’re ready to train the model:\n\n# Train text model\ntext_history = text_model.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=10,\n    verbose=1\n)\n\n\nEpoch 1/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.5584 - loss: 0.6785 - val_accuracy: 0.6753 - val_loss: 0.6213\n\nEpoch 2/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8073 - loss: 0.5964 - val_accuracy: 0.8617 - val_loss: 0.5128\n\nEpoch 3/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8938 - loss: 0.4895 - val_accuracy: 0.9235 - val_loss: 0.4160\n\nEpoch 4/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.9166 - loss: 0.4013 - val_accuracy: 0.9453 - val_loss: 0.3466\n\nEpoch 5/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.9281 - loss: 0.3396 - val_accuracy: 0.9486 - val_loss: 0.2986\n\nEpoch 6/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.9356 - loss: 0.2969 - val_accuracy: 0.9523 - val_loss: 0.2640\n\nEpoch 7/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.9412 - loss: 0.2661 - val_accuracy: 0.9536 - val_loss: 0.2383\n\nEpoch 8/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.9448 - loss: 0.2430 - val_accuracy: 0.9587 - val_loss: 0.2182\n\nEpoch 9/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.9478 - loss: 0.2254 - val_accuracy: 0.9628 - val_loss: 0.2024\n\nEpoch 10/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.9510 - loss: 0.2091 - val_accuracy: 0.9644 - val_loss: 0.1893\n\n\n\n\n\n# Visualize the model\nutils.plot_model(text_model, \"text_model.png\", \n                 show_shapes=True,\n                 show_layer_names=True)\n\n\n\n\n\n\n\n\n\n\nCombined Model (Title and Text)\nFinally, we’ll build and run the combined model:\n\n# Combined model (title and text)\ndef create_combined_model():\n    # Input layers\n    title_input = keras.Input(shape=(1,), dtype=tf.string, name=\"title\")\n    text_input = keras.Input(shape=(1,), dtype=tf.string, name=\"text\")\n    \n    # Vectorize inputs using the same vectorization layer\n    title_vectorized = vectorize_layer(title_input)\n    text_vectorized = vectorize_layer(text_input)\n    \n    # Shared embedding layer\n    embedding_layer = layers.Embedding(size_vocabulary, 10, name=\"embedding\")\n    \n    # Apply embedding to both inputs\n    title_embedding = embedding_layer(title_vectorized)\n    text_embedding = embedding_layer(text_vectorized)\n    \n    # Process title branch\n    title_x = layers.Dropout(0.2)(title_embedding)\n    title_x = layers.GlobalAveragePooling1D()(title_x)\n    title_x = layers.Dropout(0.2)(title_x)\n    \n    # Process text branch\n    text_x = layers.Dropout(0.2)(text_embedding)\n    text_x = layers.GlobalAveragePooling1D()(text_x)\n    text_x = layers.Dropout(0.2)(text_x)\n    \n    # Concatenate the two branches\n    concatenated = layers.concatenate([title_x, text_x], name=\"concatenate\")\n    \n    # Add a dense layer\n    x = layers.Dense(32, activation=\"relu\")(concatenated)\n    x = layers.Dropout(0.2)(x)\n    \n    # Output layer\n    output = layers.Dense(1, activation=\"sigmoid\")(x)\n    \n    # Create the model\n    model = keras.Model(inputs=[title_input, text_input], outputs=output, name=\"combined_model\")\n    \n    # Compile the model\n    model.compile(\n        loss=\"binary_crossentropy\",\n        optimizer=\"adam\",\n        metrics=[\"accuracy\"]\n    )\n    \n    return model\n\nLet’s create the model and summary:\n\n# Create combined model\ncombined_model = create_combined_model()\n\ncombined_model.summary()\n\nModel: \"combined_model\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ title (InputLayer)  │ (None, 1)         │          0 │ -                 │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ text (InputLayer)   │ (None, 1)         │          0 │ -                 │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ text_vectorization… │ (None, 500)       │          0 │ title[0][0],      │\n│ (TextVectorization) │                   │            │ text[0][0]        │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding           │ (None, 500, 10)   │     20,000 │ text_vectorizati… │\n│ (Embedding)         │                   │            │ text_vectorizati… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_4 (Dropout) │ (None, 500, 10)   │          0 │ embedding[0][0]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_6 (Dropout) │ (None, 500, 10)   │          0 │ embedding[1][0]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ global_average_poo… │ (None, 10)        │          0 │ dropout_4[0][0]   │\n│ (GlobalAveragePool… │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ global_average_poo… │ (None, 10)        │          0 │ dropout_6[0][0]   │\n│ (GlobalAveragePool… │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_5 (Dropout) │ (None, 10)        │          0 │ global_average_p… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_7 (Dropout) │ (None, 10)        │          0 │ global_average_p… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate         │ (None, 20)        │          0 │ dropout_5[0][0],  │\n│ (Concatenate)       │                   │            │ dropout_7[0][0]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_2 (Dense)     │ (None, 32)        │        672 │ concatenate[0][0] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_8 (Dropout) │ (None, 32)        │          0 │ dense_2[0][0]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_3 (Dense)     │ (None, 1)         │         33 │ dropout_8[0][0]   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n\n\n\n Total params: 20,705 (80.88 KB)\n\n\n\n Trainable params: 20,705 (80.88 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nNow, we’re ready to train the model:\n\n# Train combined model\ncombined_history = combined_model.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=10,\n    verbose=1\n)\n\n\nEpoch 1/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - accuracy: 0.5307 - loss: 0.6842 - val_accuracy: 0.9292 - val_loss: 0.5420\n\nEpoch 2/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.8621 - loss: 0.4532 - val_accuracy: 0.9222 - val_loss: 0.2334\n\nEpoch 3/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.9280 - loss: 0.2307 - val_accuracy: 0.9132 - val_loss: 0.1904\n\nEpoch 4/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.9359 - loss: 0.1826 - val_accuracy: 0.9668 - val_loss: 0.1332\n\nEpoch 5/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.9479 - loss: 0.1565 - val_accuracy: 0.9692 - val_loss: 0.1210\n\nEpoch 6/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.9591 - loss: 0.1327 - val_accuracy: 0.9716 - val_loss: 0.1081\n\nEpoch 7/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.9612 - loss: 0.1217 - val_accuracy: 0.9725 - val_loss: 0.1061\n\nEpoch 8/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.9635 - loss: 0.1136 - val_accuracy: 0.9725 - val_loss: 0.0965\n\nEpoch 9/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.9666 - loss: 0.1065 - val_accuracy: 0.9736 - val_loss: 0.0920\n\nEpoch 10/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.9703 - loss: 0.0974 - val_accuracy: 0.9734 - val_loss: 0.0917\n\n\n\n\n\n# Visualize the model\nutils.plot_model(combined_model, \"combined_model.png\", \n                 show_shapes=True,\n                 show_layer_names=True)"
  },
  {
    "objectID": "posts/HW6/index.html#training-history-visualization",
    "href": "posts/HW6/index.html#training-history-visualization",
    "title": "Detecting Fake News with Deep Learning: A Keras-Based Text Classification Approach",
    "section": "Training History Visualization",
    "text": "Training History Visualization\nLet’s visualize the training history to compare the performance of our three models:\n\ndef plot_history(title_history, text_history, combined_history):\n    plt.figure(figsize=(12, 5))\n    \n    # Accuracy plot\n    plt.subplot(1, 2, 1)\n    plt.plot(title_history.history['accuracy'], label='Title Model (Train)')\n    plt.plot(title_history.history['val_accuracy'], label='Title Model (Val)')\n    plt.plot(text_history.history['accuracy'], label='Text Model (Train)')\n    plt.plot(text_history.history['val_accuracy'], label='Text Model (Val)')\n    plt.plot(combined_history.history['accuracy'], label='Combined Model (Train)')\n    plt.plot(combined_history.history['val_accuracy'], label='Combined Model (Val)')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Model Accuracy')\n    plt.legend()\n    \n    # Loss plot\n    plt.subplot(1, 2, 2)\n    plt.plot(title_history.history['loss'], label='Title Model (Train)')\n    plt.plot(title_history.history['val_loss'], label='Title Model (Val)')\n    plt.plot(text_history.history['loss'], label='Text Model (Train)')\n    plt.plot(text_history.history['val_loss'], label='Text Model (Val)')\n    plt.plot(combined_history.history['loss'], label='Combined Model (Train)')\n    plt.plot(combined_history.history['val_loss'], label='Combined Model (Val)')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Model Loss')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.savefig('training_history.png')\n    plt.show()\n\n# Plot the training history\nplot_history(title_history, text_history, combined_history)\n\n\n\n\n\n\n\n\nBased on the training history, we can observe:\n\nThe Combined Model achieves the highest validation accuracy at approximately 97%, exceeding our target accuracy.\nThe Text Model performs quite well too, reaching around 96% validation accuracy.\nThe Title Model performs significantly worse, only reaching about 80% accuracy.\n\nThis indicates that while article titles provide some signal for fake news detection, the full text contains much more useful information, and using both together delivers the best performance."
  },
  {
    "objectID": "posts/HW6/index.html#evaluation-on-test-data",
    "href": "posts/HW6/index.html#evaluation-on-test-data",
    "title": "Detecting Fake News with Deep Learning: A Keras-Based Text Classification Approach",
    "section": "Evaluation on Test Data",
    "text": "Evaluation on Test Data\nLet’s evaluate our most performant model on the test dataset:\n\n# Load test data\ntest_url = \"https://raw.githubusercontent.com/pic16b-ucla/25W/refs/heads/main/datasets/fake_news_test.csv\"\ntest = pd.read_csv(test_url)\n\n# Create test dataset\ntest_dataset = make_dataset(test)\n\n# Evaluate model on the test dataset\ncombined_result = combined_model.evaluate(test_dataset)\n\n# Print the results\nprint(\"Combined Model Test Accuracy: {:.2f}%\".format(combined_result[1] * 100))\n\n\n225/225 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.9695 - loss: 0.0996\n\nCombined Model Test Accuracy: 96.96%"
  },
  {
    "objectID": "posts/HW6/index.html#embedding-visualization",
    "href": "posts/HW6/index.html#embedding-visualization",
    "title": "Detecting Fake News with Deep Learning: A Keras-Based Text Classification Approach",
    "section": "Embedding Visualization",
    "text": "Embedding Visualization\nNow let’s visualize the word embeddings learned by our best model:\n\nimport plotly.graph_objects as go\nfrom sklearn.decomposition import PCA\n\n# Get the embedding layer weights\nembedding_layer = combined_model.get_layer(\"embedding\")\nweights = embedding_layer.get_weights()[0]\n\n# Get the vocabulary from the vectorization layer\nvocab = vectorize_layer.get_vocabulary()\n\n# Use PCA to reduce dimensions for visualization (3D)\npca = PCA(n_components=3)\nembeddings_3d = pca.fit_transform(weights)\n\n# Create a DataFrame for plotting\ndf_embeddings = pd.DataFrame({\n    'word': vocab,\n    'x': embeddings_3d[:, 0],\n    'y': embeddings_3d[:, 1],\n    'z': embeddings_3d[:, 2]\n})\n\n# Filter out padding and out-of-vocabulary terms\ndf_plot = df_embeddings[~df_embeddings['word'].isin(['', '[UNK]'])].copy()\n\n# Compute distance from origin for highlighting interesting words\ndf_plot['distance'] = np.sqrt(df_plot['x']**2 + df_plot['y']**2 + df_plot['z']**2)\n\n# Selected words to highlight, including political, news-related and sentiment words\nhighlight_words = [\n    'trump', 'clinton', 'hillary', 'obama', 'democrats', 'republicans',\n    'fake', 'real', 'true', 'false', 'hoax', 'conspiracy', 'news', 'media',\n    'war', 'military', 'government'\n]\n\n# Create a new column to determine text visibility and point size\ndf_plot['show_label'] = df_plot['word'].isin(highlight_words)\ndf_plot['point_size'] = 3  # Default size\ndf_plot.loc[df_plot['show_label'], 'point_size'] = 6  # Larger for highlighted words\n\n# Create text colors - visible only for highlighted words\ntext_colors = ['rgba(0,0,0,1)' if show else 'rgba(0,0,0,0)' for show in df_plot['show_label']]\n\n# Calculate the range of each axis for better scaling\nx_range = [df_plot['x'].min(), df_plot['x'].max()]\ny_range = [df_plot['y'].min(), df_plot['y'].max()]\nz_range = [df_plot['z'].min(), df_plot['z'].max()]\n\n# Calculate axis scaling factors to make the plot more compact\n# Specifically reduce the y-axis (component 2) range\nscale_factor_y = 0.5  # Reduce y-axis scale by 50%\n\n# Create the 3D scatter plot\nfig = go.Figure(data=[\n    go.Scatter3d(\n        x=df_plot['x'],\n        y=df_plot['y'] * scale_factor_y,  # Scale down component 2\n        z=df_plot['z'],\n        mode='markers+text',\n        text=df_plot['word'],\n        textposition=\"top center\",\n        textfont=dict(\n            size=10,\n            color=text_colors\n        ),\n        marker=dict(\n            size=df_plot['point_size'],\n            color=df_plot['distance'],\n            colorscale='Viridis',\n            opacity=0.8,\n        ),\n        hoverinfo='text',\n        hovertext=df_plot['word']\n    )\n])\n\n# Update layout with tighter ranges\nfig.update_layout(\n    title=\"3D Word Embedding Visualization for Fake News Classification\",\n    width=900,\n    height=700,\n    scene=dict(\n        xaxis=dict(\n            title='PCA Component 1',\n            range=[x_range[0] * 1.1, x_range[1] * 1.1]  # Add small margin\n        ),\n        yaxis=dict(\n            title='PCA Component 2',\n            range=[y_range[0] * scale_factor_y * 1.1, y_range[1] * scale_factor_y * 1.1]\n        ),\n        zaxis=dict(\n            title='PCA Component 3',\n            range=[z_range[0] * 1.1, z_range[1] * 1.1]\n        ),\n        aspectmode='manual',  # Manual aspect ratio\n        aspectratio=dict(x=1, y=0.5, z=1)  # Reduce y-axis aspect ratio\n    )\n)\n\n# Show the plot\nfig.show()\n\n\n\n\n\nAnalysis of Word Embeddings\nScrolling over the word embeddings we can see some trends. Here are some examples:\n\nPolitical Polarization: In the embedding space, we can see that “trump” and “clinton” appear in distinct regions while “hilary” and “obama” remain close together. This might suggest that the model has learned different contextual patterns for how these names appear in news articles. This separation likely reflects the polarized nature of political reporting, where certain linguistic patterns are associated with coverage of different political figures.\nWar: “war” has words surrounding it that naturally seem to be brought up in the same discussions and context such as “targeted,” “campaign,” “missile,” etc.\nMedia References: The word “media” appears in a different region from specific news outlet names, suggesting that generic references to “the media” might be used differently in fake news (often as a collective entity to criticize) compared to specific outlet citations which might be more common in legitimate reporting.\nConspiracy-Related Terms: Words like “hoax”, “conspiracy” and related terms form their own cluster, indicating the model has learned to associate these concepts with questionable content. This semantic grouping helps the model identify articles that use language patterns typical of conspiracy theories.\nPolitical parties: words like “democrats” and “republicans” are positioned close together as they are likely often discussed together along with words such as “washington” and “administration.”\n\nThese embedding patterns help explain why our combined model performs better than either title-only or text-only models. By learning the complex relationships between words in both titles and full text, the model captures rich semantic information about how language is used differently in fake versus real news articles."
  },
  {
    "objectID": "posts/HW3/index.html#introduction",
    "href": "posts/HW3/index.html#introduction",
    "title": "Building a Visually Appealing Message Bank Web App with Dash & SQLite",
    "section": "Introduction",
    "text": "Introduction\nDash is a Python framework for building web applications using Flask, Plotly, and React. It enables developers to create interactive web apps with minimal front-end knowledge, making it a great choice for data-driven applications. We will work through the process of building a simple message bank web app using Dash and SQLite. This app will allow users to submit and view messages dynamically through an interactive interface."
  },
  {
    "objectID": "posts/HW3/index.html#getting-started",
    "href": "posts/HW3/index.html#getting-started",
    "title": "Building a Visually Appealing Message Bank Web App with Dash & SQLite",
    "section": "Getting Started…",
    "text": "Getting Started…\nBefore creating our web app, we must import the necessary packages and initialize the app.\n\nImporting Necessary Packages\nWe’ll import the following packages for our web app:\n\nimport sqlite3\nfrom dash import Dash, html, dcc, Input, Output, State\nimport dash_bootstrap_components as dbc\n\nHow the packages will be used:\n\nsqlite3: Handles database operations\nDash: Creates the web application\nhtml, dcc: Build UI components\nInput, Output, State: Used for Dash callbacks\ndbc (Dash Bootstrap Components): Provides pre-styled UI components\n\n\n\nSet Global Styling (Dark Mode)\nWe’ll build a web app with a dark theme, so we need to set our custom global CSS styling. We can do so by running:\n\n# Custom CSS for additional styling\ncustom_css = {\n    'dark-card': {\n        'backgroundColor': '#2a2a2a', # Hex Color code\n        'border': '1px solid #404040',\n        'borderRadius': '15px',\n        'boxShadow': '0 4px 8px 0 rgba(0,0,0,0.2)'\n    },\n    'input-style': {\n        'backgroundColor': '#333333',\n        'color': 'white',\n        'border': '1px solid #404040'\n    },\n    'message-card': {\n        'backgroundColor': '#1e1e1e',\n        'border': '1px solid #404040',\n        'borderRadius': '10px'\n    }\n}\n\n\n\nInitializing Our Web App\nWe’ll initialize the Dash app by running:\n\n# Initialize the app\napp = Dash(__name__, external_stylesheets=[dbc.themes.DARKLY])\napp.title = \"Message Bank\"\n\nRunning the code above does the following:\n\nInitializes the Dash application\nLoads Bootstrap styling (DARKLY for dark theme)\nSets the page title\n\n\n\nDefining the App Layout\nWe’ll begin defining the app layout by running the following:\n\n# App layout with dark theme\napp.layout = dbc.Container(fluid=True, style={'padding': '20px'}, children=[\n    html.Div([\n        html.H1(\"Message Bank\", # Create app title block\n                className=\"text-center mt-4 mb-4\", \n                style={'color': '#6f42c1', 'fontWeight': 'bold'}),\n    ], style={'backgroundColor': 'transparent', 'color': '#ffffff', \n              'fontFamily': 'Roboto, sans-serif'}), # Change color (hex code) and font\n\nThe code includes:\n\ndbc.Container(fluid=True) creates a full responsive layout.\nchildren will include all the features we add.\nhtml.H1() displays the title “Message Bank.”\nstyle contains the color information for the title.\n\nWe’ll add more to this code block as we add the layout for the submission and viewing features."
  },
  {
    "objectID": "posts/HW3/index.html#setting-up-the-submission-system",
    "href": "posts/HW3/index.html#setting-up-the-submission-system",
    "title": "Building a Visually Appealing Message Bank Web App with Dash & SQLite",
    "section": "Setting Up the Submission System…",
    "text": "Setting Up the Submission System…\nIn this section we will set up the submission system so that users will be able to to enter a name or user and a message and submit. By hitting submit, the name and message will be added to the message_db database so that later they can be retrieved in the view section. All of this functionality will be wrapped in a user-friendly UI.\n\nCreating the UI for Submitting Messages\nBefore handling the database and necessary functions, we’ll first create the input form where users will submit their messages.\nWe’ll add the code for creating the submission UI into app.layout as another child:\n\n# App layout with dark theme\napp.layout = dbc.Container(fluid=True, style={'padding': '20px'}, children=[\n    html.Div([\n        html.H1(\"Message Bank\", # Create app title block\n                className=\"text-center mt-4 mb-4\", \n                style={'color': '#6f42c1', 'fontWeight': 'bold'}),\n    ], style={'backgroundColor': 'transparent', 'color': '#ffffff', \n              'fontFamily': 'Roboto, sans-serif'}), # Change color (hex code) and font\n\n# New Code: ------------------------------------------------------------------\n    \n    # Submission Section\n    dbc.Card(className=\"dark-card\", children=[ # Submission has its own children\n        dbc.CardBody(className=\"mb-4\", children=[  \n            html.H2(\"Submit a Message\", # Title for Submission UI\n                    className=\"card-title mb-3\", \n                    style={'color': '#6f42c1'}), # Styling\n            dcc.Input( # Creates input box for user-name\n                id='handle-input',\n                type='text',\n                placeholder='Your name',\n                className='mb-3 form-control',\n                style={**custom_css['input-style'], 'height': '45px'} # Global CSS\n            ),\n            dcc.Textarea( # Creates message box\n                id='message-input',\n                placeholder='Your message...',\n                className='mb-3 form-control',\n                style={**custom_css['input-style'], 'height': '150px'}\n            ),\n            dbc.Button( # Creates submit button\n                \"Submit\",\n                id='submit-button',\n                color=\"primary\",\n                className='mb-3 w-100',\n                style={'backgroundColor': '#6f42c1', 'border': 'none'}\n            ),\n            html.Div(id='submit-confirmation') # Creates submission confirmation\n        ])\n    ]),\n\nWhat dbc.Card code does:\n\nCreates a name input field dcc.Input()\nCreates a message input field dcc.Textarea()\nAdds a Submit button dbc.Button()\nAdds a confirmation message area html.Div()\n\nAt this point, the UI is set up but does not store messages yet.\n\n\nCreating the Database to Store Messages\nSince we need to store user messages, we’ll use SQLite.\nWe can write a function def get_message_db() to initialize and return the SQLite database connection:\n\n# Database setup\nmessage_db = None\n\ndef get_message_db():\n    # Retrieve or create the message database.\n    global message_db\n    if message_db:\n        return message_db\n    else:\n        message_db = sqlite3.connect(\"messages_db.sqlite\", check_same_thread=False)\n        cmd = \"\"\"\n            CREATE TABLE IF NOT EXISTS messages \n            (handle TEXT, message TEXT)\n        \"\"\"\n        # Creates table if not existing\n        # Queries for user handle & message\n        cursor = message_db.cursor()\n        cursor.execute(cmd)\n        message_db.commit()\n        return message_db\n\nWhat the function does:\n\nConnects to messages_db.sqlite\nCreates a messages table (if it doesn’t exist) with: 1. handle: user’s name 2. message: user’s message\n\n\n\nWriting a Function to Insert Messages\nNow that we have a database, we need to store user messages.\nWe can write a function def insert_message() to insert a new message into the database:\n\ndef insert_message(handle, message):\n    db = get_message_db()\n    cursor = db.cursor()\n    # Don't use f-string.\n    # Use parameterized queries (?, ?) to prevent SQL injection & handle special characters\n    cursor.execute(\"INSERT INTO messages (handle, message) VALUES (?, ?)\", (handle, message))\n    db.commit()\n    db.close()\n    global message_db\n    message_db = None\n\nWhat the function does:\n\nSaves the user’s name: handle and message: message into the database.\nUses parameterized queries ? to prevent SQL injection.\n\n\n\nConnecting the UI to the Database (Submit Callback)\nNow, we need to connect the Submit button to the insert function so that user input is stored when they click the button.\nWe’ll write a callback @app.callback and function def submit() to handle this:\n\n@app.callback(\n    Output('submit-confirmation', 'children'), # Updates the confirmation message div\n    Input('submit-button', 'n_clicks'), # Triggered when the submit button is clicked\n    State('handle-input', 'value'), # Value from the handle input field\n    State('message-input', 'value'), # Value from the message input field\n    prevent_initial_call=True # Prevents the callback from running on app startup\n)\ndef submit(n_clicks, handle, message):\n    # Ensures both handle and message are provided and not empty\n    if not handle or not message or handle.strip() == '' or message.strip() == '':\n        return dbc.Alert(\"Please enter both a name and a message.\", color=\"danger\")\n    \n    # Insert the message into the database\n    insert_message(handle.strip(), message.strip())\n    \n    # Return a success message and thanks the user\n    return dbc.Alert(\"Message submitted successfully! Thank you!\", color=\"success\")\n\nWhat the callback and function does:\n\nRetrieves user input (handle, message).\nChecks if input is empty.\nCalls insert_message() to store data in the database.\nDisplays a success message in submit-confirmation."
  },
  {
    "objectID": "posts/HW3/index.html#viewing-user-submissions",
    "href": "posts/HW3/index.html#viewing-user-submissions",
    "title": "Building a Visually Appealing Message Bank Web App with Dash & SQLite",
    "section": "Viewing User Submissions…",
    "text": "Viewing User Submissions…\nIn this section, we’ll set up the view system so that users can click “update” to view up to five previously submitted messages randomly selected. By clicking the button, the database will be queried to retrieve handles and messages and display them neatly. Like the submission section, we’ll start by establishing the UI, defining the functions, and connecting them.\n\nCreating the UI to Display Messages\nNow, we’ll create a user-interface for users to view messages. We’ll add another child to our existing app.layout:\n\n# App layout with dark theme\napp.layout = dbc.Container(fluid=True, style={'padding': '20px'}, children=[\n    html.Div([\n        html.H1(\"Message Bank\", # Create app title block\n                className=\"text-center mt-4 mb-4\", \n                style={'color': '#6f42c1', 'fontWeight': 'bold'}),\n    ], style={'backgroundColor': 'transparent', 'color': '#ffffff', \n              'fontFamily': 'Roboto, sans-serif'}), # Change color (hex code) and font\n    \n    # Submission Section\n    dbc.Card(className=\"dark-card\", children=[ # Submission has its own children\n        dbc.CardBody(className=\"mb-4\", children=[  \n            html.H2(\"Submit a Message\", # Title for Submission UI\n                    className=\"card-title mb-3\", \n                    style={'color': '#6f42c1'}), # Styling\n            dcc.Input( # Creates input box for user-name\n                id='handle-input',\n                type='text',\n                placeholder='Your name',\n                className='mb-3 form-control',\n                style={**custom_css['input-style'], 'height': '45px'} # Global CSS\n            ),\n            dcc.Textarea( # Creates message box\n                id='message-input',\n                placeholder='Your message...',\n                className='mb-3 form-control',\n                style={**custom_css['input-style'], 'height': '150px'}\n            ),\n            dbc.Button( # Creates submit button\n                \"Submit\",\n                id='submit-button',\n                color=\"primary\",\n                className='mb-3 w-100',\n                style={'backgroundColor': '#6f42c1', 'border': 'none'}\n            ),\n            html.Div(id='submit-confirmation') # Creates submission confirmation\n        ])\n    ]),\n    \n# New Code: ------------------------------------------------------------------\n    \n        # View Messages Section\n        dbc.Card(className=\"dark-card\", children=[\n            dbc.CardBody([\n                html.H2(\"View Random Messages\", # Title of Viewing UI\n                        className=\"card-title mb-3\", \n                        style={'color': '#ffffff', 'fontFamily': 'Roboto, sans-serif'}),\n                dbc.Button( # Button to trigger random messages\n                    \"Update\", \n                    id='view-button',\n                    color=\"info\",\n                    className='mb-3 w-100',\n                    style={'backgroundColor': '#20c997', 'border': 'none'}\n                ),\n                html.Div(id='message-display') # Dynamically updated\n            ])\n        ])\n    ])\n\nWhat dbc.card does:\n\nAdds a button to refresh the displayed messages.\nhtml.Div(id='message-display') will be populated dynamically.\n\n\n\nFetching Messages from the Database\nNow, we’ll need a function to retrieve messages for display.\nWe can write a function def random_messages() to fetch up to n random messages from the database:\n\ndef random_messages(n=5): # Limit number to 5\n    db = get_message_db()\n    cursor = db.cursor()\n    # Query function with f-string to pass n into query\n    cursor.execute(f\"SELECT handle, message FROM messages ORDER BY RANDOM() LIMIT {n}\")\n    messages = cursor.fetchall()\n    db.close()\n    global message_db\n    message_db = None\n    return messages\n\nWhat this function does:\n\nRetrieves up to 5 messages randomly from the database.\nEnsures messages appear in a shuffled order.\n\n\n\nConnecting the UI to the Database (View Callback)\nNow, we’ll connect the View button to the random_messages() function.\nWe’ll write a callback @app.callback and function def view() to handle this:\n\n@app.callback(\n    Output('message-display', 'children'), # Updates the div to display messages\n    Input('view-button', 'n_clicks'), # Triggered when the view button is clicked\n    prevent_initial_call=True # Prevents the callback from running on app startup\n)\ndef view(n_clicks):\n    \"\"\"Displays a random selection of messages.\"\"\"\n    # Fetch up to 5 random messages from the database\n    messages = random_messages(5)\n    \n    # If no messages are found, display a placeholder message\n    if not messages:\n        return html.P(\"No messages to display.\", style={'color': 'white'})\n    \n    # Create a list of styled cards for each message\n    message_cards = []\n    for handle, message in messages:\n        card = dbc.Card(\n            className=\"mb-3\",\n            style=custom_css['message-card'],  # Apply custom styling for the card\n            children=[\n                dbc.CardBody([\n                    html.Blockquote(\n                        className=\"blockquote mb-0\",\n                        children=[\n                            # Display the message\n                            html.P(message, style={'color': '#dee2e6'}),\n                            html.Footer(\n                                html.Small(\n                                    # Display the handle\n                                    html.I(f\"— {handle}\"),\n                                    className=\"text-muted\"\n                                )\n                            )\n                        ]\n                    )\n                ])\n            ]\n        )\n        message_cards.append(card)  # Add the card to the list\n    \n    # Return the list of message cards to be displayed\n    return message_cards\n\nWhat the callback and function does:\n\nCalls random_messages() to fetch stored messages.\nDisplays each message in styled blockquotes."
  },
  {
    "objectID": "posts/HW3/index.html#running-the-web-app",
    "href": "posts/HW3/index.html#running-the-web-app",
    "title": "Building a Visually Appealing Message Bank Web App with Dash & SQLite",
    "section": "Running the Web App",
    "text": "Running the Web App\nTo run the app we need to write one last bit of code to prevent us from accidentally running the code mistakenly as well as defining the port our website will occupy:\n\n# Run the app\nif __name__ == '__main__':\n    app.run_server(debug=False, port=8080) # Feel free to change port\n\nIn the terminal, after changing the directory to our project folder, we can run to launch the web app:\npython &lt;your filename&gt;.py\nWe’ll get a local host link to open in our browser (private browser recommended). We should see something like the initial page shown in the beginning.\nFrom here we can type in a name/message and submit as shown:\n\nClicking on the “Update” button should show previously submitted messages:\n\nAdditionally, when a user attempts to submit with either (or both) a missing handle/message, we should see the error message:"
  },
  {
    "objectID": "posts/HW1/index.html",
    "href": "posts/HW1/index.html",
    "title": "Exploring NOAA Climate Data: SQL Queries, Trend Analysis, and Advanced Visualizations with Plotly",
    "section": "",
    "text": "3D Scatter Plot of Monthly Average Temperature by Latitude for Australia"
  },
  {
    "objectID": "posts/HW1/index.html#introduction",
    "href": "posts/HW1/index.html#introduction",
    "title": "Exploring NOAA Climate Data: SQL Queries, Trend Analysis, and Advanced Visualizations with Plotly",
    "section": "Introduction",
    "text": "Introduction\nThe NOAA Climate Data provides an extensive record of global temperature trends, collected from thousands of weather stations across various geographic regions. This dataset is important for analyzing long-term climate patterns, regional warming trends, and the impacts of climate change. In this blog, we will explore how to construct and query an SQL database to efficiently retrieve climate data, apply filtering techniques, and compute meaningful temperature trends. Additionally, we will leverage Plotly to create complex visualizations, including scatter map boxes, 3D plots, and faceted box plots, to uncover insights into global and regional temperature changes over time."
  },
  {
    "objectID": "posts/HW1/index.html#creating-an-sql-database",
    "href": "posts/HW1/index.html#creating-an-sql-database",
    "title": "Exploring NOAA Climate Data: SQL Queries, Trend Analysis, and Advanced Visualizations with Plotly",
    "section": "Creating an SQL Database",
    "text": "Creating an SQL Database\nOur first objective is to create an SQL database containing three data frames for temperatures, stations, and countries. We will begin with the temperature data as it is the most difficult.\n\nDownloading NOAA Temperature Data\nWe start by creating a folder, titled “datafiles,” in the same location as our ipynb to store our CSV files:\n\nimport os\n# create folder named \"datafiles\" if it does not exist\nif not os.path.exists(\"datafiles\"): \n    os.mkdir(\"datafiles\")\n\nFor our purpose, we will load climate data from 1901-2020. We first need to download the raw data from the GitHub URL containing the NOAA climate data. The dataset is split into decadal CSV files (e.g., 1901-1910.csv, 1911-1920.csv, etc). The script constructs URLs and downloads each file (saving it to datafiles/ folder):\n\nimport urllib.request\n# List comprehension to construct decadal files\nintervals = [f\"{i}-{i+9}\" for i in range(1901, 2020, 10)] # 1901-1910 to 2011-2020\n# Cycles through and downloads each decade associated CSV\nfor interval in intervals:\n    url = f\"https://raw.githubusercontent.com/PIC16B-ucla/24F/main/datasets/noaa-ghcn/decades/{interval}.csv\"\n    urllib.request.urlretrieve(url, f\"datafiles/{interval}.csv\")\n\nOnce all our CSVs are downloaded into the datafiles folder, we are ready to create the database.\n\n\nCreating Our SQL Database\nWe first want to import the necessary packages:\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\n\nNext, we will establish the database titled temps.db:\n\n# Establish a connection or (in this case) to create the SQLite database named \"temps.db\"\nconn = sqlite3.connect(\"temps.db\")\n\nBefore inserting the temperatures df into our database we need to write a function that will properly format the data frame. The raw NOAA dataset is stored in a wide format, where each row contains temperature readings for all 12 months of a given year. However, for effective querying and visualization, we need a long format where each row represents a single temperature reading for a specific month and year.\nWe can write the following function that melts the data into long-format:\n\ndef prepare_df(df):\n    \"\"\"\n    prepares a piece of wide format dataframe into a long format data frame\n    \"\"\"\n    # \"Melt\" converts monthly temperature columns into a single column\n    df = df.melt(\n        # Keep Station ID and Year as identifiers\n        id_vars=[\"ID\", \"Year\"],\n        # Select VALUE1 to VALUE12 (monthly temperature columns)\n        value_vars=[f\"VALUE{i}\" for i in range(1, 13)],\n        var_name=\"Month\",  # Column for month names\n        value_name=\"Temp\"  # Column for temperature values\n    )\n    # Clean the Month column (remove \"VALUE\" prefix and convert to integer)\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    # Convert temperature values to Celsius\n    df[\"Temp\"] = df[\"Temp\"] / 100  \n    # Remove NaN values (invalid temperature readings)\n    df = df[~np.isnan(df[\"Temp\"])]\n\n    return df\n\nNow that we have a callable function that will prepare our decadal temperature data frames, we are ready to add them to our temps database under a single table temperatures.\nThe code below adds all data to the database (from individual CSV files):\n\n# Once again we define our intervals with list-comprehension\nintervals = [f\"{i}-{i+9}\" for i in range(1901, 2020, 10)]\n# For-loop interates every decadal CSV file\nfor i, interval in enumerate(intervals):\n    filepath = f\"datafiles/{interval}.csv\"\n    # Reads in CSV file as pandas df\n    df = pd.read_csv(filepath)\n    # Calls function we defined above, converting each decadal df into long-format \n    df = prepare_df(df)\n    # Store data in the SQLite database \n    df.to_sql(\"temperatures\", conn, \n              if_exists = \"replace\" if i == 0 else \"append\", index = False)\n\nThus, our temps database has one table temperatures with all our decade intervals.\nWe continue by adding the stations’ data next. The station data is much simpler, containing only one CSV file. Therefore, all we need to do is read the CSV as a pandas data frame, then add the data frame into our temps database as follows:\n\n# Stores link to CSV data\nfilename = \"https://raw.githubusercontent.com/PIC16B-ucla/25W/refs/heads/main/datasets/noaa-ghcn/station-metadata.csv\"\n# Reads in CSV\nstations = pd.read_csv(filename)\n# Adds the pandas df into the temps database as \"stations\"\nstations.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)\n\n27585\n\n\nWe can repeat this process with the country data:\n\n# Stores link to CSV data\nfilename = \"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\"\n# Reads in CSV\ncountries = pd.read_csv(filename)\n# Adds the pandas df into the temps database as \"countries\"\ncountries.to_sql(\"countries\", conn, if_exists = \"replace\", index=False)\n\n279\n\n\nAt this point, we should have successfully added all three data frames to the temps database. To make sure that we did, we can run the following to see the contents of our database:\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\nprint(cursor.fetchall())\n\n[('temperatures',), ('stations',), ('countries',)]\n\n\nAs we can see, we have three tables: temperatures, stations, and countries so we are ready to proceed."
  },
  {
    "objectID": "posts/HW1/index.html#writing-a-reusable-sql-query-function",
    "href": "posts/HW1/index.html#writing-a-reusable-sql-query-function",
    "title": "Exploring NOAA Climate Data: SQL Queries, Trend Analysis, and Advanced Visualizations with Plotly",
    "section": "Writing a Reusable SQL Query Function",
    "text": "Writing a Reusable SQL Query Function\nNow that our climate data is stored in an SQLite database, we need a way to retrieve specific temperature records based on user-defined filters. Instead of writing SQL queries manually every time, we’ll create a function that allows us to query data dynamically.\nIn a seperate python file, climate_database.py, we will write a function called query_climate_database() which accepts five arguments:\n\ndb_file, the file name for the database.\ncountry, a string giving the name of a country for which data should be returned.\nyear_begin and year_end, two integers giving the earliest and latest years for which should be returned (inclusive).\nmonth, an integer giving the month of the year for which should be returned.\n\nAfter our function is written in climate_database.py, we may import the function and check if it is visible as follows:\n\nfrom climate_database import query_climate_database\nimport inspect\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    \"\"\"\n    Query the climate database for temperature readings in a specified country, date range, and month.\n\n    Parameters:\n    - db_file (str): The file name of the SQLite database (i.e. temps.db).\n    - country (str): Target country.\n    - year_begin (int): Start year.\n    - year_end (int): End year.\n    - month (int): The month of the year for which data should be returned (1-12).\n\n    Returns:\n    - pd.DataFrame: A data frame containing the \n    station name, latitude, longitude, country, year, month, and temperature.\n    \"\"\"\n    # Connect to the database\n    conn = sqlite3.connect(db_file)\n    \n    # Writes the SQL query using f-strings\n    query = f\"\"\"\n        SELECT \n            S.name AS NAME,\n            S.latitude AS LATITUDE,\n            S.longitude AS LONGITUDE,\n            C.name AS Country,\n            T.year AS Year,\n            T.month AS Month,\n            T.temp AS Temp\n        FROM \n            temperatures T\n        JOIN \n            stations S ON T.id = S.id\n        JOIN \n            countries C ON SUBSTR(S.id, 1, 2) = C.\"FIPS 10-4\" \n        WHERE \n            C.name = '{country}' AND\n            T.year BETWEEN '{year_begin}' AND '{year_end}' AND\n            T.month = '{month}'\n        ORDER BY\n            S.name ASC\n    \"\"\"\n    # Matches stations ID from the temps and stations tables\n    # Matches using countries: FIPS 10-4 with first two letters of ID from Stations\n    # Filters by country name, start/end year, and month\n    # Orders the station names in alphabetical order \n    df = pd.read_sql_query(query, conn)\n    # Close database\n    conn.close()\n    \n    return df\n\n\n\nTo check if our query is properly working we can run the following:\n\nquery_climate_database(db_file = \"temps.db\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nAGARTALA\n23.883\n91.250\nIndia\n1980\n1\n18.21\n\n\n1\nAGARTALA\n23.883\n91.250\nIndia\n1981\n1\n18.25\n\n\n2\nAGARTALA\n23.883\n91.250\nIndia\n1982\n1\n19.31\n\n\n3\nAGARTALA\n23.883\n91.250\nIndia\n1985\n1\n19.25\n\n\n4\nAGARTALA\n23.883\n91.250\nIndia\n1988\n1\n19.54\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2016\n1\n25.09\n\n\n3148\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2017\n1\n23.90\n\n\n3149\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2018\n1\n22.65\n\n\n3150\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2019\n1\n22.20\n\n\n3151\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2020\n1\n23.75\n\n\n\n\n3152 rows × 7 columns\n\n\n\nOur query function is working correctly, thus we are ready to visualize the data."
  },
  {
    "objectID": "posts/HW1/index.html#visualizing-the-data-using-plotly",
    "href": "posts/HW1/index.html#visualizing-the-data-using-plotly",
    "title": "Exploring NOAA Climate Data: SQL Queries, Trend Analysis, and Advanced Visualizations with Plotly",
    "section": "Visualizing the Data Using Plotly",
    "text": "Visualizing the Data Using Plotly\nUsing the NOAA climate database and query function, we can generate various visualizations with Plotly. Visualizations are often created to address specific research questions and challenges. Therefore, choosing the type of visualization to represent data is critical for interpreting data within the context of specific niches.\n\nGeographic Scatter Plot for Yearly Temperature Increases\nThe first question we will address is:\n\nHow does the average yearly change in temperature vary within a given country?\n\nTo explore this question, we will use the scatter_mapbox function from Plotly Express. This will allow us to visualize the stations on an interactive map. The interactive data points will reveal how certain stations report higher or lower average yearly temperature changes. We will also wrap the visualization code in a function so that users can generate visualizations for different parts of the data by providing different arguments (e.g., different dates, countries, etc.).\nFirst, let’s import Plotly Express:\n\nfrom plotly import express as px\n\nWe want the function to accept six explicit arguments and an undetermined number of keyword arguments:\n\ndb_file, country, year_begin, year_end, and month should remain as previously defined.\nmin_obs, the minimum required number of years of data for any given station.\n**kwargs, additional keyword arguments passed to px.scatter_mapbox(). These can control the colormap used, the mapbox style, etc.\n\nThe following function creates the scatter_mapbox according to user inputted specs:\n\ndef temperature_coefficient_plot(db_file, country, year_begin, \n                                 year_end, month, min_obs, **kwargs):\n    \"\"\"\n    Creates an interactive geographic scatterplot showing yearly \n    temperature change (°C) for stations in a specified country.\n    \n    Parameters:\n    - db_file (str): The file name of the SQLite database (temps.db).\n    - country (str): Target country.\n    - year_begin (int): Start year.\n    - year_end (int): End year.\n    - month (int): Target month (1-12).\n    - min_obs (int): Minimum years of data required per station.\n    - **kwargs: Additional keyword arguments for Plotly.\n    \n    Returns:\n    - plotly.graph_objs._figure.Figure: Interactive map.\n    \"\"\"\n    \n    # Fetch data\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n    \n    # Filter stations with observations &gt;= min_obs\n    df['count'] = df.groupby(['NAME', 'LATITUDE', \n                              'LONGITUDE'])['Year'].transform('count')\n    df_filtered = df[df['count'] &gt;= min_obs]\n    \n    # Function to calculate yearly temperature change coefficients\n    def coeff_func(group):\n        years = group['Year'].values\n        temps = group['Temp'].values\n        return np.polyfit(years, temps, 1)[0]  # Slope = yearly change (°C/year)\n    \n    # Group data and get coeff using .apply() method\n    coeffs = df_filtered.groupby(['NAME', 'LATITUDE', \n                                  'LONGITUDE', 'Country']).apply(coeff_func)\n    coeffs = coeffs.reset_index(name='Coefficient').dropna()\n    # Makes sure coeff are rounded to a sober number of significant figures\n    coeffs = round(coeffs, 4)\n    \n    # Converts int month (1-12) into associated str month name\n    def month_converter(month):\n        month_names = [\n        \"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n        \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n        \n        return month_names[month - 1]\n    \n    # Configures plot to meet standarized look\n    fig = px.scatter_mapbox(\n    coeffs,\n    lat=\"LATITUDE\",\n    lon=\"LONGITUDE\",\n    color=\"Coefficient\",  # Maps the color to the coeff column\n    hover_name=\"NAME\",\n    labels={\n        # Rename Coefficient in hover data\n        \"Coefficient\": \"Estimated Yearly Increase (°C)\"\n    }, **kwargs)\n    \n    # Continued plot config\n    fig.update_layout(\n        width=700,\n        height=400,\n    margin={\"r\": 140, \"t\": 60, \"l\": 0, \"b\": 0},\n    title={ # Title config with f-string to update based on input country and dates\n        \"text\": f\"\"\"The Estimated Yearly Increase in Temperature&lt;br&gt;\n                    for Stations in {country} in {month_converter(month)}\n                    Between {year_begin}-{year_end}.\"\"\",\n        \"x\": 0.5,  \n        \"xanchor\": \"center\",  \n        \"yanchor\": \"top\"},  \n    coloraxis_colorbar={\n            \"title\": \"Estimated Yearly Increase (°C)\",  # Set the color bar title\n            \"title_side\": \"right\",\n            \"x\": 1.0},  \n    coloraxis={  # Color scale range config\n            \"cmin\": -0.13,  \n            \"cmax\": 0.13})\n    \n    return fig\n\nAfter running the cell with our temperature_coefficient_plot() function, we can test it by calling it with the same specs that we fed the query function to create a plot of estimated yearly increases in temperature during the month of January, in the interval 1980-2020, in India, as follows:\n\n# assumes you have imported necessary packages\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(db_file = \"temps.db\",\n                                   country = \"India\", \n                                   year_begin = 1980, \n                                   year_end = 2020, \n                                   month = 1, \n                                   min_obs = 10,\n                                   zoom = 2, # Sets the initial camera \n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\nTo double-check that our temperature_coefficient_plot() function works properly and not just for India during the specified date range, we can try creating the Mapbox plot for Australia:\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(db_file = \"temps.db\",\n                                   country = \"Australia\", # Different country\n                                   year_begin = 2000, # Different start date\n                                   year_end = 2020, \n                                   month = 1, \n                                   min_obs = 10,\n                                   zoom = 2, # Sets the initial camera \n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\nIt looks like our temperature_coefficient_plot() function works properly. Upon analyzing the plot for India and Australia, one can infer that climate temperatures are increasing more rapidly in more densely populated areas. This is likely due to increased levels of pollution which are expediting the effects of global warming. To further explore this, we can use a boxplot graph to address a question that would build upon our inference.\n\n\nComparing Regional Climate Trends Over Time with Box Plots\nAdding to our exploration of geolocation and climate temperatures, we can also ask:\n\nHow does the rate of yearly temperature change vary across different latitude bands within a given country and month?\n\nWe will use the px.box() from Plotly Express to construct a scatter box plot, including facets that separate the data by latitude bands. Similarly to the previous plot, we will use the same query function for our data, but instead we will categorize the coefficients by latitude for faceting.\nWe can construct our visualization function as follows:\n\ndef plot_temperature_trend_boxplot(db_file, country, year_begin, \n                                   year_end, month, min_obs):\n    \"\"\"\n    Creates an interactive boxplot showing the distribution of yearly temperature \n    trends across latitude bands for stations in a specified country and time range.\n    \n    Parameters:\n    - db_file (str): The file name of the SQLite database (temps.db).\n    - country (str): Target country.\n    - year_begin (int): Start year.\n    - year_end (int): End year.\n    - month (int): Target month (1-12).\n    - min_obs (int): Minimum years of data required per station.\n    \n    Returns:\n    - plotly.graph_objs._figure.Figure: Interactive boxplot.\n    \"\"\"\n    \n    # Fetch data\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n    \n    # Filter stations with observations &gt;= min_obs\n    df['count'] = df.groupby(['NAME', 'LATITUDE', 'LONGITUDE'])['Year'].transform('count')\n    df_filtered = df[df['count'] &gt;= min_obs]\n    \n    # Function to calculate yearly temperature change coefficients\n    def coeff_func(group):\n        years = group['Year'].values\n        temps = group['Temp'].values\n        return np.polyfit(years, temps, 1)[0]  # Slope = yearly change (°C/year)\n    \n    # Group data and get coeff using .apply() method\n    coeffs = df_filtered.groupby(['NAME', 'LATITUDE', \n                                  'LONGITUDE', 'Country']).apply(coeff_func)\n    coeffs = coeffs.reset_index(name='Coefficient').dropna()\n    # Makes sure coeff are rounded to a sober number of significant figures\n    coeffs = round(coeffs, 4)\n    \n    # Converts int month (1-12) into associated str month name\n    def month_converter(month):\n        month_names = [\n        \"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n        \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n        \n        return month_names[month - 1]\n    \n    # Categorize latitude into bands for faceting\n    coeffs['Latitude Band'] = pd.cut(coeffs['LATITUDE'], bins=[-90, -30, 0, 30, 60, 90], \n                                     labels=['South Polar', 'South Temperate', \n                                             'Tropics', 'North Temperate', 'North Polar'])\n    \n    # Create faceted box plot with scatter overlay\n    fig = px.box(coeffs, y='Coefficient', x='Latitude Band', points='all',\n                 title=f'''Temperature Trend Distribution for {country} in \n                         {month_converter(month)} ({year_begin}-{year_end})''',\n                 labels={'Coefficient': 'Temperature Trend (°C/year)', \n                         'Latitude Band': 'Latitude Band'},\n                 facet_col='Latitude Band')\n    \n    return fig\n\nAfter running our boxplot function successfully, we can call it for Australia with the same inputs we made for the scatter Mapbox plot:\n\nfig = plot_temperature_trend_boxplot(db_file = \"temps.db\", \n                                   country = \"Australia\", \n                                   year_begin = 2000, \n                                   year_end = 2020,\n                                   month=1,\n                                   min_obs=10)                               \nfig.show()\n\n\n\n\nThe results point to the idea that the South polar band is warming at a rate higher than that of the South Temperate band. Considering Australia’s major urban centers are in its South, we have some substantial evidence of zones with elevated warming. We can further inquire into relationships between latitude and temperature by mapping the change in temperature throughout the year for various latitudes.\n\n\nMapping Monthly Average Temperature by Latitude with 3D Scatterplots\nIf we want to take a look at the bigger picture and see how latitude affects temperature, and how that temperature changes throughout the year in a given country, we can ask:\n\nHow does the average monthly temperature vary across latitudes over time? Is there a relationship?\n\nWe will use the px.scatter_3d() function from Plotly Express to construct an interactive three-dimensional graph with the month (1-12) on the x-axis, station latitude on the y-axis, and average monthly temperature (°C) on the z-axis. This will help identify any broad trends in both seasonal temperature and geographic factors.\nBefore we can construct a function that will plot our 3D graph, we must define a new query function in climate_database.py to handle and prep our data. Our first query function retrieved station-specific data, but this second query focuses on latitude-based temperature trends across months. Let’s call it second_query_climate_database().\nThe function will take the four following arguments for flexible user customization:\n\ndb_file, the file name for the database.\ncountry, a string giving the name of a country for which data should be returned.\nyear_begin and year_end, two integers giving the earliest and latest years for which should be returned (inclusive).\n\nAfter our function is written in climate_database.py, we may import the function and check if it is visible as follows:\n\nfrom climate_database import second_query_climate_database\nimport inspect\nprint(inspect.getsource(second_query_climate_database))\n\ndef second_query_climate_database(db_file, country, year_begin, year_end):\n    \"\"\"\n    Query the climate database for temperature data across months and latitudes.\n\n    Parameters:\n    - db_file (str): The file name of the SQLite database (i.e. temps.db).\n    - country (str): Target country.\n    - year_begin (int): Start year.\n    - year_end (int): End year.\n\n    Returns:\n    - pd.DataFrame: A data frame containing month, latitude, and average temperature.\n    \"\"\"\n    # Connect to the database\n    conn = sqlite3.connect(db_file)\n    \n    # Write the SQL query using f-strings\n    query = f\"\"\"\n        SELECT \n            T.month AS Month,\n            S.latitude AS LATITUDE,\n            ROUND(AVG(T.temp),2) AS Avg_Temp\n        FROM \n            temperatures T\n        JOIN \n            stations S ON T.id = S.id\n        JOIN \n            countries C ON SUBSTR(S.id, 1, 2) = C.\"FIPS 10-4\"\n        WHERE \n            C.name = '{country}' AND\n            T.year BETWEEN {year_begin} AND {year_end}\n        GROUP BY \n            T.month, S.latitude\n        ORDER BY \n            T.month, S.latitude;\n    \"\"\"\n    # Takes average temperature for each month and latitude\n    # Execute the query and load the results into a DataFrame\n    df = pd.read_sql_query(query, conn)\n    \n    # Close the database connection\n    conn.close()\n    \n    return df\n\n\n\nWith our query ready to go, we can write the function that will plot our 3D graph.\nWe can construct the 3D visualization as follows:\n\ndef plot_3d_temperature(db_file, country, year_begin, year_end):\n    \"\"\"\n    Create a 3D scatter plot for temperature data.\n\n    Parameters:\n    - db_file (str): The file name of the SQLite database (temps.db).\n    - country (str): Target country.\n    - year_begin (int): Start year.\n    - year_end (int): End year.\n\n    Returns:\n    - plotly.graph_objects.Figure: The 3D scatter plot.\n    \"\"\"\n    # Fetch data\n    df = second_query_climate_database(db_file, country, year_begin, year_end)\n    \n    # Create 3D scatter plot with color scale to demark higher and lower temps\n    fig = px.scatter_3d(df, x='Month', y='LATITUDE', z='Avg_Temp',\n                        color='Avg_Temp',\n                        title='temp_name',\n                        labels={'Month': 'Month', 'Latitude': 'Latitude',\n                                'Avg_Temp': 'Average Temperature (°C)'},\n                        color_continuous_scale='thermal',\n                        width=800,\n                        height=500)\n    \n    fig.update_traces(marker=dict(size=5))\n    \n    fig.update_layout(\n        margin={\"r\": 120, \"t\": 60, \"l\": 0, \"b\": 0},\n        title={ # Title config with f-string to update based on input country and dates\n        \"text\": f\"\"\"Monthly Average Temperature by Latitude in {country}&lt;br&gt;\n        Between {year_begin}-{year_end}.\"\"\",\n        \"x\": 0.5,  \n        \"xanchor\": \"center\",  \n        \"yanchor\": \"top\"},\n        coloraxis_colorbar={\n            \"title\": \"Average Temp. (°C)\",  # Set the color bar title\n            \"title_side\": \"right\",\n            \"x\": 0.85},\n    ) \n    \n    return fig\n\nAfter running our 3D scatter function successfully, we can call it for Australia with the same inputs as made previously, omitting the months key since we are now plotting values for all months.\nThe following function calls plot_3d_temperature:\n\nfig = plot_3d_temperature(db_file = \"temps.db\", \n                          country = \"Australia\", \n                          year_begin = 2000, \n                          year_end = 2020)\n\nfig.show()\n\n\n\n\nThrough the three plots we explored, we uncovered a nuanced view of Australia’s climate. This analysis reinforces the relationship between latitude, seasonal changes, and long-term warming trends."
  },
  {
    "objectID": "personal.html",
    "href": "personal.html",
    "title": "Personal",
    "section": "",
    "text": "Outside of mathematics, I enjoy photography, hiking, music production, and movies. I’m constantly seeking new trails to explore around Los Angeles, experimenting with various synthesizers, and logging movies into my Letterboxd. When I’m not crunching numbers, you’ll find me perfecting my landscape shots, crafting new beats, hiking on the weekends, or in the theater watching the latest A24 film.\nBelow are a few pictures!\n\nHiking\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese photos are from Cactus to Clouds, one of the hardest day hikes in the U.S. My roommate, Kai, and I started in Palm Springs at 3 AM and reached San Jacinto Peak around 5 PM. The hike took us 14 hours, covering 21 miles with 10,300 feet of elevation gain.\n\n\n\nSource: HikingGuy.com\n\n\n\n\nPhotography\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbove are two photos I took at the 2021 French GP when Max Verstappen won. I love Formula 1, so obviously when I had the chance to go to a race I brought my camera and took lots of photos.\nBelow are a couple of examples of my long exposure/light painting photography. If there were one photography niche that would be my favorite, it would be this. I love all types of long exposures and light painting only allowed me to be even more creative.\n\n\nFavorite Films"
  },
  {
    "objectID": "blog/posts/HW0/index.html#introduction",
    "href": "blog/posts/HW0/index.html#introduction",
    "title": "Exploring Palmer Penguins Data: Seaborn Data Visualization With Heat Maps",
    "section": "Introduction",
    "text": "Introduction\nThe Palmer Penguins dataset contains measurements of penguin species from the Palmer Archipelago in Antarctica, including numeric measurements like culmen(bill) length, flipper length, and body mass. In this blog, we will learn how to construct a heatmap to explore correlations between these numerical features across the three penguin species: Adelie, Chinstrap, and Gentoo."
  },
  {
    "objectID": "blog/posts/HW0/index.html#read-in-and-inspect-the-data",
    "href": "blog/posts/HW0/index.html#read-in-and-inspect-the-data",
    "title": "Exploring Palmer Penguins Data: Seaborn Data Visualization With Heat Maps",
    "section": "Read in and Inspect the Data",
    "text": "Read in and Inspect the Data\nWe will begin by reading the data into Python by running:\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\nThe first line imports the Pandas package into our project. We will use it to read the CSV file and manipulate/analyze data. After setting the variable “url” to our CSV URL, we can use the Pandas read CSV function to store the data frame as “penguins.”\nNext, we will inspect the data by running:\n\npenguins.head()\n\n\n\n\nPalmer Penguin data first 5 rows output"
  },
  {
    "objectID": "blog/posts/HW0/index.html#cleaning-our-data",
    "href": "blog/posts/HW0/index.html#cleaning-our-data",
    "title": "Exploring Palmer Penguins Data: Seaborn Data Visualization With Heat Maps",
    "section": "Cleaning Our Data",
    "text": "Cleaning Our Data\nAnalyzing the first five rows of the data reveals the columns we need to focus on. Since we want to find correlations between culmen length, depth, flipper length, and body mass for each species, we must manipulate the data frame to include only the relevant columns. We can achieve this using the iloc function.\nWe can create a new data frame with our selected columns by running the following code:\n\n# iloc selects columns from penguins data frame using column index positions\n# ':' selects all rows from the DataFrame\n#  '[2, 9, 10, 11, 12]' selects the columns at index positions 2, 9, 10, 11, and 12\npenguin_data = penguins.iloc[:,[2,9,10,11,12]]\n\nThe output will return a data frame, asigned to “penguin_data,” with only our desired columns. However, we still need to clean the data. Some rows in our data frame are missing inputs indicated by “NaN.” We can remove those rows with the “dropna” function.\nRunning the following code will remove all rows with missing data:\n\n# removes rows with missing values \"NaN\"\npenguin_data = penguin_data.dropna()\n\nWe can again check what our data looks like now by running:\n\npenguin_data.head()\n\n\n\n\nPalmer Penguin cleaned data first 5 rows output\n\n\nWith this, our data looks ready to be used."
  },
  {
    "objectID": "blog/posts/HW0/index.html#create-correlation-heat-maps-by-species",
    "href": "blog/posts/HW0/index.html#create-correlation-heat-maps-by-species",
    "title": "Exploring Palmer Penguins Data: Seaborn Data Visualization With Heat Maps",
    "section": "Create Correlation Heat Maps by Species",
    "text": "Create Correlation Heat Maps by Species\nFirst, we must import the relevant packages for our correlation heat maps:\n\nimport seaborn as sns # Used for plotting the heat map visualization\nimport matplotlib.pyplot as plt # Used to for annotating visualization and giving specs\n\nSince we want to create heat maps for each penguin species, we must write a function that 1.) groups the data by species, 2.) calculates the correlation matrix for each group, and 3.) plots the matrices of each group.\nLet’s name the function: “palmer_penguin_heatmap,” which takes in our data frame, a key that will group our data by (in our case, “Species”), and a list of columns that we would like to include in the correlation.\nRunning the following code will establish our function:\n\ndef palmer_penguin_heatmap(dataset, key, cols):\n    \"\"\"\n    Calculates the correlation matrices for each species and plots the heatmap.\n\n    Params:\n    -&gt; dataset (pandas df): The dataset (penguin_data).\n    -&gt; key (str): Column that will group data by (\"Species\").\n    -&gt; cols (list): List of columns for correlation analysis.\n    \"\"\"\n    grouped = dataset.groupby(key) # groups data set by species\n    \n    for species, group in grouped:\n        corr_matrix = group[cols].corr()  # Calculate correlation matrix\n        plt.figure(figsize=(8, 6))  # Create a new figure\n        sns.heatmap(corr_matrix, annot=True, cmap='crest', fmt=\".2f\")  # Plots heatmap\n        plt.title(f\"Correlation Matrix for {species} Penguins\")  # Add title for given species\n        plt.show()  # Display the heatmap\n\nAfter running our function, we are almost ready to call the function with our parameters. We have our data set and key right now, but we must define which columns we want to use for the correlation analysis.\nWe can do this by running:\n\n# Stores a list of column names that we wish to analyze \nnum_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nWe are ready to call our function “palmer_penguin_heatmap” with our three parameters. We should expect our output to be three heat maps for Adelie, Chinstrap, and Gentoo.\nWe can call our function by running:\n\n# Calls the function with our penguin data, groupby key, and target columns\npalmer_penguin_heatmap(dataset = penguin_data, key = 'Species', cols = num_col)\n\nOur outputs should look as follows:"
  },
  {
    "objectID": "blog/posts/HW0/index.html#interpreting-the-heat-maps",
    "href": "blog/posts/HW0/index.html#interpreting-the-heat-maps",
    "title": "Exploring Palmer Penguins Data: Seaborn Data Visualization With Heat Maps",
    "section": "Interpreting the Heat Maps",
    "text": "Interpreting the Heat Maps\nEach heat map is titled with the corresponding species and labeled with measurements to help us interpret the data. Heat maps visualize correlation matrices. The color-coded squares help the viewer interpret higher correlations (denoted by the color bar on the right of the heat map). Thus, each square corresponds to the correlation of two select columns (measurements). As seen by the dark squares, any measurement compared to itself correlates to 1.00. These squares help us see where specific measurements may be associated with others. For example, we can claim that flipper length corresponds to higher body masses for Gentoo penguins since we observe a strong positive correlation (0.72). For this reason, heat maps are a useful first visualization for large data sets to spot patterns that can be further explored."
  },
  {
    "objectID": "blog/posts/HW6/index.html#introduction",
    "href": "blog/posts/HW6/index.html#introduction",
    "title": "Detecting Fake News with Deep Learning: A Keras-Based Text Classification Approach",
    "section": "Introduction",
    "text": "Introduction\nIn this project, we’ll attempt to classify news articles as legitimate or fake news. This project uses deep learning techniques in Keras to classify news articles as real or fake based on their titles and content. The model pipeline includes text preprocessing with NLTK, text vectorization, and word embeddings to extract meaningful linguistic patterns. We’ll build and train three neural network architectures: one using only article titles, another using only article text, and a third combining both for improved accuracy."
  },
  {
    "objectID": "blog/posts/HW6/index.html#preprocessing",
    "href": "blog/posts/HW6/index.html#preprocessing",
    "title": "Detecting Fake News with Deep Learning: A Keras-Based Text Classification Approach",
    "section": "Preprocessing",
    "text": "Preprocessing\nIn this section, we’ll import and process the text to prepare the data for our models.\n\nSetup & Data Acquisition\nFirst, we’ll need to import the necessary libraries and load our dataset:\n\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nimport string\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import TextVectorization\nfrom nltk.corpus import stopwords\nimport nltk\n\n# Download NLTK stopwords\nnltk.download('stopwords', quiet = True)\n\n# Define our training data URL\ntrain_url = \"https://raw.githubusercontent.com/pic16b-ucla/25W/refs/heads/main/datasets/fake_news_train.csv\"\n\n# Read the data\ntrain = pd.read_csv(train_url)\n\n# Display the first few rows to understand the data\ntrain.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\ntitle\ntext\nfake\n\n\n\n\n0\n17366\nMerkel: Strong result for Austria's FPO 'big c...\nGerman Chancellor Angela Merkel said on Monday...\n0\n\n\n1\n5634\nTrump says Pence will lead voter fraud panel\nWEST PALM BEACH, Fla.President Donald Trump sa...\n0\n\n\n2\n17487\nJUST IN: SUSPECTED LEAKER and “Close Confidant...\nOn December 5, 2017, Circa s Sara Carter warne...\n1\n\n\n3\n12217\nThyssenkrupp has offered help to Argentina ove...\nGermany s Thyssenkrupp, has offered assistance...\n0\n\n\n4\n5535\nTrump say appeals court decision on travel ban...\nPresident Donald Trump on Thursday called the ...\n0\n\n\n\n\n\n\n\nOur dataset contains article titles, the full text content, and a binary label indicating whether each article is fake news (1) or real news (0).\n\n\nData Processing\nNext, we’ll create a function, make_dataset(), to process our data by removing stopwords and creating a TensorFlow dataset. The function will do three things:\n\nChange the text to lowercase: .lower().\nRemove stopwords (i.e. “the,” “and,” or “but”) from text and title using NLTK\nConstruct and return a tf.data.Dataset with two inputs (title, text) and one output (fake). The dataset is also batched with size 100.\n\n\ndef make_dataset(df):\n    # Convert to lowercase\n    df['title'] = df['title'].str.lower()\n    df['text'] = df['text'].str.lower()\n    \n    # Get stopwords from NLTK\n    stop_words = set(stopwords.words('english'))\n    \n    # Function to remove stopwords\n    def remove_stopwords(text):\n        if isinstance(text, str):\n            words = text.split()\n            filtered_words = [word for word in words if word not in stop_words]\n            return ' '.join(filtered_words)\n        return \"\"\n    \n    # Apply stopword removal\n    df['title'] = df['title'].apply(remove_stopwords)\n    df['text'] = df['text'].apply(remove_stopwords)\n    \n    # Create dataset with multiple inputs\n    titles = df['title'].values\n    texts = df['text'].values\n    labels = df['fake'].values\n    \n    # Create dictionary datasets for multiple inputs\n    dataset = tf.data.Dataset.from_tensor_slices(\n        ({\"title\": titles, \"text\": texts}, labels)\n    )\n    \n    return dataset.batch(100)  # Batch size of 100\n\n# Process the training data\ndataset = make_dataset(train)\n\n\n\nSplit Training and Validation Data\nNext, we’ll split our data into training and validation sets using a 80/20 split:\n\n# Calculate the size of the training set (80% of the data)\ntrain_size = int(0.8 * len(train))\nval_size = len(train) - train_size\n\n# Split the dataset\ntrain_dataset = dataset.take(train_size//100)  # Divide by batch size\nval_dataset = dataset.skip(train_size//100)\n\n# Calculate base rate\nbase_rate = train['fake'].mean() * 100\nprint(f\"Base rate of fake news: {base_rate:.2f}%\")\n\nBase rate of fake news: 52.30%\n\n\nThe base rate tells us what percentage of our dataset consists of fake news. This gives us a baseline accuracy to compare our models against.\n\n\nText Vectorization Layer\nNow we’ll create a text vectorization layer that will convert our text data into numerical vectors:\n\n# Preparing text vectorization layer for tf model\nsize_vocabulary = 2000\n\ndef standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase, \n                                 '[%s]' % re.escape(string.punctuation), '')\n    return no_punctuation \n\n# Create a shared vectorization layer for both title and text\nvectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary,  # only consider this many words (2000)\n    output_mode='int',\n    output_sequence_length=500) \n\n# Create text-only dataset for adaptation\ntext_dataset = train_dataset.unbatch().map(lambda x, y: x[\"text\"])\n# Create title-only dataset for adaptation\ntitle_dataset = train_dataset.unbatch().map(lambda x, y: x[\"title\"])\n\n# Combine title and text datasets for adaptation\ncombined_texts = text_dataset.concatenate(title_dataset)\nvectorize_layer.adapt(combined_texts)\n\nThe text vectorization layer transforms our text data into numerical representations that our neural network can process."
  },
  {
    "objectID": "blog/posts/HW6/index.html#creating-models",
    "href": "blog/posts/HW6/index.html#creating-models",
    "title": "Detecting Fake News with Deep Learning: A Keras-Based Text Classification Approach",
    "section": "Creating Models",
    "text": "Creating Models\nWe’ll create three different models (title only, text only, both) to compare their performance:\n\nTitle-Only Model\nWe’ll start with the title-only model, which classifies articles as Real or Fake based on the words in headlines:\n\n# Title-only model\ndef create_title_model():\n    # Input layer for title\n    title_input = keras.Input(shape=(1,), dtype=tf.string, name=\"title\")\n    \n    # Vectorize the title\n    title_vectorized = vectorize_layer(title_input)\n    \n    # Embedding layer\n    embedding = layers.Embedding(size_vocabulary, 10, name=\"embedding\")(title_vectorized)\n    \n    # Add dropout for regularization\n    x = layers.Dropout(0.2)(embedding)\n    \n    # Global pooling layer\n    x = layers.GlobalAveragePooling1D()(x)\n    \n    # Add another dropout layer\n    x = layers.Dropout(0.2)(x)\n    \n    # Output layer\n    output = layers.Dense(1, activation=\"sigmoid\")(x)\n    \n    # Create the model\n    model = keras.Model(inputs=title_input, outputs=output, name=\"title_model\")\n    \n    # Compile the model\n    model.compile(\n        loss=\"binary_crossentropy\",\n        optimizer=\"adam\",\n        metrics=[\"accuracy\"]\n    )\n    \n    return model\n\nLet’s create the model and summary:\n\n# Creatd title model\ntitle_model = create_title_model()\n\ntitle_model.summary()\n\nModel: \"title_model\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ title (InputLayer)              │ (None, 1)              │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ text_vectorization              │ (None, 500)            │             0 │\n│ (TextVectorization)             │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ embedding (Embedding)           │ (None, 500, 10)        │        20,000 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (Dropout)               │ (None, 500, 10)        │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling1d        │ (None, 10)             │             0 │\n│ (GlobalAveragePooling1D)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (Dropout)             │ (None, 10)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (Dense)                   │ (None, 1)              │            11 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 20,011 (78.17 KB)\n\n\n\n Trainable params: 20,011 (78.17 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nNow, we’re ready to train the title-only model:\n\n# Train title model\ntitle_history = title_model.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=10,\n    verbose=1\n)\n\n\nEpoch 1/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.7679 - loss: 0.5436 - val_accuracy: 0.7857 - val_loss: 0.5237\n\nEpoch 2/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.7705 - loss: 0.5262 - val_accuracy: 0.7791 - val_loss: 0.5069\n\nEpoch 3/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.7736 - loss: 0.5103 - val_accuracy: 0.7887 - val_loss: 0.4933\n\nEpoch 4/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.7833 - loss: 0.4962 - val_accuracy: 0.7991 - val_loss: 0.4821\n\nEpoch 5/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.7843 - loss: 0.4834 - val_accuracy: 0.7927 - val_loss: 0.4692\n\nEpoch 6/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.7882 - loss: 0.4755 - val_accuracy: 0.7918 - val_loss: 0.4593\n\nEpoch 7/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - accuracy: 0.7915 - loss: 0.4645 - val_accuracy: 0.8022 - val_loss: 0.4503\n\nEpoch 8/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.7971 - loss: 0.4549 - val_accuracy: 0.8033 - val_loss: 0.4417\n\nEpoch 9/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.8015 - loss: 0.4461 - val_accuracy: 0.8079 - val_loss: 0.4338\n\nEpoch 10/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.8031 - loss: 0.4395 - val_accuracy: 0.8074 - val_loss: 0.4267\n\n\n\n\n\n# Visualize the model\nfrom tensorflow.keras.utils import plot_model\nutils.plot_model(title_model, \"title_model.png\", \n                 show_shapes=True,\n                 show_layer_names=True)\n\n\n\n\n\n\n\n\n\n\nText-Only Model\nNext, we’ll build and run the text-only model:\n\n# Text-only model\ndef create_text_model():\n    # Input layer for text\n    text_input = keras.Input(shape=(1,), dtype=tf.string, name=\"text\")\n    \n    # Vectorize the text\n    text_vectorized = vectorize_layer(text_input)\n    \n    # Embedding layer\n    embedding = layers.Embedding(size_vocabulary, 10, name=\"embedding\")(text_vectorized)\n    \n    # Add dropout for regularization\n    x = layers.Dropout(0.2)(embedding)\n    \n    # Global pooling layer\n    x = layers.GlobalAveragePooling1D()(x)\n    \n    # Add another dropout layer\n    x = layers.Dropout(0.2)(x)\n    \n    # Output layer\n    output = layers.Dense(1, activation=\"sigmoid\")(x)\n    \n    # Create the model\n    model = keras.Model(inputs=text_input, outputs=output, name=\"text_model\")\n    \n    # Compile the model\n    model.compile(\n        loss=\"binary_crossentropy\",\n        optimizer=\"adam\",\n        metrics=[\"accuracy\"]\n    )\n    \n    return model\n\nLet’s create the model and summary:\n\n# Create text model\ntext_model = create_text_model()\n\ntext_model.summary()\n\nModel: \"text_model\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ text (InputLayer)               │ (None, 1)              │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ text_vectorization_8            │ (None, 500)            │             0 │\n│ (TextVectorization)             │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ embedding (Embedding)           │ (None, 500, 10)        │        20,000 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (Dropout)             │ (None, 500, 10)        │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling1d_1      │ (None, 10)             │             0 │\n│ (GlobalAveragePooling1D)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (Dropout)             │ (None, 10)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ (None, 1)              │            11 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 20,011 (78.17 KB)\n\n\n\n Trainable params: 20,011 (78.17 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nNow we’re ready to train the model:\n\n# Train text model\ntext_history = text_model.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=10,\n    verbose=1\n)\n\n\nEpoch 1/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.5584 - loss: 0.6785 - val_accuracy: 0.6753 - val_loss: 0.6213\n\nEpoch 2/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8073 - loss: 0.5964 - val_accuracy: 0.8617 - val_loss: 0.5128\n\nEpoch 3/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8938 - loss: 0.4895 - val_accuracy: 0.9235 - val_loss: 0.4160\n\nEpoch 4/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.9166 - loss: 0.4013 - val_accuracy: 0.9453 - val_loss: 0.3466\n\nEpoch 5/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.9281 - loss: 0.3396 - val_accuracy: 0.9486 - val_loss: 0.2986\n\nEpoch 6/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.9356 - loss: 0.2969 - val_accuracy: 0.9523 - val_loss: 0.2640\n\nEpoch 7/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.9412 - loss: 0.2661 - val_accuracy: 0.9536 - val_loss: 0.2383\n\nEpoch 8/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.9448 - loss: 0.2430 - val_accuracy: 0.9587 - val_loss: 0.2182\n\nEpoch 9/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.9478 - loss: 0.2254 - val_accuracy: 0.9628 - val_loss: 0.2024\n\nEpoch 10/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.9510 - loss: 0.2091 - val_accuracy: 0.9644 - val_loss: 0.1893\n\n\n\n\n\n# Visualize the model\nutils.plot_model(text_model, \"text_model.png\", \n                 show_shapes=True,\n                 show_layer_names=True)\n\n\n\n\n\n\n\n\n\n\nCombined Model (Title and Text)\nFinally, we’ll build and run the combined model:\n\n# Combined model (title and text)\ndef create_combined_model():\n    # Input layers\n    title_input = keras.Input(shape=(1,), dtype=tf.string, name=\"title\")\n    text_input = keras.Input(shape=(1,), dtype=tf.string, name=\"text\")\n    \n    # Vectorize inputs using the same vectorization layer\n    title_vectorized = vectorize_layer(title_input)\n    text_vectorized = vectorize_layer(text_input)\n    \n    # Shared embedding layer\n    embedding_layer = layers.Embedding(size_vocabulary, 10, name=\"embedding\")\n    \n    # Apply embedding to both inputs\n    title_embedding = embedding_layer(title_vectorized)\n    text_embedding = embedding_layer(text_vectorized)\n    \n    # Process title branch\n    title_x = layers.Dropout(0.2)(title_embedding)\n    title_x = layers.GlobalAveragePooling1D()(title_x)\n    title_x = layers.Dropout(0.2)(title_x)\n    \n    # Process text branch\n    text_x = layers.Dropout(0.2)(text_embedding)\n    text_x = layers.GlobalAveragePooling1D()(text_x)\n    text_x = layers.Dropout(0.2)(text_x)\n    \n    # Concatenate the two branches\n    concatenated = layers.concatenate([title_x, text_x], name=\"concatenate\")\n    \n    # Add a dense layer\n    x = layers.Dense(32, activation=\"relu\")(concatenated)\n    x = layers.Dropout(0.2)(x)\n    \n    # Output layer\n    output = layers.Dense(1, activation=\"sigmoid\")(x)\n    \n    # Create the model\n    model = keras.Model(inputs=[title_input, text_input], outputs=output, name=\"combined_model\")\n    \n    # Compile the model\n    model.compile(\n        loss=\"binary_crossentropy\",\n        optimizer=\"adam\",\n        metrics=[\"accuracy\"]\n    )\n    \n    return model\n\nLet’s create the model and summary:\n\n# Create combined model\ncombined_model = create_combined_model()\n\ncombined_model.summary()\n\nModel: \"combined_model\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ title (InputLayer)  │ (None, 1)         │          0 │ -                 │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ text (InputLayer)   │ (None, 1)         │          0 │ -                 │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ text_vectorization… │ (None, 500)       │          0 │ title[0][0],      │\n│ (TextVectorization) │                   │            │ text[0][0]        │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding           │ (None, 500, 10)   │     20,000 │ text_vectorizati… │\n│ (Embedding)         │                   │            │ text_vectorizati… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_4 (Dropout) │ (None, 500, 10)   │          0 │ embedding[0][0]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_6 (Dropout) │ (None, 500, 10)   │          0 │ embedding[1][0]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ global_average_poo… │ (None, 10)        │          0 │ dropout_4[0][0]   │\n│ (GlobalAveragePool… │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ global_average_poo… │ (None, 10)        │          0 │ dropout_6[0][0]   │\n│ (GlobalAveragePool… │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_5 (Dropout) │ (None, 10)        │          0 │ global_average_p… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_7 (Dropout) │ (None, 10)        │          0 │ global_average_p… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate         │ (None, 20)        │          0 │ dropout_5[0][0],  │\n│ (Concatenate)       │                   │            │ dropout_7[0][0]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_2 (Dense)     │ (None, 32)        │        672 │ concatenate[0][0] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_8 (Dropout) │ (None, 32)        │          0 │ dense_2[0][0]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_3 (Dense)     │ (None, 1)         │         33 │ dropout_8[0][0]   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n\n\n\n Total params: 20,705 (80.88 KB)\n\n\n\n Trainable params: 20,705 (80.88 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nNow, we’re ready to train the model:\n\n# Train combined model\ncombined_history = combined_model.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=10,\n    verbose=1\n)\n\n\nEpoch 1/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - accuracy: 0.5307 - loss: 0.6842 - val_accuracy: 0.9292 - val_loss: 0.5420\n\nEpoch 2/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.8621 - loss: 0.4532 - val_accuracy: 0.9222 - val_loss: 0.2334\n\nEpoch 3/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.9280 - loss: 0.2307 - val_accuracy: 0.9132 - val_loss: 0.1904\n\nEpoch 4/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.9359 - loss: 0.1826 - val_accuracy: 0.9668 - val_loss: 0.1332\n\nEpoch 5/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.9479 - loss: 0.1565 - val_accuracy: 0.9692 - val_loss: 0.1210\n\nEpoch 6/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.9591 - loss: 0.1327 - val_accuracy: 0.9716 - val_loss: 0.1081\n\nEpoch 7/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.9612 - loss: 0.1217 - val_accuracy: 0.9725 - val_loss: 0.1061\n\nEpoch 8/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.9635 - loss: 0.1136 - val_accuracy: 0.9725 - val_loss: 0.0965\n\nEpoch 9/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.9666 - loss: 0.1065 - val_accuracy: 0.9736 - val_loss: 0.0920\n\nEpoch 10/10\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.9703 - loss: 0.0974 - val_accuracy: 0.9734 - val_loss: 0.0917\n\n\n\n\n\n# Visualize the model\nutils.plot_model(combined_model, \"combined_model.png\", \n                 show_shapes=True,\n                 show_layer_names=True)"
  },
  {
    "objectID": "blog/posts/HW6/index.html#training-history-visualization",
    "href": "blog/posts/HW6/index.html#training-history-visualization",
    "title": "Detecting Fake News with Deep Learning: A Keras-Based Text Classification Approach",
    "section": "Training History Visualization",
    "text": "Training History Visualization\nLet’s visualize the training history to compare the performance of our three models:\n\ndef plot_history(title_history, text_history, combined_history):\n    plt.figure(figsize=(12, 5))\n    \n    # Accuracy plot\n    plt.subplot(1, 2, 1)\n    plt.plot(title_history.history['accuracy'], label='Title Model (Train)')\n    plt.plot(title_history.history['val_accuracy'], label='Title Model (Val)')\n    plt.plot(text_history.history['accuracy'], label='Text Model (Train)')\n    plt.plot(text_history.history['val_accuracy'], label='Text Model (Val)')\n    plt.plot(combined_history.history['accuracy'], label='Combined Model (Train)')\n    plt.plot(combined_history.history['val_accuracy'], label='Combined Model (Val)')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Model Accuracy')\n    plt.legend()\n    \n    # Loss plot\n    plt.subplot(1, 2, 2)\n    plt.plot(title_history.history['loss'], label='Title Model (Train)')\n    plt.plot(title_history.history['val_loss'], label='Title Model (Val)')\n    plt.plot(text_history.history['loss'], label='Text Model (Train)')\n    plt.plot(text_history.history['val_loss'], label='Text Model (Val)')\n    plt.plot(combined_history.history['loss'], label='Combined Model (Train)')\n    plt.plot(combined_history.history['val_loss'], label='Combined Model (Val)')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Model Loss')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.savefig('training_history.png')\n    plt.show()\n\n# Plot the training history\nplot_history(title_history, text_history, combined_history)\n\n\n\n\n\n\n\n\nBased on the training history, we can observe:\n\nThe Combined Model achieves the highest validation accuracy at approximately 97%, exceeding our target accuracy.\nThe Text Model performs quite well too, reaching around 96% validation accuracy.\nThe Title Model performs significantly worse, only reaching about 80% accuracy.\n\nThis indicates that while article titles provide some signal for fake news detection, the full text contains much more useful information, and using both together delivers the best performance."
  },
  {
    "objectID": "blog/posts/HW6/index.html#evaluation-on-test-data",
    "href": "blog/posts/HW6/index.html#evaluation-on-test-data",
    "title": "Detecting Fake News with Deep Learning: A Keras-Based Text Classification Approach",
    "section": "Evaluation on Test Data",
    "text": "Evaluation on Test Data\nLet’s evaluate our most performant model on the test dataset:\n\n# Load test data\ntest_url = \"https://raw.githubusercontent.com/pic16b-ucla/25W/refs/heads/main/datasets/fake_news_test.csv\"\ntest = pd.read_csv(test_url)\n\n# Create test dataset\ntest_dataset = make_dataset(test)\n\n# Evaluate model on the test dataset\ncombined_result = combined_model.evaluate(test_dataset)\n\n# Print the results\nprint(\"Combined Model Test Accuracy: {:.2f}%\".format(combined_result[1] * 100))\n\n\n225/225 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.9695 - loss: 0.0996\n\nCombined Model Test Accuracy: 96.96%"
  },
  {
    "objectID": "blog/posts/HW6/index.html#embedding-visualization",
    "href": "blog/posts/HW6/index.html#embedding-visualization",
    "title": "Detecting Fake News with Deep Learning: A Keras-Based Text Classification Approach",
    "section": "Embedding Visualization",
    "text": "Embedding Visualization\nNow let’s visualize the word embeddings learned by our best model:\n\nimport plotly.graph_objects as go\nfrom sklearn.decomposition import PCA\n\n# Get the embedding layer weights\nembedding_layer = combined_model.get_layer(\"embedding\")\nweights = embedding_layer.get_weights()[0]\n\n# Get the vocabulary from the vectorization layer\nvocab = vectorize_layer.get_vocabulary()\n\n# Use PCA to reduce dimensions for visualization (3D)\npca = PCA(n_components=3)\nembeddings_3d = pca.fit_transform(weights)\n\n# Create a DataFrame for plotting\ndf_embeddings = pd.DataFrame({\n    'word': vocab,\n    'x': embeddings_3d[:, 0],\n    'y': embeddings_3d[:, 1],\n    'z': embeddings_3d[:, 2]\n})\n\n# Filter out padding and out-of-vocabulary terms\ndf_plot = df_embeddings[~df_embeddings['word'].isin(['', '[UNK]'])].copy()\n\n# Compute distance from origin for highlighting interesting words\ndf_plot['distance'] = np.sqrt(df_plot['x']**2 + df_plot['y']**2 + df_plot['z']**2)\n\n# Selected words to highlight, including political, news-related and sentiment words\nhighlight_words = [\n    'trump', 'clinton', 'hillary', 'obama', 'democrats', 'republicans',\n    'fake', 'real', 'true', 'false', 'hoax', 'conspiracy', 'news', 'media',\n    'war', 'military', 'government'\n]\n\n# Create a new column to determine text visibility and point size\ndf_plot['show_label'] = df_plot['word'].isin(highlight_words)\ndf_plot['point_size'] = 3  # Default size\ndf_plot.loc[df_plot['show_label'], 'point_size'] = 6  # Larger for highlighted words\n\n# Create text colors - visible only for highlighted words\ntext_colors = ['rgba(0,0,0,1)' if show else 'rgba(0,0,0,0)' for show in df_plot['show_label']]\n\n# Calculate the range of each axis for better scaling\nx_range = [df_plot['x'].min(), df_plot['x'].max()]\ny_range = [df_plot['y'].min(), df_plot['y'].max()]\nz_range = [df_plot['z'].min(), df_plot['z'].max()]\n\n# Calculate axis scaling factors to make the plot more compact\n# Specifically reduce the y-axis (component 2) range\nscale_factor_y = 0.5  # Reduce y-axis scale by 50%\n\n# Create the 3D scatter plot\nfig = go.Figure(data=[\n    go.Scatter3d(\n        x=df_plot['x'],\n        y=df_plot['y'] * scale_factor_y,  # Scale down component 2\n        z=df_plot['z'],\n        mode='markers+text',\n        text=df_plot['word'],\n        textposition=\"top center\",\n        textfont=dict(\n            size=10,\n            color=text_colors\n        ),\n        marker=dict(\n            size=df_plot['point_size'],\n            color=df_plot['distance'],\n            colorscale='Viridis',\n            opacity=0.8,\n        ),\n        hoverinfo='text',\n        hovertext=df_plot['word']\n    )\n])\n\n# Update layout with tighter ranges\nfig.update_layout(\n    title=\"3D Word Embedding Visualization for Fake News Classification\",\n    width=900,\n    height=700,\n    scene=dict(\n        xaxis=dict(\n            title='PCA Component 1',\n            range=[x_range[0] * 1.1, x_range[1] * 1.1]  # Add small margin\n        ),\n        yaxis=dict(\n            title='PCA Component 2',\n            range=[y_range[0] * scale_factor_y * 1.1, y_range[1] * scale_factor_y * 1.1]\n        ),\n        zaxis=dict(\n            title='PCA Component 3',\n            range=[z_range[0] * 1.1, z_range[1] * 1.1]\n        ),\n        aspectmode='manual',  # Manual aspect ratio\n        aspectratio=dict(x=1, y=0.5, z=1)  # Reduce y-axis aspect ratio\n    )\n)\n\n# Show the plot\nfig.show()\n\n\n\n\n\nAnalysis of Word Embeddings\nScrolling over the word embeddings we can see some trends. Here are some examples:\n\nPolitical Polarization: In the embedding space, we can see that “trump” and “clinton” appear in distinct regions while “hilary” and “obama” remain close together. This might suggest that the model has learned different contextual patterns for how these names appear in news articles. This separation likely reflects the polarized nature of political reporting, where certain linguistic patterns are associated with coverage of different political figures.\nWar: “war” has words surrounding it that naturally seem to be brought up in the same discussions and context such as “targeted,” “campaign,” “missile,” etc.\nMedia References: The word “media” appears in a different region from specific news outlet names, suggesting that generic references to “the media” might be used differently in fake news (often as a collective entity to criticize) compared to specific outlet citations which might be more common in legitimate reporting.\nConspiracy-Related Terms: Words like “hoax”, “conspiracy” and related terms form their own cluster, indicating the model has learned to associate these concepts with questionable content. This semantic grouping helps the model identify articles that use language patterns typical of conspiracy theories.\nPolitical parties: words like “democrats” and “republicans” are positioned close together as they are likely often discussed together along with words such as “washington” and “administration.”\n\nThese embedding patterns help explain why our combined model performs better than either title-only or text-only models. By learning the complex relationships between words in both titles and full text, the model captures rich semantic information about how language is used differently in fake versus real news articles."
  },
  {
    "objectID": "blog/posts/HW3/index.html#introduction",
    "href": "blog/posts/HW3/index.html#introduction",
    "title": "Building a Visually Appealing Message Bank Web App with Dash & SQLite",
    "section": "Introduction",
    "text": "Introduction\nDash is a Python framework for building web applications using Flask, Plotly, and React. It enables developers to create interactive web apps with minimal front-end knowledge, making it a great choice for data-driven applications. We will work through the process of building a simple message bank web app using Dash and SQLite. This app will allow users to submit and view messages dynamically through an interactive interface."
  },
  {
    "objectID": "blog/posts/HW3/index.html#getting-started",
    "href": "blog/posts/HW3/index.html#getting-started",
    "title": "Building a Visually Appealing Message Bank Web App with Dash & SQLite",
    "section": "Getting Started…",
    "text": "Getting Started…\nBefore creating our web app, we must import the necessary packages and initialize the app.\n\nImporting Necessary Packages\nWe’ll import the following packages for our web app:\n\nimport sqlite3\nfrom dash import Dash, html, dcc, Input, Output, State\nimport dash_bootstrap_components as dbc\n\nHow the packages will be used:\n\nsqlite3: Handles database operations\nDash: Creates the web application\nhtml, dcc: Build UI components\nInput, Output, State: Used for Dash callbacks\ndbc (Dash Bootstrap Components): Provides pre-styled UI components\n\n\n\nSet Global Styling (Dark Mode)\nWe’ll build a web app with a dark theme, so we need to set our custom global CSS styling. We can do so by running:\n\n# Custom CSS for additional styling\ncustom_css = {\n    'dark-card': {\n        'backgroundColor': '#2a2a2a', # Hex Color code\n        'border': '1px solid #404040',\n        'borderRadius': '15px',\n        'boxShadow': '0 4px 8px 0 rgba(0,0,0,0.2)'\n    },\n    'input-style': {\n        'backgroundColor': '#333333',\n        'color': 'white',\n        'border': '1px solid #404040'\n    },\n    'message-card': {\n        'backgroundColor': '#1e1e1e',\n        'border': '1px solid #404040',\n        'borderRadius': '10px'\n    }\n}\n\n\n\nInitializing Our Web App\nWe’ll initialize the Dash app by running:\n\n# Initialize the app\napp = Dash(__name__, external_stylesheets=[dbc.themes.DARKLY])\napp.title = \"Message Bank\"\n\nRunning the code above does the following:\n\nInitializes the Dash application\nLoads Bootstrap styling (DARKLY for dark theme)\nSets the page title\n\n\n\nDefining the App Layout\nWe’ll begin defining the app layout by running the following:\n\n# App layout with dark theme\napp.layout = dbc.Container(fluid=True, style={'padding': '20px'}, children=[\n    html.Div([\n        html.H1(\"Message Bank\", # Create app title block\n                className=\"text-center mt-4 mb-4\", \n                style={'color': '#6f42c1', 'fontWeight': 'bold'}),\n    ], style={'backgroundColor': 'transparent', 'color': '#ffffff', \n              'fontFamily': 'Roboto, sans-serif'}), # Change color (hex code) and font\n\nThe code includes:\n\ndbc.Container(fluid=True) creates a full responsive layout.\nchildren will include all the features we add.\nhtml.H1() displays the title “Message Bank.”\nstyle contains the color information for the title.\n\nWe’ll add more to this code block as we add the layout for the submission and viewing features."
  },
  {
    "objectID": "blog/posts/HW3/index.html#setting-up-the-submission-system",
    "href": "blog/posts/HW3/index.html#setting-up-the-submission-system",
    "title": "Building a Visually Appealing Message Bank Web App with Dash & SQLite",
    "section": "Setting Up the Submission System…",
    "text": "Setting Up the Submission System…\nIn this section we will set up the submission system so that users will be able to to enter a name or user and a message and submit. By hitting submit, the name and message will be added to the message_db database so that later they can be retrieved in the view section. All of this functionality will be wrapped in a user-friendly UI.\n\nCreating the UI for Submitting Messages\nBefore handling the database and necessary functions, we’ll first create the input form where users will submit their messages.\nWe’ll add the code for creating the submission UI into app.layout as another child:\n\n# App layout with dark theme\napp.layout = dbc.Container(fluid=True, style={'padding': '20px'}, children=[\n    html.Div([\n        html.H1(\"Message Bank\", # Create app title block\n                className=\"text-center mt-4 mb-4\", \n                style={'color': '#6f42c1', 'fontWeight': 'bold'}),\n    ], style={'backgroundColor': 'transparent', 'color': '#ffffff', \n              'fontFamily': 'Roboto, sans-serif'}), # Change color (hex code) and font\n\n# New Code: ------------------------------------------------------------------\n    \n    # Submission Section\n    dbc.Card(className=\"dark-card\", children=[ # Submission has its own children\n        dbc.CardBody(className=\"mb-4\", children=[  \n            html.H2(\"Submit a Message\", # Title for Submission UI\n                    className=\"card-title mb-3\", \n                    style={'color': '#6f42c1'}), # Styling\n            dcc.Input( # Creates input box for user-name\n                id='handle-input',\n                type='text',\n                placeholder='Your name',\n                className='mb-3 form-control',\n                style={**custom_css['input-style'], 'height': '45px'} # Global CSS\n            ),\n            dcc.Textarea( # Creates message box\n                id='message-input',\n                placeholder='Your message...',\n                className='mb-3 form-control',\n                style={**custom_css['input-style'], 'height': '150px'}\n            ),\n            dbc.Button( # Creates submit button\n                \"Submit\",\n                id='submit-button',\n                color=\"primary\",\n                className='mb-3 w-100',\n                style={'backgroundColor': '#6f42c1', 'border': 'none'}\n            ),\n            html.Div(id='submit-confirmation') # Creates submission confirmation\n        ])\n    ]),\n\nWhat dbc.Card code does:\n\nCreates a name input field dcc.Input()\nCreates a message input field dcc.Textarea()\nAdds a Submit button dbc.Button()\nAdds a confirmation message area html.Div()\n\nAt this point, the UI is set up but does not store messages yet.\n\n\nCreating the Database to Store Messages\nSince we need to store user messages, we’ll use SQLite.\nWe can write a function def get_message_db() to initialize and return the SQLite database connection:\n\n# Database setup\nmessage_db = None\n\ndef get_message_db():\n    # Retrieve or create the message database.\n    global message_db\n    if message_db:\n        return message_db\n    else:\n        message_db = sqlite3.connect(\"messages_db.sqlite\", check_same_thread=False)\n        cmd = \"\"\"\n            CREATE TABLE IF NOT EXISTS messages \n            (handle TEXT, message TEXT)\n        \"\"\"\n        # Creates table if not existing\n        # Queries for user handle & message\n        cursor = message_db.cursor()\n        cursor.execute(cmd)\n        message_db.commit()\n        return message_db\n\nWhat the function does:\n\nConnects to messages_db.sqlite\nCreates a messages table (if it doesn’t exist) with: 1. handle: user’s name 2. message: user’s message\n\n\n\nWriting a Function to Insert Messages\nNow that we have a database, we need to store user messages.\nWe can write a function def insert_message() to insert a new message into the database:\n\ndef insert_message(handle, message):\n    db = get_message_db()\n    cursor = db.cursor()\n    # Don't use f-string.\n    # Use parameterized queries (?, ?) to prevent SQL injection & handle special characters\n    cursor.execute(\"INSERT INTO messages (handle, message) VALUES (?, ?)\", (handle, message))\n    db.commit()\n    db.close()\n    global message_db\n    message_db = None\n\nWhat the function does:\n\nSaves the user’s name: handle and message: message into the database.\nUses parameterized queries ? to prevent SQL injection.\n\n\n\nConnecting the UI to the Database (Submit Callback)\nNow, we need to connect the Submit button to the insert function so that user input is stored when they click the button.\nWe’ll write a callback @app.callback and function def submit() to handle this:\n\n@app.callback(\n    Output('submit-confirmation', 'children'), # Updates the confirmation message div\n    Input('submit-button', 'n_clicks'), # Triggered when the submit button is clicked\n    State('handle-input', 'value'), # Value from the handle input field\n    State('message-input', 'value'), # Value from the message input field\n    prevent_initial_call=True # Prevents the callback from running on app startup\n)\ndef submit(n_clicks, handle, message):\n    # Ensures both handle and message are provided and not empty\n    if not handle or not message or handle.strip() == '' or message.strip() == '':\n        return dbc.Alert(\"Please enter both a name and a message.\", color=\"danger\")\n    \n    # Insert the message into the database\n    insert_message(handle.strip(), message.strip())\n    \n    # Return a success message and thanks the user\n    return dbc.Alert(\"Message submitted successfully! Thank you!\", color=\"success\")\n\nWhat the callback and function does:\n\nRetrieves user input (handle, message).\nChecks if input is empty.\nCalls insert_message() to store data in the database.\nDisplays a success message in submit-confirmation."
  },
  {
    "objectID": "blog/posts/HW3/index.html#viewing-user-submissions",
    "href": "blog/posts/HW3/index.html#viewing-user-submissions",
    "title": "Building a Visually Appealing Message Bank Web App with Dash & SQLite",
    "section": "Viewing User Submissions…",
    "text": "Viewing User Submissions…\nIn this section, we’ll set up the view system so that users can click “update” to view up to five previously submitted messages randomly selected. By clicking the button, the database will be queried to retrieve handles and messages and display them neatly. Like the submission section, we’ll start by establishing the UI, defining the functions, and connecting them.\n\nCreating the UI to Display Messages\nNow, we’ll create a user-interface for users to view messages. We’ll add another child to our existing app.layout:\n\n# App layout with dark theme\napp.layout = dbc.Container(fluid=True, style={'padding': '20px'}, children=[\n    html.Div([\n        html.H1(\"Message Bank\", # Create app title block\n                className=\"text-center mt-4 mb-4\", \n                style={'color': '#6f42c1', 'fontWeight': 'bold'}),\n    ], style={'backgroundColor': 'transparent', 'color': '#ffffff', \n              'fontFamily': 'Roboto, sans-serif'}), # Change color (hex code) and font\n    \n    # Submission Section\n    dbc.Card(className=\"dark-card\", children=[ # Submission has its own children\n        dbc.CardBody(className=\"mb-4\", children=[  \n            html.H2(\"Submit a Message\", # Title for Submission UI\n                    className=\"card-title mb-3\", \n                    style={'color': '#6f42c1'}), # Styling\n            dcc.Input( # Creates input box for user-name\n                id='handle-input',\n                type='text',\n                placeholder='Your name',\n                className='mb-3 form-control',\n                style={**custom_css['input-style'], 'height': '45px'} # Global CSS\n            ),\n            dcc.Textarea( # Creates message box\n                id='message-input',\n                placeholder='Your message...',\n                className='mb-3 form-control',\n                style={**custom_css['input-style'], 'height': '150px'}\n            ),\n            dbc.Button( # Creates submit button\n                \"Submit\",\n                id='submit-button',\n                color=\"primary\",\n                className='mb-3 w-100',\n                style={'backgroundColor': '#6f42c1', 'border': 'none'}\n            ),\n            html.Div(id='submit-confirmation') # Creates submission confirmation\n        ])\n    ]),\n    \n# New Code: ------------------------------------------------------------------\n    \n        # View Messages Section\n        dbc.Card(className=\"dark-card\", children=[\n            dbc.CardBody([\n                html.H2(\"View Random Messages\", # Title of Viewing UI\n                        className=\"card-title mb-3\", \n                        style={'color': '#ffffff', 'fontFamily': 'Roboto, sans-serif'}),\n                dbc.Button( # Button to trigger random messages\n                    \"Update\", \n                    id='view-button',\n                    color=\"info\",\n                    className='mb-3 w-100',\n                    style={'backgroundColor': '#20c997', 'border': 'none'}\n                ),\n                html.Div(id='message-display') # Dynamically updated\n            ])\n        ])\n    ])\n\nWhat dbc.card does:\n\nAdds a button to refresh the displayed messages.\nhtml.Div(id='message-display') will be populated dynamically.\n\n\n\nFetching Messages from the Database\nNow, we’ll need a function to retrieve messages for display.\nWe can write a function def random_messages() to fetch up to n random messages from the database:\n\ndef random_messages(n=5): # Limit number to 5\n    db = get_message_db()\n    cursor = db.cursor()\n    # Query function with f-string to pass n into query\n    cursor.execute(f\"SELECT handle, message FROM messages ORDER BY RANDOM() LIMIT {n}\")\n    messages = cursor.fetchall()\n    db.close()\n    global message_db\n    message_db = None\n    return messages\n\nWhat this function does:\n\nRetrieves up to 5 messages randomly from the database.\nEnsures messages appear in a shuffled order.\n\n\n\nConnecting the UI to the Database (View Callback)\nNow, we’ll connect the View button to the random_messages() function.\nWe’ll write a callback @app.callback and function def view() to handle this:\n\n@app.callback(\n    Output('message-display', 'children'), # Updates the div to display messages\n    Input('view-button', 'n_clicks'), # Triggered when the view button is clicked\n    prevent_initial_call=True # Prevents the callback from running on app startup\n)\ndef view(n_clicks):\n    \"\"\"Displays a random selection of messages.\"\"\"\n    # Fetch up to 5 random messages from the database\n    messages = random_messages(5)\n    \n    # If no messages are found, display a placeholder message\n    if not messages:\n        return html.P(\"No messages to display.\", style={'color': 'white'})\n    \n    # Create a list of styled cards for each message\n    message_cards = []\n    for handle, message in messages:\n        card = dbc.Card(\n            className=\"mb-3\",\n            style=custom_css['message-card'],  # Apply custom styling for the card\n            children=[\n                dbc.CardBody([\n                    html.Blockquote(\n                        className=\"blockquote mb-0\",\n                        children=[\n                            # Display the message\n                            html.P(message, style={'color': '#dee2e6'}),\n                            html.Footer(\n                                html.Small(\n                                    # Display the handle\n                                    html.I(f\"— {handle}\"),\n                                    className=\"text-muted\"\n                                )\n                            )\n                        ]\n                    )\n                ])\n            ]\n        )\n        message_cards.append(card)  # Add the card to the list\n    \n    # Return the list of message cards to be displayed\n    return message_cards\n\nWhat the callback and function does:\n\nCalls random_messages() to fetch stored messages.\nDisplays each message in styled blockquotes."
  },
  {
    "objectID": "blog/posts/HW3/index.html#running-the-web-app",
    "href": "blog/posts/HW3/index.html#running-the-web-app",
    "title": "Building a Visually Appealing Message Bank Web App with Dash & SQLite",
    "section": "Running the Web App",
    "text": "Running the Web App\nTo run the app we need to write one last bit of code to prevent us from accidentally running the code mistakenly as well as defining the port our website will occupy:\n\n# Run the app\nif __name__ == '__main__':\n    app.run_server(debug=False, port=8080) # Feel free to change port\n\nIn the terminal, after changing the directory to our project folder, we can run to launch the web app:\npython &lt;your filename&gt;.py\nWe’ll get a local host link to open in our browser (private browser recommended). We should see something like the initial page shown in the beginning.\nFrom here we can type in a name/message and submit as shown:\n\nClicking on the “Update” button should show previously submitted messages:\n\nAdditionally, when a user attempts to submit with either (or both) a missing handle/message, we should see the error message:"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Welcome to my blog! Here you’ll find posts about my thoughts, projects, and whatever else I’m currently exploring.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nA Coin Mixture Paradox: The Interesting Case of Constant Variance\n\n\n\nBayesian Statistics\n\nProbability\n\n\n\n\n\n\n\n\n\nApr 3, 2025\n\n\nEmil Blaignan\n\n\n\n\n\n\n\n\n\n\n\n\nYieldCurveForecaster: 3D Yield Curve Modeling and Forecasting Dashboard\n\n\n\nSQL\n\nPlotly\n\nDash\n\n\n\n\n\n\n\n\n\nMar 20, 2025\n\n\nEmil Blaignan\n\n\n\n\n\n\n\n\n\n\n\n\nDetecting Fake News with Deep Learning: A Keras-Based Text Classification Approach\n\n\n\nTensorflow\n\nPandas\n\nNumPy\n\nNLTK\n\n\n\n\n\n\n\n\n\nMar 15, 2025\n\n\nEmil Blaignan\n\n\n\n\n\n\n\n\n\n\n\n\nJAX-Driven Heat Diffusion: Sparse and Vectorized PDE Simulation\n\n\n\nJAX\n\nScipy\n\nNumPy\n\n\n\n\n\n\n\n\n\nFeb 24, 2025\n\n\nEmil Blaignan\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a Visually Appealing Message Bank Web App with Dash & SQLite\n\n\n\nDash\n\nSQL\n\n\n\n\n\n\n\n\n\nFeb 16, 2025\n\n\nEmil Blaignan\n\n\n\n\n\n\n\n\n\n\n\n\nData-Driven Movie Recommendations: Scrapy Web Scraping and Actor Network Graphs with NetworkX\n\n\n\nScrapy\n\nNetworkX\n\nPlotly\n\nPandas\n\nNumPy\n\n\n\n\n\n\n\n\n\nFeb 8, 2025\n\n\nEmil Blaignan\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Palmer Penguins Data: Seaborn Data Visualization With Heat Maps\n\n\n\nPandas\n\nSeaborn\n\nMatplotlib\n\n\n\n\n\n\n\n\n\nJan 21, 2025\n\n\nEmil Blaignan\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Emil Blaignan",
    "section": "",
    "text": "Hi, I am a third-year undergraduate student at UCLA majoring in Mathematics/Economics B.S. with a specialization in Computing. My intellectual pursuits center on quantitative finance (fixed-income breadth), ML-driven macroeconomic forecasting, and numerical optimization. I have also worked on projects involving time-series forecasting of Treasury yields, portfolio optimization/risk, and econometric analysis of monetary policy. See the Projects section for more details. I am particularly interested in fixed-income analysis and portfolio management and am keen to see how financial mathematics and deep learning can improve these areas.\nOutside academics, I enjoy photography, hiking, music production, and movies. See the Personal section for more details."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Emil Blaignan",
    "section": "",
    "text": "Hi, I am a third-year undergraduate student at UCLA majoring in Mathematics/Economics B.S. with a specialization in Computing. My intellectual pursuits center on quantitative finance (fixed-income breadth), ML-driven macroeconomic forecasting, and numerical optimization. I have also worked on projects involving time-series forecasting of Treasury yields, portfolio optimization/risk, and econometric analysis of monetary policy. See the Projects section for more details. I am particularly interested in fixed-income analysis and portfolio management and am keen to see how financial mathematics and deep learning can improve these areas.\nOutside academics, I enjoy photography, hiking, music production, and movies. See the Personal section for more details."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Emil Blaignan",
    "section": "Contact",
    "text": "Contact\nFeel free to reach out via email."
  }
]